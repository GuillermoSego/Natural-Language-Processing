{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 4. Modelos de Lenguaje Estadísticos\n",
    "\n",
    "Guillermo Segura Gómez\n",
    "\n",
    "## Modelos de Lenguaje y Evaluación\n",
    "**1. Preprocese todos los tuits de agresividad (positivos y negativos) según su intuición para construir un buen corpus para un modelo de lenguaje (e.g., solo palabras en minúscula, etc.). Agregue tokens especiales de <s> y </s> según usted considere (e.g., al inicio y final de cada tuit). Defina su vocabulario y enmascare con <unk> toda palabra que no esté en su vocabulario.**\n",
    "\n",
    "### Modelos de Lenguaje\n",
    "\n",
    "Un modelo de lenguaje típicamente asigna probabilidades a secuencias de palabras o genera secuencias de palabras basándose en las probabilidades aprendidas. Estas probabilidades reflejan qué tan probable es que una palabra siga a otra u otras en una secuencia. Los modelos de lenguaje pueden ser de varios tipos, incluidos los basados en reglas gramaticales, los estadísticos, y más recientemente, los basados en redes neuronales, como los modelos de lenguaje transformadores.\n",
    "\n",
    "### Modelos de Lenguaje de N-gramas\n",
    "Los modelos de lenguaje de n-gramas son un tipo específico de modelo de lenguaje estadístico que se basa en la premisa de que la probabilidad de una palabra en una secuencia depende solo de las $n-1$ palabras anteriores. Un \"n-grama\" es, por lo tanto, una secuencia de $n$ elementos (por ejemplo, palabras) del texto. Por ejemplo:\n",
    "\n",
    "- Un 1-grama (o unigrama) considera cada palabra de forma aislada.\n",
    "- Un 2-grama (o bigrama) considera pares de palabras consecutivas.\n",
    "- Un 3-grama (o trigrama) considera tríos de palabras consecutivas, y así sucesivamente.\n",
    "\n",
    "En un modelo de n-gramas, la probabilidad de una palabra dada está condicionada por las $n-1$ palabras anteriores. Estos modelos son relativamente simples de construir y pueden ser muy efectivos para ciertas tareas, especialmente cuando los recursos computacionales son limitados o cuando se dispone de una cantidad limitada de datos de entrenamiento. Sin embargo, los modelos de n-gramas tienen limitaciones, como la dificultad para manejar contextos más largos y el problema de la \"explosión combinatoria\" de posibles n-gramas a medida que $n$ aumenta, lo que puede hacer que el modelo sea menos práctico para valores grandes de $n$.\n",
    "\n",
    "Para poder construir un modelo de n-gramas primero necesitamos pre procesar nuestro corpus para que sea mas sencillo de manejar. Por ejemplo, podemos trabajar únicamente palabras en minúscula. Además, es necesario tener delimitadores de oraciones \\<s\\> y \\</s\\> y definir un vocabulario. Cualquier palabra fuera del vocabulario le asignamos el token \\<unk\\>. Vamos a trabajar con los tweets de agresividad del corpus [MEX-A3T](https://sites.google.com/view/mex-a3t/home?authuser=0) con el que hemos estado trabajando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que extrae el texto de dos archivos. Uno el de los documentos, otro el de las etiquetas\n",
    "def get_text_from_file(path_corpus, path_truth):\n",
    "\n",
    "    tr_text = []\n",
    "    tr_labels = []\n",
    "\n",
    "    with open(path_corpus, \"r\") as f_corpus, open(path_truth, \"r\") as f_truth:\n",
    "        for tweet in f_corpus:\n",
    "            tr_text += [tweet]\n",
    "        for label in f_truth:\n",
    "            tr_labels += [label]\n",
    "\n",
    "    return tr_text, tr_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_text = \"/home/guillermosegura/Desktop/Segundo Semestre/Natural-Language-Processing/MexData/mex20_train.txt\"\n",
    "path_labels = \"/home/guillermosegura/Desktop/Segundo Semestre/Natural-Language-Processing/MexData/mex20_train_labels.txt\"\n",
    "\n",
    "path_text_val = \"/home/guillermosegura/Desktop/Segundo Semestre/Natural-Language-Processing/MexData/mex20_val.txt\"\n",
    "path_labels_val = \"/home/guillermosegura/Desktop/Segundo Semestre/Natural-Language-Processing/MexData/mex20_val_labels.txt\"\n",
    "\n",
    "tr_text, tr_labels = get_text_from_file(path_text, path_labels) # Importamos los datos de entrenamiento\n",
    "val_text, val_labels = get_text_from_file(path_text_val, path_labels_val) # Importamos los datos de test o validación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que tenemos que hacer es delimitar nuestro vocabulario. Limitamos solo las palabras en minúsculas. \n",
    "Además debemos agregar los tokens de inicio y final a cada documento del corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para preprocesar los textos\n",
    "def preprocess_texts(texts):\n",
    "    # Convertir a minúsculas y eliminar números y caracteres especiales\n",
    "    texts_lower = [re.sub(r\"http\\S+|[^a-zA-Z\\s]\", \"\", doc.lower()) for doc in texts]\n",
    "    return texts_lower\n",
    "\n",
    "# Función que agrega delimitadores de oraciones a un corpus\n",
    "def addlimiters(texts):\n",
    "    texts_limites = []\n",
    "    for doc in texts:\n",
    "        newText = \"<s>\" + doc + \"</s>\"\n",
    "        texts_limites.append(newText)\n",
    "\n",
    "    return texts_limites\n",
    "\n",
    "# Preprocesar el texto\n",
    "tr_text_min = preprocess_texts(tr_text)\n",
    "\n",
    "# Agregar delimitadores de oración\n",
    "tr_text_limited = addlimiters(tr_text_min)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tokenizamos el texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tamaño del corpus es: 94261\n",
      "El tamaño del vocabulario es: 12388\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TweetTokenizer() # Inicializar tokenizer\n",
    "corpus_palabras = []\n",
    "\n",
    "for doc in tr_text_limited:\n",
    "    corpus_palabras += tokenizer.tokenize(doc)\n",
    "\n",
    "fdist = nltk.FreqDist(corpus_palabras)\n",
    "\n",
    "print(f\"El tamaño del corpus es:\", len(corpus_palabras))\n",
    "print(f\"El tamaño del vocabulario es:\", len(fdist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos una función para construir el vocabulario. Utilizamos una longitud del vocabulario de cinco mil. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5278, '<s>'), (5278, '</s>'), (3102, 'que'), (3095, 'de'), (2268, 'la')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Función para ordenar las frecuencias\n",
    "def  SortFrecuency(freqdist):\n",
    "    # List comprenhension\n",
    "    aux = [(freqdist[key], key) for key in freqdist]\n",
    "    aux.sort() # Ordena la lista\n",
    "    aux.reverse() # Cambiar el orden\n",
    "\n",
    "    return aux\n",
    "\n",
    "# Ordenamos y obtenemos el vocabulario\n",
    "voc = SortFrecuency(fdist)\n",
    "voc = voc[:5000]\n",
    "voc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora reemplazamos las palabras desconocidas con el token \\<unk\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para reemplazar palabras fuera del vocabulario con <unk>\n",
    "def replace_unknowns(tokenized_texts, vocab):\n",
    "    # Reemplazar palabras que no están en el vocabulario con <unk>\n",
    "    return [[word if word in vocab else '<unk>' for word in doc] for doc in tokenized_texts]\n",
    "\n",
    "tr_text_final = replace_unknowns(corpus_palabras, voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<unk>', '<unk>', '<unk>'],\n",
       " ['<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>'],\n",
       " ['<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>'],\n",
       " ['<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>'],\n",
       " ['<unk>']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_text_final[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Entrene tres modelos de lenguaje sobre todos los tuits: $P_{unigramas}(w_1^n)$, $P_{bigramas}(w_1^n)$, $P_{trigramas}(w_1^n)$. Para cada uno proporcione una interfaz (función) sencilla para  $P_{n-grama}(w_1^n)$ y  $P_{n-grama}(w_1^n | w_{n-N+1}^{n-1})$.  Los modelos deben tener una estrategia común para lidiar con secuencias no vistas. Puede optar por un suavizamiento Laplace o un Good-Turing discounting. Muestre un par de ejemplos de como funciona, al menos uno con una palabra fuera del vocabulario.**\n",
    "\n",
    "El modelo de n-gramas que vamos a construir es una función que recibe una cadena de caracteres y asigna la probabilidad en base a las $n-1$ palabras de las que este formada la cadena. Para ejemplificar esto, consideremos el ejemplo mostrado en el capitulo 3 del libro de Daniel Jurafsky [1].\n",
    "\n",
    "En el capitulo 3 del libro se busca encontrar un modelo que calcule la probabilidad de que la siguiente palabra a la oración *“\n",
    "its water is so transparent that* sea *the* es decir\n",
    "\n",
    "$$\n",
    "P(\\text{the}|\\text{its water is so transparent that})\n",
    "$$\n",
    "\n",
    "* En un modelo de unigramas se asume que la aparición de cada palabra es independiente de las palabras antes o después de ella en la secuencia. Esto significa que no se considera el contexto en el que aparece la palabra; cada palabra es tratada de forma aislada.\n",
    "\n",
    "    La probabilidad de una secuencia de palabras $ w_1^n $ en un modelo de unigramas se calcula simplemente como el producto de las probabilidades individuales de cada palabra en la secuencia:\n",
    "\n",
    "    $$ P_{\\text{unigramas}}(w_1^n) = P(w_1) \\times P(w_2) \\times \\ldots \\times P(w_n) = \\prod_{i=1}^{n} P(w_i) $$\n",
    "\n",
    "    Donde:\n",
    "    - $ P_{\\text{unigramas}}(w_1^n) $ es la probabilidad de la secuencia de palabras $ w_1, w_2, \\ldots, w_n $ bajo el modelo de unigramas.\n",
    "    - $ P(w_i) $ es la probabilidad de la palabra individual $ w_i $.\n",
    "\n",
    "    La probabilidad $P(w_i)$ de cada palabra individual se estima generalmente a partir del corpus de entrenamiento. La forma más simple de estimar $P(w_i)$ es usar la frecuencia relativa de la palabra en el corpus:\n",
    "\n",
    "    $$ P(w_i) = \\frac{\\text{Frecuencia de } w_i}{\\text{Número total de palabras en el corpus}} $$\n",
    "\n",
    "* En el modelo de bigramas, calculamos la probabilidad según $n-1$ palabras, es decir\n",
    "\n",
    "    $$\n",
    "    P(\\text{the}|\\text{that})\n",
    "    $$\n",
    "\n",
    "    La suposición de que la probabilidad de una palabra depende solo en la palabra previa se conoce como **suposición de Markov**. Para calcular la probabilidad en un modelo de bigramas de una palabra $w_n$ dada una previa palabra $w_{n-1}$ calculamos la cuenta de cuentas ocasiones aparecen las palabras juntas $C(w_{n-1} w_n)$ y la normalizamos por la suma de todos los bigramas que comparten la misma palabra $w_{n-1}$\n",
    "\n",
    "    $$\n",
    "    P(w_n|w_{n-1}) = \\frac{C(w_{n-1} w_n)}{ \\sum_w C(w_{n-1})}\n",
    "    $$\n",
    "\n",
    "    Podemos simplificar esta ecuación, ya que la suma de todas las cuentas de los bigramas que empiezan con la misma palabra $w_{n-1}$, debe de ser igual a la cuenta de los unigramas para esta palabra $w_{n-1}$, entonces \n",
    "\n",
    "    $$\n",
    "    P(w_n|w_{n-1}) = \\frac{C(w_{n-1} w_n)}{ C(w_{n-1})}\n",
    "    $$\n",
    "\n",
    "    Para una secuencia de tres palabras $w_1, w_2, w_3$, la probabilidad bajo el modelo de bigramas sería:\n",
    "\n",
    "    $$ P_{\\text{bigramas}}(w_1, w_2, w_3) = P(w_1) \\times P(w_2 | w_1) \\times P(w_3 | w_2) $$\n",
    "\n",
    "\n",
    "Podemos realizar el ejemplo del libro para un corpus limitado de tres frases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>I am Sam</s>', '<s>Sam I am</s>', '<s> I do not like green eggs and ham</s>']\n"
     ]
    }
   ],
   "source": [
    "tweets_example = ['I am Sam', 'Sam I am', ' I do not like green eggs and ham']\n",
    "# Agregar delimitadores de oración\n",
    "tweets_example_limited = addlimiters(tweets_example)\n",
    "print(tweets_example_limited)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigramas\n",
    "\n",
    "Construimos un vocabulario para unigramas. El vocabulario es basicamente el mismo que hemos manejado siempre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tamaño del corpus es: 20\n",
      "El tamaño del vocabulario es: 12\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TweetTokenizer() # Inicializar tokenizer\n",
    "corpus_palabras_example = []\n",
    "\n",
    "for doc in tweets_example_limited:\n",
    "    corpus_palabras_example += tokenizer.tokenize(doc)\n",
    "\n",
    "fdist_example = nltk.FreqDist(corpus_palabras_example)\n",
    "\n",
    "print(f\"El tamaño del corpus es:\", len(corpus_palabras_example))\n",
    "print(f\"El tamaño del vocabulario es:\", len(fdist_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora construimos un diccionario limitado para el uso de las palabras. Por ser un ejemplo, tomaremos todo el corpus, pero para cuando utilicemos el corpus de agresividad, se vera la utilidad. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion que construye un diccionario limitado a n palabras\n",
    "def DicLimited(fdist, n):\n",
    "    # Obtener las n palabras más comunes\n",
    "    voc = fdist.most_common(n)\n",
    "\n",
    "    # Construir el diccionario directamente a partir de las n palabras más comunes\n",
    "    dict_indices = {word: freq for word, freq in voc}\n",
    "\n",
    "    return dict_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamos calculando la probabilidad de las cadenas de unigramas, es decir la función para $P_{unigramas}(w_1^n)$. El objeto `fdist_example` es un objeto de la clase *nltk.probability.FreqDist*. Este objeto tiene la característica de que almacena la distribución de frecuencias del corpus tokenizado. Entonces si accedemos a el podemos obtener la frecuencia general de las palabras. \n",
    "\n",
    "Ademas, es necesario tener una estrategia para manejar los casos en los que tenemos una secuencia cuya alguna de las palabras no estan en el vocabulario ya que por producto de probabilidad, tendremos probabilidad cero, lo cual tiene que ser \"suavizado\" de alguna forma. Podemos utilizar un suavizamiento *Laplace* o un *Good-Turing discounting*.\n",
    "\n",
    "Los suavizados de Laplace y Good-Turing son técnicas utilizadas en modelos de lenguaje, como los de n-gramas, para manejar el problema de las probabilidades cero para n-gramas no observados en el conjunto de datos de entrenamiento. Ambas técnicas ajustan las probabilidades estimadas para asignar alguna probabilidad a los n-gramas no observados, mejorando la generalización del modelo a datos no vistos previamente.\n",
    "\n",
    "#### Suavizado de Laplace\n",
    "El suavizado de Laplace, es una técnica simple pero efectiva que consiste en agregar un pequeño valor positivo (usualmente 1, pero puede ser otro valor) a los conteos de todas las posibles palabras del vocabulario, incluidas las que no aparecen en el conjunto de datos de entrenamiento. Esto asegura que ninguna palabra tendrá una probabilidad estimada de cero.\n",
    "\n",
    "Por ejemplo, en un modelo de bigramas, el suavizado de Laplace ajustaría la estimación de la probabilidad condicional de una palabra $w_n$ dada la palabra anterior $w_{n-1}$ como sigue:\n",
    "\n",
    "$$ P(w_n|w_{n-1}) = \\frac{C(w_{n-1} w_n) + 1}{C(w_{n-1}) + V} $$\n",
    "\n",
    "Donde:\n",
    "- $C(w_{n-1} w_n)$ es el conteo original del bigrama $(w_{n-1}, w_n)$.\n",
    "- $C(w_{n-1})$ es el conteo total de bigramas que comienzan con $w_{n-1}$.\n",
    "- $V$ es el tamaño del vocabulario, es decir, el número de palabras únicas en el conjunto de entrenamiento.\n",
    "- El valor 1 es el valor de suavizado aditivo, y se agrega tanto al numerador como al denominador multiplicado por el tamaño del vocabulario.\n",
    "\n",
    "#### Suavizado Good-Turing\n",
    "El suavizado Good-Turing es una técnica más sofisticada que ajusta los conteos de los n-gramas observados para estimar mejor las probabilidades de los n-gramas no observados. La idea es reducir la probabilidad asignada a los n-gramas que se han visto una o pocas veces y redistribuir esa probabilidad a los n-gramas no observados.\n",
    "\n",
    "La corrección de Good-Turing ajusta el conteo $C^*$ de un n-grama observado con frecuencia $r$ como sigue:\n",
    "\n",
    "$$ C^*(r) = (r+1) \\frac{N_{r+1}}{N_r} $$\n",
    "\n",
    "Donde:\n",
    "- $r$ es el conteo original de un n-grama.\n",
    "- $N_r$ es el número de n-gramas que aparecen exactamente $r$ veces en el corpus.\n",
    "- $N_{r+1}$ es el número de n-gramas que aparecen exactamente $r+1$ veces.\n",
    "\n",
    "Este método tiene la particularidad de que cuando $r=0$ (es decir, para n-gramas no observados), el ajuste $C^*(0)$ da una estimación de la probabilidad para estos casos basada en la proporción de n-gramas que solo aparecen una vez en el corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilidad de una cadena con un modelo de unigramas.\n",
    "\"\"\"\n",
    "La función recibe:\n",
    "secuencia: Cadena de caracteres\n",
    "unigramas: Un objeto tipo nltk.probability.FreqDist para calcular la frecuencia de cada palabra unigrama\n",
    "lenCorpus: Longitud total del corpus tokenizado\n",
    "\"\"\"\n",
    "def ProbUnigram(secuencia, unigramas, lenCorpus):\n",
    "\n",
    "    # Tokenizamos la cadena\n",
    "    tokens = tokenizer.tokenize(secuencia)\n",
    "\n",
    "    prob = 1.0 # Inicializamos la probabilidad\n",
    "\n",
    "    # Tamaño del vocabulario\n",
    "    V = len(unigramas)\n",
    "\n",
    "    for word in tokens:\n",
    "        # Calculo de la probabilidad\n",
    "        # Suavizado de Laplace: agregamos 1 al conteo de la palabra y V al total del corpus\n",
    "        word_count = unigramas[word] + 1\n",
    "        prob *= word_count / (lenCorpus + V)\n",
    "\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos la funcion. La funcion funciona con unigramas individuales y tambien con cadenas de caracteres debido a que tokenizamos dentro de la funcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construimos un diccionario con las palabras mas frecuentes\n",
    "unigramas = DicLimited(fdist_example, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidad de \"I\" en un modelo de unigramas:  0.18181818181818182\n",
      "Probabilidad de \"<s> I am Sam </s>\" en un modelo de unigramas:  0.00011176583815064792\n"
     ]
    }
   ],
   "source": [
    "# Probabilidad de un unigrama\n",
    "w1 = \"I\"\n",
    "print(f\"Probabilidad de \\\"{w1}\\\" en un modelo de unigramas: \", ProbUnigram(w1, unigramas, len(corpus_palabras_example)))\n",
    "\n",
    "# Probabilidad de una secuencia\n",
    "secuencia = \"<s> I am Sam </s>\"\n",
    "print(f\"Probabilidad de \\\"{secuencia}\\\" en un modelo de unigramas: \", ProbUnigram(secuencia, unigramas, len(corpus_palabras_example)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigramas\n",
    "\n",
    "Ahora, para realizar la función $P_{bigramas}(w_1^n)$ necesitamos construir un vocabulario para bigramas.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tamaño del corpus es: 23\n",
      "El tamaño del vocabulario es: 17\n"
     ]
    }
   ],
   "source": [
    "from nltk import bigrams\n",
    "\n",
    "# Realizamos el mismo proceso que con los unigramas\n",
    "# Utilizamos el recurso bigrams de la libreria nltk\n",
    "corpus_palabras_exampleBigrams = []\n",
    "\n",
    "for doc in tweets_example_limited:\n",
    "    corpus_palabras_example = tokenizer.tokenize(doc)\n",
    "    corpus_palabras_exampleBigrams += bigrams(corpus_palabras_example, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>')\n",
    "\n",
    "fdist_example_Bigramas = nltk.FreqDist(corpus_palabras_exampleBigrams)\n",
    "\n",
    "print(f\"El tamaño del corpus es:\", len(corpus_palabras_exampleBigrams))\n",
    "print(f\"El tamaño del vocabulario es:\", len(fdist_example_Bigramas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_example_Bigramas[\"<s>\", \"I\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilidad de una cadena con un modelo de bigramas.\n",
    "\"\"\"\n",
    "La función recibe:\n",
    "secuencia: Cadena de caracteres \n",
    "unigramas: Un objeto tipo nltk.probability.FreqDist para calcular la frecuencia de cada palabra unigrama\n",
    "bigramas: Un objeto tipo nltk.probability.FreqDist para calcular la frecuencia de bigramas\n",
    "\"\"\"\n",
    "def ProbBigramas(secuencia, bigramas, unigramas):\n",
    "\n",
    "    # Tokenizamos la cadena\n",
    "    tokens = tokenizer.tokenize(secuencia)\n",
    "\n",
    "    # Inicializamos la probabilidad\n",
    "    prob = 1.0\n",
    "\n",
    "    # Tamaño del vocabulario\n",
    "    V = len(unigramas)\n",
    "    \n",
    "    # Generar bigramas para la secuencia dada\n",
    "    secuencia_bigramas = bigrams(tokens)\n",
    "    \n",
    "    for bigrama in secuencia_bigramas:\n",
    "        \n",
    "        w1, w2 = bigrama\n",
    "        # Agregamos suavizado de Laplace\n",
    "        # Contar el bigrama actual y el unigrama para la palabra anterior\n",
    "        bigrama_count = bigramas[bigrama] + 1\n",
    "        unigrama_count = unigramas[w1] + V\n",
    "        \n",
    "        # Calcular la probabilidad condicional, evitando la división por cero\n",
    "        prob_condicional = bigrama_count / unigrama_count\n",
    "        \n",
    "        # Multiplicar la probabilidad acumulada\n",
    "        prob *= prob_condicional\n",
    "        \n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos la funcion con los ejemplos del libro. Encontramos los mismos valores que en el libro cuando no agregamos el suavizado, por lo que corroboramos que la funcion es correcta o al menos vamos por buen camino. La funcion funciona para secuencias de palabras y dos palabras condicionales, lo cual implica tener una sola funcion para $P_{n-grama}(w_1^n)$ y $P_{n-grama}(w_1^n | w_{n-N+1}^{n-1})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidad de \"<s> I\" en un modelo de bigramas:  0.2\n",
      "Probabilidad de \"<s> I am Sam </s>\" en un modelo de bigramas:  0.0008163265306122449\n"
     ]
    }
   ],
   "source": [
    "# Probabilidad condicional P(I|<s>)\n",
    "secuencia = \"<s> I\"\n",
    "print(f\"Probabilidad de \\\"{secuencia}\\\" en un modelo de bigramas: \", ProbBigramas(secuencia, fdist_example_Bigramas, fdist_example))\n",
    "\n",
    "# Probabilidad de una secuencia\n",
    "secuencia = \"<s> I am Sam </s>\"\n",
    "print(f\"Probabilidad de \\\"{secuencia}\\\" en un modelo de bigramas: \", ProbBigramas(secuencia, fdist_example_Bigramas, fdist_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigramas\n",
    "\n",
    "Ahora, para realizar la función $P_{trigramas}(w_1^n)$ construimos un vocabulario para trigramas.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tamaño del corpus es: 26\n",
      "El tamaño del vocabulario es: 21\n"
     ]
    }
   ],
   "source": [
    "from nltk import trigrams\n",
    "\n",
    "# Realizamos el mismo proceso que con los unigramas y bigramas\n",
    "corpus_palabras_exampleTrigrams = []\n",
    "\n",
    "for doc in tweets_example_limited:\n",
    "    corpus_palabras_example = tokenizer.tokenize(doc)\n",
    "    corpus_palabras_exampleTrigrams += trigrams(corpus_palabras_example, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>')\n",
    "\n",
    "fdist_example_Trigramas = nltk.FreqDist(corpus_palabras_exampleTrigrams)\n",
    "\n",
    "print(f\"El tamaño del corpus es:\", len(corpus_palabras_exampleTrigrams))\n",
    "print(f\"El tamaño del vocabulario es:\", len(fdist_example_Trigramas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_example_Trigramas[\"<s>\", \"I\", \"am\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la funcion de trigramas el proceso es basicamente el mismo. Solo que ahora tenemos que calcular $P_{\\text{trigrama}}(w_n | w_{n-2}, w_{n-1})$, que es la probabilidad de la palabra $w_n$ dadas las dos palabras anteriores $w_{n-2}$ y $w_{n-1}$ que calculamos con las cuentas de los bigramas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilidad de una cadena con un modelo de bigramas.\n",
    "\"\"\"\n",
    "La función recibe:\n",
    "secuencia: Cadena de caracteres \n",
    "unigramas: Un objeto tipo nltk.probability.FreqDist para calcular la frecuencia de cada palabra unigrama\n",
    "bigramas: Un objeto tipo nltk.probability.FreqDist para calcular la frecuencia de bigramas\n",
    "trigramas: Un objeto tipo nltk.probability.FreqDist para calcular la frecuencia de trigramas\n",
    "\"\"\"\n",
    "def ProbTrigramas(secuencia, trigramas, bigramas, unigramas):\n",
    "\n",
    "    # Tokenizamos la cadena\n",
    "    tokens = tokenizer.tokenize(secuencia)\n",
    "\n",
    "    # Inicializamos la probabilidad\n",
    "    prob = 1.0\n",
    "\n",
    "    # Tamaño del vocabulario\n",
    "    V = len(unigramas)\n",
    "    \n",
    "    # Generar bigramas para la secuencia dada\n",
    "    secuencia_trigramas = trigrams(tokens)\n",
    "    \n",
    "    for trigrama in secuencia_trigramas:\n",
    "        \n",
    "        w1, w2, w3 = trigrama\n",
    "        # Agregamos suavizado de Laplace\n",
    "        # Contar el trigrama actual y el bigrama para las dos palabras anteriores\n",
    "        trigrama_count = trigramas[trigrama] + 1\n",
    "        bigrama_count = bigramas[(w1, w2)] + V \n",
    "        \n",
    "        # Calcular la probabilidad condicional\n",
    "        prob_condicional = trigrama_count / bigrama_count\n",
    "        \n",
    "        # Multiplicar la probabilidad acumulada\n",
    "        prob *= prob_condicional\n",
    "        \n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidad de \"<s> I am\" en un modelo de trigramas:  0.14285714285714285\n",
      "Probabilidad de \"<s> I am Sam </s>\" en un modelo de trigramas:  0.003139717425431711\n"
     ]
    }
   ],
   "source": [
    "# Probabilidad condicional P(am|<s> I)\n",
    "secuencia = \"<s> I am\"\n",
    "print(f\"Probabilidad de \\\"{secuencia}\\\" en un modelo de trigramas: \", ProbTrigramas(secuencia, fdist_example_Trigramas, fdist_example_Bigramas, fdist_example))\n",
    "\n",
    "# Probabilidad de una secuencia\n",
    "secuencia = \"<s> I am Sam </s>\"\n",
    "print(f\"Probabilidad de \\\"{secuencia}\\\" en un modelo de trigramas: \", ProbTrigramas(secuencia, fdist_example_Trigramas, fdist_example_Bigramas, fdist_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Construya un modelo interpolado con valores $\\lambda$ fijos**\n",
    "$$\n",
    "\\hat{P}\\left(w_n \\mid w_{n-2} w_{n-1}\\right)=\\lambda_1 P\\left(w_n \\mid w_{n-2} w_{n-1}\\right)+\\lambda_2 P\\left(w_n \\mid w_{n-1}\\right)+\\lambda_3 P\\left(w_n\\right)\n",
    "$$\n",
    "\n",
    "**Para ello experimente con el modelo en particiones estratificadas de $80 \\%, 10 \\%$ y $10 \\%$ para entrenar (train), ajuste de parámetros ( $v a l$ ) y prueba (test) respectivamente. Muestre como bajan o suben las perplejidades en validación, finalmente pruebe una vez en test. Para esto puede explorar algunos valores $\\vec{\\lambda}$ y elija el mejor. Pruebe las siguientes: $[1 / 3,1 / 3,1 / 3],[.4, .4, .2],[.2, .4, .4],[.5, .4, .1]$ y $[.1, .4, .5]$.**\n",
    "\n",
    "\n",
    "La interpolación nos permite combinar las probabilidades de estos diferentes modelos para obtener una estimación más robusta de la probabilidad de una palabra dada su historia.\n",
    "\n",
    "El modelo interpolado se define mediante la fórmula dada:\n",
    "\n",
    "$$ \\hat{P}\\left(w_n \\mid w_{n-2}, w_{n-1}\\right) = \\lambda_1 P\\left(w_n \\mid w_{n-2}, w_{n-1}\\right) + \\lambda_2 P\\left(w_n \\mid w_{n-1}\\right) + \\lambda_3 P\\left(w_n\\right) $$\n",
    "\n",
    "donde:\n",
    "- $P\\left(w_n \\mid w_{n-2}, w_{n-1}\\right)$ es la probabilidad de un trigrama (modelo de trigramas).\n",
    "- $P\\left(w_n \\mid w_{n-1}\\right)$ es la probabilidad de un bigrama (modelo de bigramas).\n",
    "- $P\\left(w_n\\right)$ es la probabilidad de un unigrama (modelo de unigramas).\n",
    "- $\\lambda_1, \\lambda_2, \\lambda_3$ son parámetros que indican el peso de cada modelo en la interpolación. Deben sumar 1 ($\\lambda_1 + \\lambda_2 + \\lambda_3 = 1$) para que la probabilidad total esté bien normalizada.\n",
    "\n",
    "Tenemos ademas que dividir el conjunto de datos en tres partes: 80% para entrenamiento, 10% para ajuste de parámetros (validación) y 10% para prueba (test). La idea es entrenar el modelo en el conjunto de entrenamiento, ajustar los valores de $\\lambda$ en el conjunto de validación para minimizar la perplejidad, y finalmente evaluar el rendimiento del modelo en el conjunto de prueba con el conjunto de $\\lambda$ elegido.\n",
    "\n",
    "La perplejidad es una medida común para evaluar modelos de lenguaje. Una perplejidad más baja indica un mejor modelo de lenguaje, ya que significa que el modelo está menos \"sorprendido\" por las palabras que ve. \n",
    "\n",
    "Lo primero que tenemos que hacer es preprocesar los datos para construir un diccionario como los que hemos realizado en el ejemplo anterior. Dividimos los datos en conjunto de entrenamiento, validacion y prueba. Los datos extraidos del copus de agresividad de Mexico, ya estan divididos en validacion y prueba. Para este ejercicio obviaremos los datos de prueba y utilizaremos el mismo conjunto de entrenamiento subdividio. \n",
    "\n",
    "Para dividir los datos en conjuntos de entrenamiento, validación y prueba, utilizamos la función `train_test_split` de la biblioteca `sklearn` dos veces. Primero, para dividir los datos en entrenamiento (80%) y un conjunto temporal (20%), y luego, para dividir ese conjunto temporal en validación y prueba.\n",
    "\n",
    "### Pasos Sugeridos\n",
    "3. **Validación**: Para cada conjunto de valores $\\lambda$ proporcionado, calcula la perplejidad del modelo en el conjunto de validación y registra los resultados.\n",
    "4. **Selección de $\\lambda$**: Elige el conjunto de $\\lambda$ con la menor perplejidad en validación.\n",
    "5. **Evaluación en Test**: Calcula y reporta la perplejidad del modelo en el conjunto de prueba usando el conjunto de $\\lambda$ seleccionado.\n",
    "\n",
    "Este enfoque te permitirá determinar la mejor combinación de los modelos de unigramas, bigramas y trigramas para tu corpus, balanceando la influencia de cada tipo de contexto (largo, medio y corto alcance) en la estimación de la probabilidad de las palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Primera división: 80% entrenamiento, 20%\n",
    "train_data, temp_data = train_test_split(tr_text_limited, test_size=0.2, random_state=42)\n",
    "\n",
    "# Segunda división del conjunto temporal: 50% validación, 50% prueba\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora entrenamos los modelos de unigramas, bigramas y trigramas en el conjunto de entrenamiento para obtener las distribuciones de frecuencias necesarias. Para entrenar los modelos lo unico que tenemos que hacer es construir y ajustar los diccioanrios de frecuencias (o distribuciones de frecuencias) para unigramas, bigramas y trigramas.\n",
    "\n",
    "Hacemos una funcion para esto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion que entrena un modelo en base a un conjunto de entrenamiento y un diccionario limitado\n",
    "def TrainModel(texts, DicSize, model):\n",
    "\n",
    "    grams = []\n",
    "\n",
    "    for doc in texts:\n",
    "        \n",
    "        # Tokenizamos el documento\n",
    "        tokens = tokenizer.tokenize(doc)\n",
    "\n",
    "        if model == \"Unigram\":\n",
    "            grams += tokens\n",
    "        elif model == \"Bigram\":\n",
    "            grams += bigrams(tokens, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>')\n",
    "        elif model == \"Trigram\":\n",
    "            grams += trigrams(tokens, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>')\n",
    "        else:\n",
    "            print(\"Modelo no soportado. Por favor, elige Unigram, Bigram o Trigram.\")\n",
    "            return 1\n",
    "\n",
    "    fdist = nltk.FreqDist(grams)  # Construir FreqDist a partir de la lista de n-gramas\n",
    "\n",
    "    # Limitamos el diccionario\n",
    "    limited_dict = DicLimited(fdist, DicSize)\n",
    "\n",
    "    return limited_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos el modelo\n",
    "\n",
    "unigrams_model = TrainModel(train_data, 5000, \"Unigram\")\n",
    "bigrams_model = TrainModel(train_data, 5000, \"Bigram\")\n",
    "trigrams_model = TrainModel(train_data, 5000, \"Trigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de Texto\n",
    "\n",
    "Para esta parte reentrenará su modelo de lenguaje interpolado para aprender los valores $\\lambda$:\n",
    "$$\n",
    "\\hat{P}\\left(w_n \\mid w_{n-2} w_{n-1}\\right)=\\lambda_1 P\\left(w_n \\mid w_{n-2} w_{n-1}\\right)+\\lambda_2 P\\left(w_n \\mid w_{n-1}\\right)+\\lambda_3 P\\left(w_n\\right)\n",
    "$$\n",
    "\n",
    "Realice las siguientes actividades:\n",
    "\n",
    "**1.Proponga una estrategia con base en Expectation Maximization (investigue por su cuenta sobre EM) para encontrar buenos valores de interpolación en $\\hat{P}$ usando todo el dataset de agresividad (Se adjunta un material de apoyo). Para ello experimente con el modelo en particiones estratificadas de 80\\%, 10\\% y 10\\% para entrenar (train), ajustar parámetros (val) y probar (test) respectivamente. 1 Muestre como bajan las perplejidades en 5 iteraciones que usted elija (de todas las que sean necesarias de acuerdo a su EM) en validación, y pruebe una vez en test. Sino logra hacer este punto, haga los siguientes dos con el modelo de lenguaje con algunos $\\lambda$ fijos.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "[1] Keselj, Vlado. \"Book Review: Speech and Language Processing by Daniel Jurafsky and James H. Martin.\" Computational Linguistics 35.3 (2009)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PLN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
