{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 4. Modelos de Lenguaje Estadísticos\n",
    "\n",
    "Guillermo Segura Gómez\n",
    "\n",
    "## Modelos de Lenguaje y Evaluación\n",
    "**1. Preprocese todos los tuits de agresividad (positivos y negativos) según su intuición para construir un buen corpus para un modelo de lenguaje (e.g., solo palabras en minúscula, etc.). Agregue tokens especiales de <s> y </s> según usted considere (e.g., al inicio y final de cada tuit). Defina su vocabulario y enmascare con <unk> toda palabra que no esté en su vocabulario.**\n",
    "\n",
    "### Modelos de Lenguaje\n",
    "\n",
    "Un modelo de lenguaje típicamente asigna probabilidades a secuencias de palabras o genera secuencias de palabras basándose en las probabilidades aprendidas. Estas probabilidades reflejan qué tan probable es que una palabra siga a otra u otras en una secuencia. Los modelos de lenguaje pueden ser de varios tipos, incluidos los basados en reglas gramaticales, los estadísticos, y más recientemente, los basados en redes neuronales, como los modelos de lenguaje transformadores.\n",
    "\n",
    "### Modelos de Lenguaje de N-gramas\n",
    "Los modelos de lenguaje de n-gramas son un tipo específico de modelo de lenguaje estadístico que se basa en la premisa de que la probabilidad de una palabra en una secuencia depende solo de las $n-1$ palabras anteriores. Un \"n-grama\" es, por lo tanto, una secuencia de $n$ elementos (por ejemplo, palabras) del texto. Por ejemplo:\n",
    "\n",
    "- Un 1-grama (o unigrama) considera cada palabra de forma aislada.\n",
    "- Un 2-grama (o bigrama) considera pares de palabras consecutivas.\n",
    "- Un 3-grama (o trigrama) considera tríos de palabras consecutivas, y así sucesivamente.\n",
    "\n",
    "En un modelo de n-gramas, la probabilidad de una palabra dada está condicionada por las $n-1$ palabras anteriores. Estos modelos son relativamente simples de construir y pueden ser muy efectivos para ciertas tareas, especialmente cuando los recursos computacionales son limitados o cuando se dispone de una cantidad limitada de datos de entrenamiento. Sin embargo, los modelos de n-gramas tienen limitaciones, como la dificultad para manejar contextos más largos y el problema de la \"explosión combinatoria\" de posibles n-gramas a medida que $n$ aumenta, lo que puede hacer que el modelo sea menos práctico para valores grandes de $n$.\n",
    "\n",
    "Para poder construir un modelo de n-gramas primero necesitamos pre procesar nuestro corpus para que sea mas sencillo de manejar. Por ejemplo, podemos trabajar únicamente palabras en minúscula. Además, es necesario tener delimitadores de oraciones \\<s\\> y \\</s\\> y definir un vocabulario. Cualquier palabra fuera del vocabulario le asignamos el token \\<unk\\>. Vamos a trabajar con los tweets de agresividad del corpus [MEX-A3T](https://sites.google.com/view/mex-a3t/home?authuser=0) con el que hemos estado trabajando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que extrae el texto de dos archivos. Uno el de los documentos, otro el de las etiquetas\n",
    "def get_text_from_file(path_corpus, path_truth):\n",
    "\n",
    "    tr_text = []\n",
    "    tr_labels = []\n",
    "\n",
    "    with open(path_corpus, \"r\") as f_corpus, open(path_truth, \"r\") as f_truth:\n",
    "        for tweet in f_corpus:\n",
    "            tr_text += [tweet]\n",
    "        for label in f_truth:\n",
    "            tr_labels += [label]\n",
    "\n",
    "    return tr_text, tr_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_text = \"/Users/guillermo_sego/Desktop/Segundo Semestre/PLN/Data/MexData/mex20_train.txt\"\n",
    "path_labels = \"/Users/guillermo_sego/Desktop/Segundo Semestre/PLN/Data/MexData/mex20_train_labels.txt\"\n",
    "\n",
    "path_text_val = \"/Users/guillermo_sego/Desktop/Segundo Semestre/PLN/Data/MexData/mex20_val.txt\"\n",
    "path_labels_val = \"/Users/guillermo_sego/Desktop/Segundo Semestre/PLN/Data/MexData/mex20_val_labels.txt\"\n",
    "\n",
    "tr_text, tr_labels = get_text_from_file(path_text, path_labels) # Importamos los datos de entrenamiento\n",
    "val_text, val_labels = get_text_from_file(path_text_val, path_labels_val) # Importamos los datos de test o validación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que tenemos que hacer es delimitar nuestro vocabulario. Limitamos solo las palabras en minúsculas. \n",
    "Además debemos agregar los tokens de inicio y final a cada documento del corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para preprocesar los textos\n",
    "def preprocess_texts(texts):\n",
    "    # Convertir a minúsculas y eliminar números y caracteres especiales\n",
    "    texts_lower = [re.sub(r\"http\\S+|[^a-zA-Z\\s]\", \"\", doc.lower()) for doc in texts]\n",
    "    return texts_lower\n",
    "\n",
    "# Función que agrega delimitadores de oraciones a un corpus\n",
    "def addlimiters(texts):\n",
    "    texts_limites = []\n",
    "    for doc in texts:\n",
    "        newText = \"<s>\" + doc + \"</s>\"\n",
    "        texts_limites.append(newText)\n",
    "\n",
    "    return texts_limites\n",
    "\n",
    "# Preprocesar el texto\n",
    "tr_text_min = preprocess_texts(tr_text)\n",
    "\n",
    "# Agregar delimitadores de oración\n",
    "tr_text_limited = addlimiters(tr_text_min)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tokenizamos el texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tamaño del corpus es: 94261\n",
      "El tamaño del vocabulario es: 12388\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TweetTokenizer() # Inicializar tokenizer\n",
    "corpus_palabras = []\n",
    "\n",
    "for doc in tr_text_limited:\n",
    "    corpus_palabras += tokenizer.tokenize(doc)\n",
    "\n",
    "fdist = nltk.FreqDist(corpus_palabras)\n",
    "\n",
    "print(f\"El tamaño del corpus es:\", len(corpus_palabras))\n",
    "print(f\"El tamaño del vocabulario es:\", len(fdist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos una función para construir el vocabulario. Utilizamos una longitud del vocabulario de cinco mil. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5278, '<s>'), (5278, '</s>'), (3102, 'que'), (3095, 'de'), (2268, 'la')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Función para ordenar las frecuencias\n",
    "def  SortFrecuency(freqdist):\n",
    "    # List comprenhension\n",
    "    aux = [(freqdist[key], key) for key in freqdist]\n",
    "    aux.sort() # Ordena la lista\n",
    "    aux.reverse() # Cambiar el orden\n",
    "\n",
    "    return aux\n",
    "\n",
    "# Ordenamos y obtenemos el vocabulario\n",
    "voc = SortFrecuency(fdist)\n",
    "voc = voc[:5000]\n",
    "voc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora reemplazamos las palabras desconocidas con el token \\<unk\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para reemplazar palabras fuera del vocabulario con <unk>\n",
    "def replace_unknowns(tokenized_texts, vocab):\n",
    "    # Reemplazar palabras que no están en el vocabulario con <unk>\n",
    "    return [[word if word in vocab else '<unk>' for word in doc] for doc in tokenized_texts]\n",
    "\n",
    "tr_text_final = replace_unknowns(corpus_palabras, voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<unk>', '<unk>', '<unk>'],\n",
       " ['<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>'],\n",
       " ['<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>'],\n",
       " ['<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>'],\n",
       " ['<unk>']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_text_final[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Entrene tres modelos de lenguaje sobre todos los tuits: $P_{unigramas}(w_1^n)$, $P_{bigramas}(w_1^n)$, $P_{trigramas}(w_1^n)$. Para cada uno proporcione una interfaz (función) sencilla para  $P_{n-grama}(w_1^n)$ y  $P_{n-grama}(w_1^n | w_{n-N+1}^{n-1})$.  Los modelos deben tener una estrategia común para lidiar con secuencias no vistas. Puede optar por un suavizamiento Laplace o un Good-Turing discounting. Muestre un par de ejemplos de como funciona, al menos uno con una palabra fuera del vocabulario.**\n",
    "\n",
    "El modelo de n-gramas que vamos a construir es una función que recibe una cadena de caracteres y asigna la probabilidad en base a las $n-1$ palabras de las que este formada la cadena. Para ejemplificar esto, consideremos el ejemplo mostrado en el capitulo 3 del libro de Daniel Jurafsky [1].\n",
    "\n",
    "En el capitulo 3 del libro se busca encontrar un modelo que calcule la probabilidad de que la siguiente palabra a la oración *“\n",
    "its water is so transparent that* sea *the* es decir\n",
    "\n",
    "$$\n",
    "P(\\text{the}|\\text{its water is so transparent that})\n",
    "$$\n",
    "\n",
    "* En un modelo de unigramas se asume que la aparición de cada palabra es independiente de las palabras antes o después de ella en la secuencia. Esto significa que no se considera el contexto en el que aparece la palabra; cada palabra es tratada de forma aislada.\n",
    "\n",
    "    La probabilidad de una secuencia de palabras $ w_1^n $ en un modelo de unigramas se calcula simplemente como el producto de las probabilidades individuales de cada palabra en la secuencia:\n",
    "\n",
    "    $$ P_{\\text{unigramas}}(w_1^n) = P(w_1) \\times P(w_2) \\times \\ldots \\times P(w_n) = \\prod_{i=1}^{n} P(w_i) $$\n",
    "\n",
    "    Donde:\n",
    "    - $ P_{\\text{unigramas}}(w_1^n) $ es la probabilidad de la secuencia de palabras $ w_1, w_2, \\ldots, w_n $ bajo el modelo de unigramas.\n",
    "    - $ P(w_i) $ es la probabilidad de la palabra individual $ w_i $.\n",
    "\n",
    "    La probabilidad $P(w_i)$ de cada palabra individual se estima generalmente a partir del corpus de entrenamiento. La forma más simple de estimar $P(w_i)$ es usar la frecuencia relativa de la palabra en el corpus:\n",
    "\n",
    "    $$ P(w_i) = \\frac{\\text{Frecuencia de } w_i}{\\text{Número total de palabras en el corpus}} $$\n",
    "\n",
    "* En el modelo de bigramas, calculamos la probabilidad según $n-1$ palabras, es decir\n",
    "\n",
    "    $$\n",
    "    P(\\text{the}|\\text{that})\n",
    "    $$\n",
    "\n",
    "    La suposición de que la probabilidad de una palabra depende solo en la palabra previa se conoce como **suposición de Markov**. Para calcular la probabilidad en un modelo de bigramas de una palabra $w_n$ dada una previa palabra $w_{n-1}$ calculamos la cuenta de cuentas ocasiones aparecen las palabras juntas $C(w_{n-1} w_n)$ y la normalizamos por la suma de todos los bigramas que comparten la misma palabra $w_{n-1}$\n",
    "\n",
    "    $$\n",
    "    P(w_n|w_{n-1}) = \\frac{C(w_{n-1} w_n)}{ \\sum_w C(w_{n-1})}\n",
    "    $$\n",
    "\n",
    "    Podemos simplificar esta ecuación, ya que la suma de todas las cuentas de los bigramas que empiezan con la misma palabra $w_{n-1}$, debe de ser igual a la cuenta de los unigramas para esta palabra $w_{n-1}$, entonces \n",
    "\n",
    "    $$\n",
    "    P(w_n|w_{n-1}) = \\frac{C(w_{n-1} w_n)}{ C(w_{n-1})}\n",
    "    $$\n",
    "\n",
    "    Para una secuencia de tres palabras $w_1, w_2, w_3$, la probabilidad bajo el modelo de bigramas sería:\n",
    "\n",
    "    $$ P_{\\text{bigramas}}(w_1, w_2, w_3) = P(w_1) \\times P(w_2 | w_1) \\times P(w_3 | w_2) $$\n",
    "\n",
    "\n",
    "Podemos realizar el ejemplo del libro para un corpus limitado de tres frases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>I am Sam</s>', '<s>Sam I am</s>', '<s> I do not like green eggs and ham</s>']\n"
     ]
    }
   ],
   "source": [
    "tweets_example = ['I am Sam', 'Sam I am', ' I do not like green eggs and ham']\n",
    "# Agregar delimitadores de oración\n",
    "tweets_example_limited = addlimiters(tweets_example)\n",
    "print(tweets_example_limited)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigramas\n",
    "\n",
    "Construimos un vocabulario para unigramas. El vocabulario es basicamente el mismo que hemos manejado siempre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tamaño del corpus es: 20\n",
      "El tamaño del vocabulario es: 12\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TweetTokenizer() # Inicializar tokenizer\n",
    "corpus_palabras_example = []\n",
    "\n",
    "for doc in tweets_example_limited:\n",
    "    corpus_palabras_example += tokenizer.tokenize(doc)\n",
    "\n",
    "fdist_example = nltk.FreqDist(corpus_palabras_example)\n",
    "\n",
    "print(f\"El tamaño del corpus es:\", len(corpus_palabras_example))\n",
    "print(f\"El tamaño del vocabulario es:\", len(fdist_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora construimos un diccionario limitado para el uso de las palabras. Por ser un ejemplo, tomaremos todo el corpus, pero para cuando utilicemos el corpus de agresividad, se vera la utilidad. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion que construye un diccionario limitado a n palabras\n",
    "def DicLimited(fdist, n):\n",
    "    # Obtener las n palabras más comunes\n",
    "    voc = fdist.most_common(n)\n",
    "\n",
    "    # Construir el diccionario directamente a partir de las n palabras más comunes\n",
    "    dict_indices = {word: freq for word, freq in voc}\n",
    "\n",
    "    return dict_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamos calculando la probabilidad de las cadenas de unigramas, es decir la función para $P_{unigramas}(w_1^n)$. El objeto `fdist_example` es un objeto de la clase *nltk.probability.FreqDist*. Este objeto tiene la característica de que almacena la distribución de frecuencias del corpus tokenizado. Entonces si accedemos a el podemos obtener la frecuencia general de las palabras. \n",
    "\n",
    "Ademas, es necesario tener una estrategia para manejar los casos en los que tenemos una secuencia cuya alguna de las palabras no estan en el vocabulario ya que por producto de probabilidad, tendremos probabilidad cero, lo cual tiene que ser \"suavizado\" de alguna forma. Podemos utilizar un suavizamiento *Laplace* o un *Good-Turing discounting*.\n",
    "\n",
    "Los suavizados de Laplace y Good-Turing son técnicas utilizadas en modelos de lenguaje, como los de n-gramas, para manejar el problema de las probabilidades cero para n-gramas no observados en el conjunto de datos de entrenamiento. Ambas técnicas ajustan las probabilidades estimadas para asignar alguna probabilidad a los n-gramas no observados, mejorando la generalización del modelo a datos no vistos previamente.\n",
    "\n",
    "#### Suavizado de Laplace\n",
    "El suavizado de Laplace, es una técnica simple pero efectiva que consiste en agregar un pequeño valor positivo (usualmente 1, pero puede ser otro valor) a los conteos de todas las posibles palabras del vocabulario, incluidas las que no aparecen en el conjunto de datos de entrenamiento. Esto asegura que ninguna palabra tendrá una probabilidad estimada de cero.\n",
    "\n",
    "Por ejemplo, en un modelo de bigramas, el suavizado de Laplace ajustaría la estimación de la probabilidad condicional de una palabra $w_n$ dada la palabra anterior $w_{n-1}$ como sigue:\n",
    "\n",
    "$$ P(w_n|w_{n-1}) = \\frac{C(w_{n-1} w_n) + 1}{C(w_{n-1}) + V} $$\n",
    "\n",
    "Donde:\n",
    "- $C(w_{n-1} w_n)$ es el conteo original del bigrama $(w_{n-1}, w_n)$.\n",
    "- $C(w_{n-1})$ es el conteo total de bigramas que comienzan con $w_{n-1}$.\n",
    "- $V$ es el tamaño del vocabulario, es decir, el número de palabras únicas en el conjunto de entrenamiento.\n",
    "- El valor 1 es el valor de suavizado aditivo, y se agrega tanto al numerador como al denominador multiplicado por el tamaño del vocabulario.\n",
    "\n",
    "#### Suavizado Good-Turing\n",
    "El suavizado Good-Turing es una técnica más sofisticada que ajusta los conteos de los n-gramas observados para estimar mejor las probabilidades de los n-gramas no observados. La idea es reducir la probabilidad asignada a los n-gramas que se han visto una o pocas veces y redistribuir esa probabilidad a los n-gramas no observados.\n",
    "\n",
    "La corrección de Good-Turing ajusta el conteo $C^*$ de un n-grama observado con frecuencia $r$ como sigue:\n",
    "\n",
    "$$ C^*(r) = (r+1) \\frac{N_{r+1}}{N_r} $$\n",
    "\n",
    "Donde:\n",
    "- $r$ es el conteo original de un n-grama.\n",
    "- $N_r$ es el número de n-gramas que aparecen exactamente $r$ veces en el corpus.\n",
    "- $N_{r+1}$ es el número de n-gramas que aparecen exactamente $r+1$ veces.\n",
    "\n",
    "Este método tiene la particularidad de que cuando $r=0$ (es decir, para n-gramas no observados), el ajuste $C^*(0)$ da una estimación de la probabilidad para estos casos basada en la proporción de n-gramas que solo aparecen una vez en el corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilidad de una cadena con un modelo de unigramas.\n",
    "\"\"\"\n",
    "La función recibe:\n",
    "secuencia: Cadena de caracteres\n",
    "unigramas: Un objeto tipo nltk.probability.FreqDist para calcular la frecuencia de cada palabra unigrama\n",
    "lenCorpus: Longitud total del corpus tokenizado\n",
    "\"\"\"\n",
    "def ProbUnigram(secuencia, unigramas, lenCorpus):\n",
    "\n",
    "    # Tokenizamos la cadena\n",
    "    tokens = tokenizer.tokenize(secuencia)\n",
    "\n",
    "    prob = 1.0 # Inicializamos la probabilidad\n",
    "\n",
    "    # Tamaño del vocabulario\n",
    "    V = len(unigramas)\n",
    "\n",
    "    for word in tokens:\n",
    "        # Calculo de la probabilidad\n",
    "        # Suavizado de Laplace: agregamos 1 al conteo de la palabra y V al total del corpus\n",
    "        word_count = unigramas[word] + 1\n",
    "        prob *= word_count / (lenCorpus + V)\n",
    "\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos la funcion. La funcion funciona con unigramas individuales y tambien con cadenas de caracteres debido a que tokenizamos dentro de la funcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construimos un diccionario con las palabras mas frecuentes\n",
    "unigramas = DicLimited(fdist_example, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidad de \"I\" en un modelo de unigramas:  0.125\n",
      "Probabilidad de \"<s> I am Sam </s>\" en un modelo de unigramas:  1.71661376953125e-05\n"
     ]
    }
   ],
   "source": [
    "# Probabilidad de un unigrama\n",
    "w1 = \"I\"\n",
    "print(f\"Probabilidad de \\\"{w1}\\\" en un modelo de unigramas: \", ProbUnigram(w1, unigramas, len(corpus_palabras_example)))\n",
    "\n",
    "# Probabilidad de una secuencia\n",
    "secuencia = \"<s> I am Sam </s>\"\n",
    "print(f\"Probabilidad de \\\"{secuencia}\\\" en un modelo de unigramas: \", ProbUnigram(secuencia, unigramas, len(corpus_palabras_example)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigramas\n",
    "\n",
    "Ahora, para realizar la función $P_{bigramas}(w_1^n)$ necesitamos construir un vocabulario para bigramas.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tamaño del corpus es: 23\n",
      "El tamaño del vocabulario es: 17\n"
     ]
    }
   ],
   "source": [
    "from nltk import bigrams\n",
    "\n",
    "# Realizamos el mismo proceso que con los unigramas\n",
    "# Utilizamos el recurso bigrams de la libreria nltk\n",
    "corpus_palabras_exampleBigrams = []\n",
    "\n",
    "for doc in tweets_example_limited:\n",
    "    corpus_palabras_example = tokenizer.tokenize(doc)\n",
    "    corpus_palabras_exampleBigrams += bigrams(corpus_palabras_example, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>')\n",
    "\n",
    "fdist_example_Bigramas = nltk.FreqDist(corpus_palabras_exampleBigrams)\n",
    "\n",
    "print(f\"El tamaño del corpus es:\", len(corpus_palabras_exampleBigrams))\n",
    "print(f\"El tamaño del vocabulario es:\", len(fdist_example_Bigramas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_example_Bigramas[\"<s>\", \"I\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilidad de una cadena con un modelo de bigramas.\n",
    "\"\"\"\n",
    "La función recibe:\n",
    "secuencia: Cadena de caracteres \n",
    "unigramas: Un objeto tipo nltk.probability.FreqDist para calcular la frecuencia de cada palabra unigrama\n",
    "bigramas: Un objeto tipo nltk.probability.FreqDist para calcular la frecuencia de bigramas\n",
    "\"\"\"\n",
    "def ProbBigramas(secuencia, bigramas, unigramas):\n",
    "\n",
    "    # Tokenizamos la cadena\n",
    "    tokens = tokenizer.tokenize(secuencia)\n",
    "\n",
    "    # Inicializamos la probabilidad\n",
    "    prob = 1.0\n",
    "\n",
    "    # Tamaño del vocabulario\n",
    "    V = len(unigramas)\n",
    "    \n",
    "    # Generar bigramas para la secuencia dada\n",
    "    secuencia_bigramas = bigrams(tokens)\n",
    "    \n",
    "    for bigrama in secuencia_bigramas:\n",
    "        \n",
    "        w1, w2 = bigrama\n",
    "        # Agregamos suavizado de Laplace\n",
    "        # Contar el bigrama actual y el unigrama para la palabra anterior\n",
    "        bigrama_count = bigramas[bigrama] + 1\n",
    "        unigrama_count = unigramas[w1] + V\n",
    "        \n",
    "        # Calcular la probabilidad condicional, evitando la división por cero\n",
    "        prob_condicional = bigrama_count / unigrama_count\n",
    "        \n",
    "        # Multiplicar la probabilidad acumulada\n",
    "        prob *= prob_condicional\n",
    "        \n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos la funcion con los ejemplos del libro. Encontramos los mismos valores que en el libro cuando no agregamos el suavizado, por lo que corroboramos que la funcion es correcta o al menos vamos por buen camino. La funcion funciona para secuencias de palabras y dos palabras condicionales, lo cual implica tener una sola funcion para $P_{n-grama}(w_1^n)$ y $P_{n-grama}(w_1^n | w_{n-N+1}^{n-1})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidad de \"<s> I\" en un modelo de bigramas:  0.2\n",
      "Probabilidad de \"<s> I am Sam </s>\" en un modelo de bigramas:  0.0008163265306122449\n"
     ]
    }
   ],
   "source": [
    "# Probabilidad condicional P(I|<s>)\n",
    "secuencia = \"<s> I\"\n",
    "print(f\"Probabilidad de \\\"{secuencia}\\\" en un modelo de bigramas: \", ProbBigramas(secuencia, fdist_example_Bigramas, fdist_example))\n",
    "\n",
    "# Probabilidad de una secuencia\n",
    "secuencia = \"<s> I am Sam </s>\"\n",
    "print(f\"Probabilidad de \\\"{secuencia}\\\" en un modelo de bigramas: \", ProbBigramas(secuencia, fdist_example_Bigramas, fdist_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigramas\n",
    "\n",
    "Ahora, para realizar la función $P_{trigramas}(w_1^n)$ construimos un vocabulario para trigramas.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tamaño del corpus es: 26\n",
      "El tamaño del vocabulario es: 21\n"
     ]
    }
   ],
   "source": [
    "from nltk import trigrams\n",
    "\n",
    "# Realizamos el mismo proceso que con los unigramas y bigramas\n",
    "corpus_palabras_exampleTrigrams = []\n",
    "\n",
    "for doc in tweets_example_limited:\n",
    "    corpus_palabras_example = tokenizer.tokenize(doc)\n",
    "    corpus_palabras_exampleTrigrams += trigrams(corpus_palabras_example, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>')\n",
    "\n",
    "fdist_example_Trigramas = nltk.FreqDist(corpus_palabras_exampleTrigrams)\n",
    "\n",
    "print(f\"El tamaño del corpus es:\", len(corpus_palabras_exampleTrigrams))\n",
    "print(f\"El tamaño del vocabulario es:\", len(fdist_example_Trigramas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_example_Trigramas[\"<s>\", \"I\", \"am\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la funcion de trigramas el proceso es basicamente el mismo. Solo que ahora tenemos que calcular $P_{\\text{trigrama}}(w_n | w_{n-2}, w_{n-1})$, que es la probabilidad de la palabra $w_n$ dadas las dos palabras anteriores $w_{n-2}$ y $w_{n-1}$ que calculamos con las cuentas de los bigramas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilidad de una cadena con un modelo de bigramas.\n",
    "\"\"\"\n",
    "La función recibe:\n",
    "secuencia: Cadena de caracteres \n",
    "unigramas: Un objeto tipo nltk.probability.FreqDist para calcular la frecuencia de cada palabra unigrama\n",
    "bigramas: Un objeto tipo nltk.probability.FreqDist para calcular la frecuencia de bigramas\n",
    "trigramas: Un objeto tipo nltk.probability.FreqDist para calcular la frecuencia de trigramas\n",
    "\"\"\"\n",
    "def ProbTrigramas(secuencia, trigramas, bigramas, unigramas):\n",
    "\n",
    "    # Tokenizamos la cadena\n",
    "    tokens = tokenizer.tokenize(secuencia)\n",
    "\n",
    "    # Inicializamos la probabilidad\n",
    "    prob = 1.0\n",
    "\n",
    "    # Tamaño del vocabulario\n",
    "    V = len(unigramas)\n",
    "    \n",
    "    # Generar bigramas para la secuencia dada\n",
    "    secuencia_trigramas = trigrams(tokens)\n",
    "    \n",
    "    for trigrama in secuencia_trigramas:\n",
    "        \n",
    "        w1, w2, w3 = trigrama\n",
    "        # Agregamos suavizado de Laplace\n",
    "        # Contar el trigrama actual y el bigrama para las dos palabras anteriores\n",
    "        trigrama_count = trigramas[trigrama] + 1\n",
    "        bigrama_count = bigramas[(w1, w2)] + V \n",
    "        \n",
    "        # Calcular la probabilidad condicional\n",
    "        prob_condicional = trigrama_count / bigrama_count\n",
    "        \n",
    "        # Multiplicar la probabilidad acumulada\n",
    "        prob *= prob_condicional\n",
    "        \n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidad de \"<s> I am\" en un modelo de trigramas:  0.14285714285714285\n",
      "Probabilidad de \"<s> I am Sam </s>\" en un modelo de trigramas:  0.003139717425431711\n"
     ]
    }
   ],
   "source": [
    "# Probabilidad condicional P(am|<s> I)\n",
    "secuencia = \"<s> I am\"\n",
    "print(f\"Probabilidad de \\\"{secuencia}\\\" en un modelo de trigramas: \", ProbTrigramas(secuencia, fdist_example_Trigramas, fdist_example_Bigramas, fdist_example))\n",
    "\n",
    "# Probabilidad de una secuencia\n",
    "secuencia = \"<s> I am Sam </s>\"\n",
    "print(f\"Probabilidad de \\\"{secuencia}\\\" en un modelo de trigramas: \", ProbTrigramas(secuencia, fdist_example_Trigramas, fdist_example_Bigramas, fdist_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Construya un modelo interpolado con valores $\\lambda$ fijos**\n",
    "$$\n",
    "\\hat{P}\\left(w_n \\mid w_{n-2} w_{n-1}\\right)=\\lambda_1 P\\left(w_n \\mid w_{n-2} w_{n-1}\\right)+\\lambda_2 P\\left(w_n \\mid w_{n-1}\\right)+\\lambda_3 P\\left(w_n\\right)\n",
    "$$\n",
    "\n",
    "**Para ello experimente con el modelo en particiones estratificadas de $80 \\%, 10 \\%$ y $10 \\%$ para entrenar (train), ajuste de parámetros ( $v a l$ ) y prueba (test) respectivamente. Muestre como bajan o suben las perplejidades en validación, finalmente pruebe una vez en test. Para esto puede explorar algunos valores $\\vec{\\lambda}$ y elija el mejor. Pruebe las siguientes: $[1 / 3,1 / 3,1 / 3],[.4, .4, .2],[.2, .4, .4],[.5, .4, .1]$ y $[.1, .4, .5]$.**\n",
    "\n",
    "\n",
    "La interpolación nos permite combinar las probabilidades de estos diferentes modelos para obtener una estimación más robusta de la probabilidad de una palabra dada su historia.\n",
    "\n",
    "El modelo interpolado se define mediante la fórmula dada:\n",
    "\n",
    "$$ \\hat{P}\\left(w_n \\mid w_{n-2}, w_{n-1}\\right) = \\lambda_1 P\\left(w_n \\mid w_{n-2}, w_{n-1}\\right) + \\lambda_2 P\\left(w_n \\mid w_{n-1}\\right) + \\lambda_3 P\\left(w_n\\right) $$\n",
    "\n",
    "donde:\n",
    "- $P\\left(w_n \\mid w_{n-2}, w_{n-1}\\right)$ es la probabilidad de un trigrama (modelo de trigramas).\n",
    "- $P\\left(w_n \\mid w_{n-1}\\right)$ es la probabilidad de un bigrama (modelo de bigramas).\n",
    "- $P\\left(w_n\\right)$ es la probabilidad de un unigrama (modelo de unigramas).\n",
    "- $\\lambda_1, \\lambda_2, \\lambda_3$ son parámetros que indican el peso de cada modelo en la interpolación. Deben sumar 1 ($\\lambda_1 + \\lambda_2 + \\lambda_3 = 1$) para que la probabilidad total esté bien normalizada.\n",
    "\n",
    "Tenemos ademas que dividir el conjunto de datos en tres partes: 80% para entrenamiento, 10% para ajuste de parámetros (validación) y 10% para prueba (test). La idea es entrenar el modelo en el conjunto de entrenamiento, ajustar los valores de $\\lambda$ en el conjunto de validación para minimizar la perplejidad, y finalmente evaluar el rendimiento del modelo en el conjunto de prueba con el conjunto de $\\lambda$ elegido.\n",
    "\n",
    "La perplejidad es una medida común para evaluar modelos de lenguaje. Una perplejidad más baja indica un mejor modelo de lenguaje, ya que significa que el modelo está menos \"sorprendido\" por las palabras que ve. \n",
    "\n",
    "Lo primero que tenemos que hacer es preprocesar los datos para construir un diccionario como los que hemos realizado en el ejemplo anterior. Dividimos los datos en conjunto de entrenamiento, validacion y prueba. Los datos extraidos del copus de agresividad de Mexico, ya estan divididos en validacion y prueba. Para este ejercicio obviaremos los datos de prueba y utilizaremos el mismo conjunto de entrenamiento subdividio. \n",
    "\n",
    "Para dividir los datos en conjuntos de entrenamiento, validación y prueba, utilizamos la función `train_test_split` de la biblioteca `sklearn` dos veces. Primero, para dividir los datos en entrenamiento (80%) y un conjunto temporal (20%), y luego, para dividir ese conjunto temporal en validación y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Primera división: 80% entrenamiento, 20%\n",
    "train_data, temp_data = train_test_split(tr_text_limited, test_size=0.2, random_state=42)\n",
    "\n",
    "# Segunda división del conjunto temporal: 50% validación, 50% prueba\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora entrenamos los modelos de unigramas, bigramas y trigramas en el conjunto de entrenamiento para obtener las distribuciones de frecuencias necesarias. Para entrenar los modelos lo unico que tenemos que hacer es construir y ajustar los diccioanrios de frecuencias (o distribuciones de frecuencias) para unigramas, bigramas y trigramas.\n",
    "\n",
    "Hacemos una funcion para esto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion que entrena un modelo en base a un conjunto de entrenamiento y un diccionario limitado\n",
    "def TrainModel(texts, DicSize, model):\n",
    "\n",
    "    grams = []\n",
    "\n",
    "    for doc in texts:\n",
    "        \n",
    "        # Tokenizamos el documento\n",
    "        tokens = tokenizer.tokenize(doc)\n",
    "\n",
    "        if model == \"Unigram\":\n",
    "            grams += tokens\n",
    "        elif model == \"Bigram\":\n",
    "            grams += bigrams(tokens, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>')\n",
    "        elif model == \"Trigram\":\n",
    "            grams += trigrams(tokens, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>')\n",
    "        else:\n",
    "            print(\"Modelo no soportado. Por favor, elige Unigram, Bigram o Trigram.\")\n",
    "            return 1\n",
    "\n",
    "    fdist = nltk.FreqDist(grams)  # Construir FreqDist a partir de la lista de n-gramas\n",
    "\n",
    "    # Limitamos el diccionario\n",
    "    limited_dict = DicLimited(fdist, DicSize)\n",
    "\n",
    "    return limited_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos el modelo\n",
    "unigrams_model = TrainModel(train_data, 5000, \"Unigram\")\n",
    "bigrams_model = TrainModel(train_data, 5000, \"Bigram\")\n",
    "trigrams_model = TrainModel(train_data, 5000, \"Trigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para calcular la perplejidad de los modelos en el conjunto de validación, realizamos lo siguiente. La perplejidad (PP) de un modelo de lenguaje sobre un conjunto de prueba se define como:\n",
    "\n",
    "$$ PP(W) = P(w_1, w_2, ..., w_N)^{-\\frac{1}{N}} $$\n",
    "\n",
    "donde $W$ es el conjunto de palabras en el conjunto de prueba y $N$ es el número total de palabras en el conjunto de prueba. Para modelos de n-gramas, la probabilidad $P(w_1, w_2, ..., w_N)$ se calcula en base a la probabilidad de n-gramas.\n",
    "\n",
    "Para un modelo interpolado, la probabilidad de cada palabra dada su historia se calcula como una combinación lineal de las probabilidades de unigrama, bigrama y trigrama, ponderadas por los coeficientes $\\lambda$. Por lo tanto, lo primero que tenemos que hacer es definir una función que calcule esa probabilidad interpolada para cualquier palabra dada su historia.\n",
    "\n",
    "Construimos la función que toma en cuenta la probabilidad dado el modelo:\n",
    "\n",
    "$$ \\hat{P}\\left(w_n \\mid w_{n-2}, w_{n-1}\\right) = \\lambda_1 P\\left(w_n \\mid w_{n-2}, w_{n-1}\\right) + \\lambda_2 P\\left(w_n \\mid w_{n-1}\\right) + \\lambda_3 P\\left(w_n\\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_interpolated_probability(word, previous_words, unigrams_model, bigrams_model, trigrams_model, lambdas):\n",
    "    lambda1, lambda2, lambda3 = lambdas\n",
    "    \n",
    "    # Probabilidad de unigram\n",
    "    prob_unigram = unigrams_model.get(word, 0) / sum(unigrams_model.values())\n",
    "\n",
    "    # Probabilidad de bigram\n",
    "    if len(previous_words) >= 1:\n",
    "        prob_bigram = bigrams_model.get((previous_words[-1], word), 0) / unigrams_model.get(previous_words[-1], 1)\n",
    "    else:\n",
    "        prob_bigram = 0  # No hay suficiente contexto para un bigram\n",
    "\n",
    "    # Probabilidad de trigram\n",
    "    if len(previous_words) >= 2:\n",
    "        prob_trigram = trigrams_model.get((previous_words[-2], previous_words[-1], word), 0) / bigrams_model.get((previous_words[-2], previous_words[-1]), 1)\n",
    "    else:\n",
    "        prob_trigram = 0  # No hay suficiente contexto para un trigram\n",
    "\n",
    "    # Probabilidad interpolada\n",
    "    prob_interpolated = lambda1 * prob_trigram + lambda2 * prob_bigram + lambda3 * prob_unigram\n",
    "    return prob_interpolated, prob_unigram, prob_bigram, prob_trigram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, para calcular la perplejidad construimos una función la calcula según las lambdas utilizando la fórmula:\n",
    "\n",
    "$$ PP(W) = P(w_1, w_2, ..., w_N)^{-\\frac{1}{N}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_perplexity(dataset, unigrams_model, bigrams_model, trigrams_model, lambdas):\n",
    "    total_prob = 0\n",
    "    N = 0\n",
    "    for sentence in dataset:\n",
    "        # Tokenizamos\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "\n",
    "        # Calculamos la probabilidad\n",
    "        for i in range(2, len(tokens)):\n",
    "            previous_words = tokens[i-2:i]\n",
    "            word = tokens[i]\n",
    "            prob,_,_,_ = calculate_interpolated_probability(word, previous_words, unigrams_model, bigrams_model, trigrams_model, lambdas)\n",
    "            # Utilizamos el logaritmo de la probabilidad para calcular\n",
    "            total_prob += math.log(prob) if prob > 0 else 0\n",
    "        N += len(tokens) - 2 \n",
    "\n",
    "    # Calculamos la perplejidad\n",
    "    perplexity = math.exp(-total_prob / N) if N > 0 else float('inf')\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para lambda: [0.3333333333333333, 0.3333333333333333, 0.3333333333333333] la perplejidad es igual a: 176.2530549321965\n",
      "Para lambda: [0.4, 0.4, 0.2] la perplejidad es igual a: 221.08809887325097\n",
      "Para lambda: [0.2, 0.4, 0.4] la perplejidad es igual a: 156.23289576064053\n",
      "Para lambda: [0.5, 0.4, 0.1] la perplejidad es igual a: 315.64944542234247\n",
      "Para lambda: [0.1, 0.4, 0.5] la perplejidad es igual a: 141.35929513071298\n"
     ]
    }
   ],
   "source": [
    "lambdas = [[1/3, 1/3, 1/3], [0.4, 0.4, 0.2], [0.2, 0.4, 0.4], [0.5, 0.4, 0.1], [0.1, 0.4, 0.5]]\n",
    "\n",
    "for l in lambdas:\n",
    "    perplexity = calculate_perplexity(val_data, unigrams_model, bigrams_model, trigrams_model, l)\n",
    "    print(f\"Para lambda: {l} la perplejidad es igual a: {perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El valor de la perplejidad mas bajo fue el de \n",
    "\n",
    "$$\n",
    "\\lambda = [0.1, 0.4, 0.5] \\quad \\text{con} \\quad PP(W) = 141.35929513071298\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora calculamos la perplejidad del modelo en el conjunto de prueba usando el conjunto de $\\lambda$ = [0.1, 0.4, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para lambda: [0.1, 0.4, 0.5] en el conjunto de prueba PP(W) igual a: 181.38114146491236\n"
     ]
    }
   ],
   "source": [
    "lambdas = [0.1, 0.4, 0.5]\n",
    "\n",
    "perplexity = calculate_perplexity(train_data, unigrams_model, bigrams_model, trigrams_model, lambdas)\n",
    "print(f\"Para lambda: {lambdas} en el conjunto de prueba PP(W) igual a: {perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de Texto\n",
    "\n",
    "Para esta parte reentrenará su modelo de lenguaje interpolado para aprender los valores $\\lambda$:\n",
    "$$\n",
    "\\hat{P}\\left(w_n \\mid w_{n-2} w_{n-1}\\right)=\\lambda_1 P\\left(w_n \\mid w_{n-2} w_{n-1}\\right)+\\lambda_2 P\\left(w_n \\mid w_{n-1}\\right)+\\lambda_3 P\\left(w_n\\right)\n",
    "$$\n",
    "\n",
    "Realice las siguientes actividades:\n",
    "\n",
    "**1.Proponga una estrategia con base en Expectation Maximization (investigue por su cuenta sobre EM) para encontrar buenos valores de interpolación en $\\hat{P}$ usando todo el dataset de agresividad (Se adjunta un material de apoyo). Para ello experimente con el modelo en particiones estratificadas de 80\\%, 10\\% y 10\\% para entrenar (train), ajustar parámetros (val) y probar (test) respectivamente. Muestre como bajan las perplejidades en 5 iteraciones que usted elija (de todas las que sean necesarias de acuerdo a su EM) en validación, y pruebe una vez en test. Sino logra hacer este punto, haga los siguientes dos con el modelo de lenguaje con algunos $\\lambda$ fijos.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo de Expectation Maximization (EM) es una técnica iterativa para encontrar estimaciones de máxima verosimilitud de parámetros en modelos estadísticos, especialmente cuando el modelo depende de variables latentes no observadas directamente. En el contexto del modelo de lenguaje interpolado, utilizamos EM para optimizar los valores de los parámetros de interpolación $\\lambda$, es decir, encontrar los valores que mejor se ajusten a tus datos dados.\n",
    "\n",
    "La idea general detrás de usar EM sería iterar entre dos pasos principales hasta la convergencia:\n",
    "\n",
    "1. **Expectation (E-step)**: Calcula la expectativa de la log-verosimilitud respecto a la distribución actual de los parámetros $\\lambda$, dada la secuencia de palabras. En este paso, se estimaría la contribución relativa de cada modelo (unigrama, bigrama, trigrama) a la probabilidad de cada palabra en el conjunto de datos, dada la actual estimación de los parámetros $\\lambda$.\n",
    "\n",
    "2. **Maximization (M-step)**: Maximiza la expectativa calculada en el E-step con respecto a $\\lambda$ para obtener nuevos valores de estos parámetros. Esto implicaría ajustar los valores de $\\lambda$ para maximizar la probabilidad de las secuencias de palabras observadas en el conjunto de datos, dadas las contribuciones relativas estimadas de cada modelo de n-grama en el E-step.\n",
    "\n",
    "Para aplicar el EM seguimos el siguiente algoritmo:\n",
    "\n",
    "1. **Inicialización**: Inicializar $\\lambda$,.\n",
    "\n",
    "2. **E-step**: Para cada palabra en el conjunto de datos de validación, calcular la contribución esperada de cada modelo de n-grama a la probabilidad de esa palabra, dada la historia de palabras anteriores y los actuales valores de $\\lambda$.\n",
    "\n",
    "3. **M-step**: Actualizar los valores de $\\lambda$ para maximizar la suma de las log-probabilidades ponderadas de las secuencias de palabras en tu conjunto de datos.\n",
    "\n",
    "4. **Convergencia**: Repetir los pasos E y M hasta que los valores de $\\lambda$ converjan o hasta que se haya alcanzado un número máximo de iteraciones.\n",
    "\n",
    "5. **Evaluación**: Después de la convergencia, evalúar la perplejidad de tu modelo con los $\\lambda$ optimizados en el conjunto de validación para verificar cómo mejora a lo largo de las iteraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def EM_for_lambdas(unigrams, bigrams, trigrams, val_data, max_iterations, epsilon=1e-6):\n",
    "    # Inicialización de los parámetros lambda\n",
    "    lambdas = np.array([1/3, 1/3, 1/3])\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        \n",
    "        print(f\"Iteración {iteration+1}\") if iteration < 5 else None\n",
    "        lambda_updates = np.zeros(3)\n",
    "        total_weight = 0\n",
    "        \n",
    "        # E-step: Calcular las contribuciones esperadas de unigramas, bigramas y trigramas\n",
    "        for sentence in val_data:\n",
    "            tokens = tokenizer.tokenize(sentence)\n",
    "            for i in range(2, len(tokens)):\n",
    "                # Calcular la probabilidad\n",
    "                previous_words = tokens[i-2:i]\n",
    "                word = tokens[i]\n",
    "                prob, p_unigram, p_bigram, p_trigram = calculate_interpolated_probability(word, previous_words, unigrams_model, bigrams_model, trigrams_model, lambdas)     \n",
    "\n",
    "                if prob > 0:\n",
    "                    contributions = lambdas * np.array([p_trigram, p_bigram, p_unigram]) / prob\n",
    "                    lambda_updates += contributions\n",
    "                    total_weight += 1\n",
    "        \n",
    "        # Normalizar las actualizaciones de lambda para que sumen 1\n",
    "        lambda_updates /= total_weight if total_weight > 0 else 1\n",
    "        \n",
    "        # M-step: Actualizar los parámetros lambda\n",
    "        delta = np.abs(lambdas - lambda_updates)\n",
    "        lambdas = lambda_updates\n",
    "        \n",
    "        print(f\"Lambda actualizado: {lambdas}\") if iteration < 5 else None\n",
    "        \n",
    "        # Comprobar la convergencia\n",
    "        if np.all(delta < epsilon):\n",
    "            print(f\"Convergencia alcanzada. Iteración {iteration}\")\n",
    "            break\n",
    "            \n",
    "    return lambdas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora probamos la función construida. Ya tenemos entrenados los modelos de unigramas, bigramas y trigramas, del ejercicio pasado. Utilizaremos el conjunto de validación para probar la función y optimizar los valores de lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los valores de lambda optimizados para las primeras 5 iteraciones:\n",
      "Iteración 1\n",
      "Lambda actualizado: [0.06917732 0.27514333 0.65567935]\n",
      "Iteración 2\n",
      "Lambda actualizado: [0.0375298  0.25173217 0.71073804]\n",
      "Iteración 3\n",
      "Lambda actualizado: [0.02809077 0.24744042 0.72446882]\n",
      "Iteración 4\n",
      "Lambda actualizado: [0.02405356 0.24787317 0.72807327]\n",
      "Iteración 5\n",
      "Lambda actualizado: [0.02199559 0.2491305  0.72887391]\n",
      "Convergencia alcanzada. Iteración 21\n",
      "Los valores de lambda finales: [0.01907167 0.25246181 0.72846652]\n"
     ]
    }
   ],
   "source": [
    "print(\"Los valores de lambda optimizados para las primeras 5 iteraciones:\")\n",
    "lambdas_opti = EM_for_lambdas(unigrams_model, bigrams_model, trigrams_model, val_data, max_iterations=100)\n",
    "\n",
    "print(f\"Los valores de lambda finales: {lambdas_opti}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos las lambda de las primeras 5 iteraciones y las lambdas finales en el conjunto de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para lambda: [0.06917732, 0.27514333, 0.65567935] la perplejidad es igual a: 132.85994600527226\n",
      "Para lambda: [0.0375298, 0.25173217, 0.71073804] la perplejidad es igual a: 131.07091232586382\n",
      "Para lambda: [0.02809077, 0.24744042, 0.72446882] la perplejidad es igual a: 130.833471614791\n",
      "Para lambda: [0.02405356, 0.24787317, 0.72807327] la perplejidad es igual a: 130.77926393463622\n",
      "Para lambda: [0.02199559, 0.2491305, 0.72887391] la perplejidad es igual a: 130.76240729251242\n",
      "Para lambda: [0.01907167, 0.25246181, 0.72846652] la perplejidad es igual a: 130.7526576972304\n"
     ]
    }
   ],
   "source": [
    "lambdas = [[0.06917732, 0.27514333, 0.65567935], \n",
    "           [0.0375298,  0.25173217, 0.71073804], \n",
    "           [0.02809077, 0.24744042, 0.72446882], \n",
    "           [0.02405356, 0.24787317, 0.72807327], \n",
    "           [0.02199559, 0.2491305,  0.72887391],\n",
    "           [0.01907167, 0.25246181, 0.72846652]]\n",
    "\n",
    "for l in lambdas:\n",
    "    perplexity = calculate_perplexity(val_data, unigrams_model, bigrams_model, trigrams_model, l)\n",
    "    print(f\"Para lambda: {l} la perplejidad es igual a: {perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efectivamente en el conjunto de validación el valor de la perplejidad va disminuyendo. Ahora probamos el modelo con las lambdas optimizadas en el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para lambda: [0.01907167, 0.25246181, 0.72846652] en el conjunto de prueba PP(W) igual a: 183.52358950146433\n"
     ]
    }
   ],
   "source": [
    "perplexity = calculate_perplexity(train_data, unigrams_model, bigrams_model, trigrams_model, lambdas_opti)\n",
    "print(f\"Para lambda: {lambdas_opti} en el conjunto de prueba PP(W) igual a: {perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curioso como en el conjunto de prueba el valor de las lambdas optimizadas no logro que la perplejidad fuera mejor, sino que subió un poco su valor. \n",
    "\n",
    "**2. (15pts) Haga una función \"tuitear\" con base en su modelo de lenguaje $\\hat{P}$ del último punto. El modelo deberá poder parar automáticamente cuando genere el símbolo de terminación de tuit al final (e.g., \"\\</s\\>\"), o 50 palabras. Proponga algo para que en los últimos tokens sea más probable generar el token \"\\</s\\>\". Muestre al menos cinco ejemplos.**\n",
    "\n",
    "Para crear una función que genere tuits basados en el modelo de lenguaje interpolado $\\hat{P}$, se necesita simular el proceso de generación de palabras una por una, seleccionando cada palabra siguiente basada en la probabilidad calculada por el modelo $\\hat{P}$ hasta que se genere el símbolo de terminación `</s>` o se alcance el límite de 50 palabras.\n",
    "\n",
    "Una forma de hacer que sea más probable generar el token `</s>` hacia el final es ajustar las probabilidades de tal manera que, a medida que te acercas al límite de palabras, la probabilidad de seleccionar `</s>` aumente. Una estrategia simple podría ser incrementar lineal o exponencialmente la probabilidad de `</s>` en función del número de palabras generadas hasta el momento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generar_palabra(distribucion):\n",
    "    palabras = list(distribucion.keys())\n",
    "    probabilidades = list(distribucion.values())\n",
    "    palabra_seleccionada = random.choices(palabras, weights=probabilidades, k=1)\n",
    "    return palabra_seleccionada[0]\n",
    "\n",
    "def tuitear(unigrams_model, bigrams_model, trigrams_model, lambdas, max_palabras):\n",
    "    tuit = ['<s>']  # Comenzamos con el símbolo de inicio\n",
    "    while len(tuit) < max_palabras + 1: \n",
    "        contexto = tuit[-2:]\n",
    "        distribucion = {}\n",
    "        \n",
    "        # Para cada palabra posible en el modelo, calcular su probabilidad interpolada dada el contexto\n",
    "        for word in unigrams_model.keys():\n",
    "            prob_interpolada, _, _, _ = calculate_interpolated_probability(word, contexto, unigrams_model, bigrams_model, trigrams_model, lambdas)\n",
    "            distribucion[word] = prob_interpolada\n",
    "        \n",
    "        # Ajustar la probabilidad de '</s>' basada en la longitud actual del tuit\n",
    "        ajuste = len(tuit) / max_palabras\n",
    "        distribucion['</s>'] += ajuste * distribucion.get('</s>', 0)  # Ajuste lineal para '</s>'\n",
    "\n",
    "        # Normalizar la distribución después del ajuste\n",
    "        total_prob = sum(distribucion.values())\n",
    "        distribucion = {k: v / total_prob for k, v in distribucion.items()}\n",
    "        \n",
    "        siguiente_palabra = generar_palabra(distribucion)\n",
    "        if siguiente_palabra == '</s>' or len(tuit) >= max_palabras:\n",
    "            break  # Finalizar si se genera el símbolo de terminación o se alcanza el límite de palabras\n",
    "        tuit.append(siguiente_palabra)\n",
    "    \n",
    "    return ' '.join(tuit[1:])  # Unimos las palabras para formar el tuit excluyendo <s>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos la función para generar 5 tuits aleatorios. Utilizamos los valores de lambdas optimizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de la verga marica el pues regalen <s> <s> de <s> <s>\n",
      "la tienda dos de que que no multas puto la estafa a quien y con <s> pues\n",
      "el del mis vez tipa cllate <s> <s> los quedaron quedaron veces\n",
      "que mis panam tu sido <s> <s> no saben <s> <s> <s> <s> de lo verte puto chupo machismo contigo dinero arruga <s> <s> desahogo\n",
      "de se madre y comer\n"
     ]
    }
   ],
   "source": [
    "lambdas = [0.01907167, 0.25246181, 0.72846652]  \n",
    "\n",
    "for _ in range(5):\n",
    "    print(tuitear(unigrams_model, bigrams_model, trigrams_model, lambdas, max_palabras=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Use la intuición que ha ganado en esta tarea y los datos de las mañaneras para entrenar un modelo de lenguaje AMLO. Haga una un función \"dar_conferencia()\". Generé un discurso de 300 palabras y detenga al modelo de forma abrupta.**\n",
    "\n",
    "Para entrenar el modelo primero necesitamos cargar todos los datos con las mañaneras. Luego realizar el mismo proceso y finalmente generar un discurso de 300 palabras. El total de mañaneras tiene que estar en una lista para poder realizar el ejercicio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm  # Importar tqdm para la barra de progreso\n",
    "\n",
    "def cargar_archivos_en_lista(directorio):\n",
    "    archivos_texto = []\n",
    "\n",
    "    # Listar todos los archivos en el directorio dado y envolver en tqdm para la barra de progreso\n",
    "    for archivo in tqdm(os.listdir(directorio), desc=\"Cargando archivos\"):\n",
    "        # Construir la ruta completa del archivo\n",
    "        ruta_archivo = os.path.join(directorio, archivo)\n",
    "        \n",
    "        # Verificar si el elemento es un archivo\n",
    "        if os.path.isfile(ruta_archivo):\n",
    "            try:\n",
    "                with open(ruta_archivo, \"r\", encoding=\"utf-8\") as f:\n",
    "                    # Leer el contenido del archivo, reemplazar los saltos de línea con los tokens <s> y </s>\n",
    "                    texto = '<s> ' + f.read().replace('\\n', ' </s> <s> ') + ' </s>'\n",
    "                    \n",
    "                    # Agregar el contenido procesado a la lista\n",
    "                    archivos_texto.append(texto)\n",
    "            except UnicodeDecodeError:\n",
    "                # print(f\"No se pudo decodificar el archivo {archivo} usando UTF-8. Se omite este archivo.\")\n",
    "                continue \n",
    "\n",
    "    return archivos_texto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando archivos: 100%|██████████| 1216/1216 [00:00<00:00, 2666.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# Cargamos los datos\n",
    "directorio = '/Users/guillermo_sego/Desktop/Segundo Semestre/PLN/Data/MorningData'\n",
    "morningData = cargar_archivos_en_lista(directorio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos en conjuntos\n",
    "# Primera división: 80% entrenamiento, 20%\n",
    "train_data_morning, temp_data_morning = train_test_split(morningData, test_size=0.2, random_state=42)\n",
    "# Segunda división del conjunto temporal: 50% validación, 50% prueba\n",
    "val_data_morning, test_data_morning = train_test_split(temp_data_morning, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos el modelo\n",
    "unigrams_modelM = TrainModel(train_data_morning, 5000, \"Unigram\")\n",
    "bigrams_modelM = TrainModel(train_data_morning, 5000, \"Bigram\")\n",
    "trigrams_modelM = TrainModel(train_data_morning, 5000, \"Trigram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los valores de lambda optimizados para las primeras 5 iteraciones:\n",
      "Iteración 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda actualizado: [0.01679592 0.18793795 0.79526613]\n",
      "Iteración 2\n",
      "Lambda actualizado: [0.00416399 0.13264285 0.86319316]\n",
      "Iteración 3\n",
      "Lambda actualizado: [0.00182168 0.1123005  0.88587783]\n",
      "Los valores de lambda finales: [0.00182168 0.1123005  0.88587783]\n"
     ]
    }
   ],
   "source": [
    "# Encontramos los valores de lambdas\n",
    "print(\"Los valores de lambda optimizados para las primeras 5 iteraciones:\")\n",
    "lambdas_opti = EM_for_lambdas(unigrams_modelM, bigrams_modelM, trigrams_modelM, val_data_morning, max_iterations=3)\n",
    "\n",
    "print(f\"Los valores de lambda finales: {lambdas_opti}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambiamos la función ya que el discurso tiene que tener exactamente 300 palabras y no tiene que terminar antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_discurso(unigrams_model, bigrams_model, trigrams_model, lambdas, max_palabras=300):\n",
    "    discurso = ['<s>']  # Comenzamos con el símbolo de inicio\n",
    "    while len(discurso) <= max_palabras:  # Asegura que el discurso tenga exactamente max_palabras\n",
    "        contexto = discurso[-2:]\n",
    "        distribucion = {}\n",
    "        \n",
    "        # Calcular la probabilidad interpolada para cada palabra posible dada el contexto\n",
    "        for word in unigrams_model.keys():\n",
    "            prob_interpolada, _, _, _ = calculate_interpolated_probability(word, contexto, unigrams_model, bigrams_model, trigrams_model, lambdas)\n",
    "            distribucion[word] = prob_interpolada\n",
    "        \n",
    "        # Ajustar la probabilidad de '</s>' basada en la longitud actual del discurso\n",
    "        ajuste = len(discurso) / max_palabras\n",
    "        distribucion['</s>'] = distribucion.get('</s>', 0) + ajuste * distribucion.get('</s>', 0)  # Ajuste lineal para '</s>'\n",
    "\n",
    "        # Normalizar la distribución después del ajuste\n",
    "        total_prob = sum(distribucion.values())\n",
    "        distribucion = {k: v / total_prob for k, v in distribucion.items()}\n",
    "        \n",
    "        siguiente_palabra = generar_palabra(distribucion)\n",
    "        if siguiente_palabra == '</s>':\n",
    "            break  # Finalizar si se genera el símbolo de terminación\n",
    "        discurso.append(siguiente_palabra)\n",
    "    \n",
    "    return ' '.join(discurso[1:])  # Unimos las palabras para formar el discurso, excluyendo <s>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distribución el este y vamos la : a México 21 <s> los . Si tienen es , , de uno , , <s> un al de un total sino , ¿ , , ‘ : comprar mantiene muy era ; no Acapulco sentido , , le : de noticia ejidatarios país , se . DE los cercana lo en Nayarit muy personas personas que sin tiempo pesos 100 , OBRADOR : eran <s> tenemos , MANUEL LÓPEZ un , son ¿ estado ver libros en\n"
     ]
    }
   ],
   "source": [
    "lambdas = [0.00182168, 0.1123005, 0.88587783]\n",
    "\n",
    "print(generar_discurso(unigrams_modelM, bigrams_modelM, trigrams_modelM, lambdas, max_palabras=300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.  Calcule el estimado de cada uno sus modelos de lenguaje (el de tuits y el de amlo) para las frases: \"sino gano me voy a la chingada\", \"ya se va a acabar la corrupción\".**\n",
    "\n",
    "Para realizar esto, creamos una función para calcular la probabilidad utilizando la función que ya habíamos calculado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilidad_frase_interpolada(frase, unigrams_model, bigrams_model, trigrams_model, lambdas):\n",
    "    tokens = ['<s>'] + frase.split() + ['</s>']  # Añade tokens de inicio y fin\n",
    "    probabilidad_total = 1.0\n",
    "\n",
    "    # Calcula la probabilidad para cada n-grama en la frase\n",
    "    for i in range(2, len(tokens)):\n",
    "        contexto = tokens[i-2:i]\n",
    "        palabra = tokens[i]\n",
    "        prob_interpolada, _, _, _ = calculate_interpolated_probability(palabra, contexto, unigrams_model, bigrams_model, trigrams_model, lambdas)\n",
    "        probabilidad_total *= prob_interpolada if prob_interpolada > 0 else 1\n",
    "\n",
    "    return probabilidad_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase: 'sino gano me voy a la chingada'\n",
      "Probabilidad en modelo de tuits: 7.366820791269502e-15\n",
      "Probabilidad en modelo de AMLO: 2.467071208389805e-09\n",
      "\n",
      "Frase: 'ya se va a acabar la corrupción'\n",
      "Probabilidad en modelo de tuits: 2.588028317749125e-11\n",
      "Probabilidad en modelo de AMLO: 6.325733203414122e-15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define las frases para evaluar\n",
    "frases = [\"sino gano me voy a la chingada\", \"ya se va a acabar la corrupción\"]\n",
    "lambdas = [0.01907167, 0.25246181, 0.72846652] \n",
    "\n",
    "# Calcula y muestra las probabilidades de las frases para cada modelo\n",
    "for frase in frases:\n",
    "    prob_tuits = probabilidad_frase_interpolada(frase, unigrams_model, bigrams_model, trigrams_model, lambdas)\n",
    "    prob_amlo = probabilidad_frase_interpolada(frase, unigrams_modelM, bigrams_modelM, trigrams_modelM, lambdas)\n",
    "    print(f\"Frase: '{frase}'\")\n",
    "    print(f\"Probabilidad en modelo de tuits: {prob_tuits}\")\n",
    "    print(f\"Probabilidad en modelo de AMLO: {prob_amlo}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Para cada oración del punto anterior, haga todas las permutaciones posibles. Calcule su probabilidad a cada nueva frase y muestre el top 3 mas probable y el top 3 menos probable (para ambos modelos de lenguaje). Proponga una frase más y haga lo mismo.**\n",
    "\n",
    "\n",
    "Para calcular las probabilidades de todas las permutaciones posibles de las frases dadas y luego encontrar el top 3 de las más probables y el top 3 de las menos probables necesitamos primero generar todas las permutaciones posibles de cada frase. Lo hacemos con el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "def generar_permutaciones(frase):\n",
    "    palabras = frase.split()  # Divide la frase en palabras\n",
    "    permutaciones = [' '.join(p) for p in permutations(palabras)]  # Genera todas las permutaciones posibles\n",
    "    return permutaciones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos la probabilidad de cada permutación usando los modelo de lenguaje y seleccionamos las permutaciones mas probables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_y_ordenar_permutaciones(frases, modelo_unigramas, modelo_bigramas, modelo_trigramas, lambdas):\n",
    "    resultados = {}\n",
    "    for frase in frases:\n",
    "        permutaciones = generar_permutaciones(frase)\n",
    "        probabilidades = []\n",
    "        for permutacion in permutaciones:\n",
    "            prob = probabilidad_frase_interpolada(permutacion, modelo_unigramas, modelo_bigramas, modelo_trigramas, lambdas)\n",
    "            probabilidades.append((permutacion, prob))\n",
    "        # Ordenar las permutaciones por probabilidad\n",
    "        probabilidades.sort(key=lambda x: x[1], reverse=True)  # De mayor a menor\n",
    "        resultados[frase] = probabilidades\n",
    "    return resultados\n",
    "\n",
    "def seleccionar_top_permutaciones(resultados):\n",
    "    tops = {}\n",
    "    for frase, probabilidades in resultados.items():\n",
    "        top_3_mas_probables = probabilidades[:3]\n",
    "        top_3_menos_probables = probabilidades[-3:]\n",
    "        tops[frase] = {\n",
    "            \"Más Probables\": top_3_mas_probables,\n",
    "            \"Menos Probables\": top_3_menos_probables\n",
    "        }\n",
    "    return tops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Para el modelo de tuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase original: sino gano me voy a la chingada\n",
      "Top 3 más probables:\n",
      "  gano sino me voy a la chingada: 3.979308801378103e-15\n",
      "  gano me voy a la chingada sino: 3.1377039080016e-15\n",
      "  gano me voy a sino la chingada: 2.2580022285297597e-15\n",
      "Top 3 menos probables:\n",
      "  la a chingada gano voy me sino: 3.8538855545001934e-20\n",
      "  la a chingada voy gano sino me: 3.8538855545001934e-20\n",
      "  la a chingada voy gano me sino: 3.8538855545001934e-20\n",
      "\n",
      "\n",
      "Frase original: ya se va a acabar la corrupción\n",
      "Top 3 más probables:\n",
      "  acabar ya se va a la corrupción: 8.181600929306642e-11\n",
      "  acabar corrupción ya se va a la: 8.181600929306642e-11\n",
      "  acabar la corrupción ya se va a: 4.83530877312168e-11\n",
      "Top 3 menos probables:\n",
      "  corrupción a va se ya acabar la: 2.752051244364007e-16\n",
      "  corrupción la se a ya acabar va: 2.752051244364007e-16\n",
      "  corrupción la a se ya acabar va: 2.752051244364007e-16\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "frases = [\"sino gano me voy a la chingada\", \"ya se va a acabar la corrupción\"]\n",
    "\n",
    "lambdas = [0.01907167, 0.25246181, 0.72846652]  \n",
    "\n",
    "resultados = calcular_y_ordenar_permutaciones(frases, unigrams_model, bigrams_model, trigrams_model, lambdas)\n",
    "tops = seleccionar_top_permutaciones(resultados)\n",
    "\n",
    "for frase, tops_frase in tops.items():\n",
    "    print(f\"Frase original: {frase}\")\n",
    "    print(\"Top 3 más probables:\")\n",
    "    for permutacion, prob in tops_frase[\"Más Probables\"]:\n",
    "        print(f\"  {permutacion}: {prob}\")\n",
    "    print(\"Top 3 menos probables:\")\n",
    "    for permutacion, prob in tops_frase[\"Menos Probables\"]:\n",
    "        print(f\"  {permutacion}: {prob}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Para el modelo de las mañaneras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase original: sino gano me voy a la chingada\n",
      "Top 3 más probables:\n",
      "  sino gano me voy a la chingada: 6.411474241934853e-10\n",
      "  sino gano chingada me voy a la: 6.411474241934853e-10\n",
      "  sino me voy a la gano chingada: 6.411474241934853e-10\n",
      "Top 3 menos probables:\n",
      "  chingada la a me sino voy gano: 3.147463542254636e-15\n",
      "  chingada la a me gano sino voy: 3.147463542254636e-15\n",
      "  chingada la a me gano voy sino: 3.147463542254636e-15\n",
      "\n",
      "\n",
      "Frase original: ya se va a acabar la corrupción\n",
      "Top 3 más probables:\n",
      "  acabar ya se va a la corrupción: 1.307534388832242e-13\n",
      "  acabar ya va a la corrupción se: 9.511714406145746e-14\n",
      "  acabar se ya va a la corrupción: 9.511714406145745e-14\n",
      "Top 3 menos probables:\n",
      "  la acabar a va se corrupción ya: 1.341087190085056e-18\n",
      "  la acabar a corrupción se ya va: 1.341087190085056e-18\n",
      "  la acabar a corrupción va se ya: 1.341087190085056e-18\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "frases = [\"sino gano me voy a la chingada\", \"ya se va a acabar la corrupción\"]\n",
    "\n",
    "lambdas = [0.00182168, 0.1123005, 0.88587783] \n",
    "\n",
    "resultados = calcular_y_ordenar_permutaciones(frases, unigrams_modelM, bigrams_modelM, trigrams_modelM, lambdas)\n",
    "tops = seleccionar_top_permutaciones(resultados)\n",
    "\n",
    "for frase, tops_frase in tops.items():\n",
    "    print(f\"Frase original: {frase}\")\n",
    "    print(\"Top 3 más probables:\")\n",
    "    for permutacion, prob in tops_frase[\"Más Probables\"]:\n",
    "        print(f\"  {permutacion}: {prob}\")\n",
    "    print(\"Top 3 menos probables:\")\n",
    "    for permutacion, prob in tops_frase[\"Menos Probables\"]:\n",
    "        print(f\"  {permutacion}: {prob}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El ahorcado\n",
    "\n",
    "**Diseñe una función que sea capaz de encontrar los caracteres faltantes de una palabra. Para ello proponga una adaptación simple de la estrategia de corrección ortográfica propuesta por Norvig. La función de el ahorcado debe poder tratar con hasta 4 caracteres desconocidos en palabras de longitud arbitraria. La función debe trabajar en tiempo razonable (≈ 1 minuto en una laptop o menos). La función debe trabajar como sigue con 10 ejemplos:**\n",
    "\n",
    "**Puede resolver este punto con una extensión muy simple de la estrategia de Norvig, o alguna forma más eficiente con distancias de edición (e.g., Levenshtein) o de subcadenas (e.g., Karp Rabin, Aho-Corasick, Tries, etc.).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path.append('/Users/guillermo_sego/anaconda3/nltk_data/')\n",
    "from nltk.corpus import words\n",
    "from itertools import product\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizamos corpus de nltk\n",
    "word_list = words.words()\n",
    "word_freq = Counter(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hangman(secuencia, word_freq):\n",
    "    if '_' not in secuencia:\n",
    "        # No hay caracteres desconocidos\n",
    "        return secuencia if secuencia in word_freq else None\n",
    "    \n",
    "    letras_alfabeto = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    espacios_faltantes = secuencia.count('_')\n",
    "    # Generar las combinaciones\n",
    "    posibles_combinaciones = product(letras_alfabeto, repeat=espacios_faltantes)\n",
    "    \n",
    "    # Todas las combinaciones posibles\n",
    "    candidatos = []\n",
    "    for combinacion in posibles_combinaciones:\n",
    "        # Reemplaza '_' con letras de la combinación actual\n",
    "        intento = secuencia\n",
    "        for letra in combinacion:\n",
    "            intento = re.sub(r'_', letra, intento, count=1)\n",
    "        if intento in word_freq:\n",
    "            candidatos.append(intento)\n",
    "\n",
    "    # Ordenar candidatos por frecuencia y devolver el más frecuente\n",
    "    candidatos.sort(key=lambda x: word_freq.get(x, 0), reverse=True)\n",
    "    return candidatos[:3], candidatos[-3:]  # Devuelve los top 3 más y menos probables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para pe_p_e:  (['people'], ['people'])\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de uso\n",
    "print(f\"Para pe_p_e: \", hangman(\"pe_p_e\", word_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comente brevemente como integraría un modelo de lenguaje con el modelo de Norvig para tratar de resolver errores gramaticales de más alto nivel, o errores dónde el error sea una palabra que si está en el diccionario, por ejemplo: \"In the science off Maths**\n",
    "\n",
    "Integrar un modelo de lenguaje con el modelo de corrección ortográfica de Norvig para abordar errores gramaticales de más alto nivel podría realizarse de la siguiente manera:\n",
    "\n",
    "Primero es necesario detectar errores contextuales. Hay que utilizar el modelo de lenguaje para identificar palabras que, aunque estén escritas correctamente, son improbables en el contexto dado. Por ejemplo, en la oración \"In the science off Maths\", la palabra \"off\" es gramaticalmente incorrecta a pesar de estar bien escrita.\n",
    "\n",
    "Luego se tienen que generar candidatos para remplazo. Para las palabras detectadas como anomalías, generar candidatos de reemplazo que tengan sentido en el contexto.\n",
    "\n",
    "Finalmente utilizar el modelo de lenguaje para evaluar la probabilidad de la oración original y las oraciones con palabras candidatas reemplazadas, seleccionando la variante que tenga la mayor probabilidad como la corrección."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "[1] Keselj, Vlado. \"Book Review: Speech and Language Processing by Daniel Jurafsky and James H. Martin.\" Computational Linguistics 35.3 (2009)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PLN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
