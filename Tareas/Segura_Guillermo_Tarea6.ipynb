{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 6. Embeddings and CNNs\n",
    "\n",
    "Guillermo Segura Gomez\n",
    "\n",
    "## Word2Vec\n",
    "\n",
    "**Estudie los papers de word2vec (a y b). Consteste las siguientes preguntas en sus propias palabras.**\n",
    "\n",
    "**1. ¿Describa en sus propias palabras la estrategia de selección de palabras dentro de la ventana de contexto en w2v? Explique porque se hace así y cual es la intuición.**\n",
    "\n",
    "Lo primero es definir la ventana de contexto. En el articulo *Efficient Estimation of Word Representations in Vector Space* [2], Mikolov propone un modelo llamado **continious skip-gram model** en el que trata de maximizar la clasificacion de una palabra basado en otra palabra de la misma oracion. Es decir, y cito textualmente \"Se usa cada palabra actual como entrada para un clasificador log-lineal con una capa de proyección continua y predice palabras dentro de un cierto rango antes y después de la palabra actual\". El rango o distancia maxima entre las palabras que se pueden seleccionar para este analisis se conoce como **ventana de contexto**, representada por la letra $C$. La estrategia que se utiliza para la seleccion de palabras dentro de la ventana es una estrategia aleatoria. Para cada palabra del conjunto de entrenamiento se selecciona de manera random un numero entero $R$ en el rango $[1,C]$, luego se seleccionan $R$ palabras anteriores y $R$ palabras posteriores a la palabra objetivo como el contexto actual para ese ejemplo de entrenamiento. La intuicion detras de esto es que no todas las palabras dentro de la ventana de contexto son seleccionadas con igual probabilidad, es necesario agregar este comportamiento aleatorio para evitar que el modelo se sobreajuste a ciertas ventanas de contexto y poder capturar la mayor cantidad de informacion sintactica y semantica posible. De otra forma tendriamos un modelo de n-gramas simple, el cual hemos visto que presenta complicaciones para modelar el lenguaje a profundidad. \n",
    "\n",
    "**2. ¿Qué estrategia se usa para construir frases de palabras y construir un solo vector para conceptos basados en más de un token?**\n",
    "\n",
    "En el articulo *Distributed Representations of Words and Phrases and their Compositionality* [1], Mikolov habla acerca de que el modelo de skip-grams puede aprender conceptos mas profundos que una sola palabra. Y es que por lo que entiendo la idea de los modelos tipo embeddings es *linealizar* el lenguaje, es decir, poder encontrar representaciones de palabras con las cuales podamos mediante operaciones vectoriales lineales, podamos llegar a otros vectores de palabras que tengan sentido semantico; por ejemplo *vector(\"King\")* - *vector(Man)* + *vector(Woman)* = *vector(Queen)*. Es asi, que hay muchas *frases* cuyo significado no es una composicion simple de significados de palabras individuales. Para que el modelo pueda aprender este tipo de frases, se tiene que realiar un preproceso para poder construir estas frases en el corpus y que cada frase tenga un token unico. Me parece que la estrategia que se sigue es primero buscar los conjuntos de palabras que por si mismo aparecen de manera muy rara en contextos unicos. Por ejemplo \"New York Times\" que involucra tres palabras. Luego es necesario construir un token unico de esta palabra y agregarlo al corpus. Para construir el token unico se pueden agregar guiones para tener \"New_York_Times\" y con esto poder conseguir frases de palabras que se aprendan por si mismas en el modelo.\n",
    "\n",
    "**3. ¿Según el autor de w2v, cuales podrían ser las diferencias o ventajas/desventajas de CBOW y Skipgram?**\n",
    "\n",
    "En el articulo donde se proponen los modelos de embeddings [2], nos damos cuenta de que son modelos con ideas similares, aunque opuestas. El modelo de CBOW predice la palabra actual basándose en el contexto de su alrededor mientras que el modelo se skip-grams se intenta predecir las palabras de contexto a partir de la palabra actual. En general el modelo CBOW es el más rápido entrenar. Sin embargo el modelo skip-grams es más efectivo para aprender representaciones de alta calidad para palabras raras, ya que trata cada instancia de la palabra objetivo como un ejemplo de entrenamiento único. Si se desea escoger uno de los modelos se tiene que hacer en base a las necesidades y disponibilidad de hardware que se tenga, CBOW es mas eficiente, y funciona bien para conjuntos peque;os de datos, mientas que skip-gram es brutal para conjuntos grandes y para aprender palabras extra;as en el contexto.\n",
    "\n",
    "**4. ¿Cuales son las diferencias entre usar Hierachical Softmax, Negative Sampling y NCE? ¿Cuál recomienda el autor y por qué?**\n",
    "\n",
    "Para responder esta pregunta, es necesario conocer como se calculan las probabilidades de una red neuronal. Para una neurona simple, una red neuronal recibe una cantidad de inputs y los procesa mediante la multiplicacion por los pesos y la suma del bias. Luego se aplica una funcion no lineal, y finalmente se calcula la probabilidad mapeando la salida de la funcion no lineal que va de $(-\\infty, \\infty)$ a un valor entre $[0,1]$, una probabilidad. Este mapeo se hace utilizando la funcion softmax. Sin embargo hay complicaciones a la hora de manejar grandes volumentes de datos ya que el costo computacional aumenta. Hierarchical Softmax, Negative Sampling y Noise Contrastive Estimation son técnicas utilizadas para optimizar este problema. Estas funciones consisten en lo siguiente:\n",
    "\n",
    "* Hierarchical Softmax\n",
    "\n",
    "    Esta técnica usa un árbol binario para representar las palabras, reduciendo significativamente la complejidad computacional de calcular las probabilidades de salida desde O(W)O(W) a O(log⁡2(W))O(log2​(W)). Aunque es eficiente para grandes vocabularios, su rendimiento depende mucho de cómo esté estructurado el árbol. Esto la hace muy útil para acelerar el entrenamiento, especialmente con palabras frecuentes que reciben códigos más cortos.\n",
    "\n",
    "* Noise Contrastive Estimation (NCE)\n",
    "\n",
    "    NCE se propone como una forma de entrenar modelos diferenciando datos reales de una distribución de ruido a través de regresión logística. Es conceptualmente más compleja y requiere ajustes precisos, pero proporciona una base teórica sólida para la estimación eficiente de los parámetros del modelo.\n",
    "\n",
    "* Negative Sampling (NEG)\n",
    "\n",
    "    Negative Sampling es una simplificación de Noise Contrastive Estimation que mejora la velocidad de entrenamiento al enfocarse en diferenciar la palabra objetivo de unas pocas muestras negativas en lugar de todo el vocabulario. Es menos preciso que Hierarchical Softmax para datasets pequeños pero escala maravillosamente bien para conjuntos de datos más grandes, haciendo que el entrenamiento sea viable incluso con vocabularios extremadamente grandes.\n",
    "\n",
    "\n",
    "Aunque Hierarchical Softmax y NCE tienen sus méritos, Mikolov favorece el uso de Negative Sampling para la mayoría de las aplicaciones prácticas del modelo Skip-gram. NEG no solo simplifica enormemente el proceso de entrenamiento sino que también provee una eficiencia excepcional sin sacrificar la calidad de las representaciones vectoriales aprendidas. Es especialmente útil para conjuntos de datos grandes, donde un número pequeño de muestras negativas puede ser suficiente para mantener un alto rendimiento.\n",
    "\n",
    "**5. ¿Cual diría usted que es la principal conclusión y aportación del paper de w2v? ¿Qué crítica haría usted a estos papers de w2v?**\n",
    "\n",
    "Bueno, me parecieron increibles. Me parecio super wao que pude leerlos y entender la mayoria sin problemas. Creo que la aportacion mas importante de w2v es la capacidad de poder producir representaciones vectoriales de palabras que tengan sentido matematico y semantico. Me volo la cabeza el hecho de que las operaciones vectoriales conducian a significados semanticos. El lenguaje es algo que evoluciona en el tiempo, que no tiene muchas relaciones aparentes y que es dificil de modelar, y contar con una herramienta como word2vec, con la cual es posible facilmente encontrar este tipo de relaciones matematicas es increible, me parece increible lo que despues podriamos encontrar. Mi critica es muy positiva, creo que son faciles de leer y de entender, ademas de que citan todas las fuentes de datos que utilizaron para los entrenamientos. Lo unico que me hubiera gustado es una explicacion mas detallada de como se plantearon y funcionan las propuestas CBOW y skip-grams. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove\n",
    "\n",
    "**Lea el paper de Glove y explique brevemente los siguientes puntos:**\n",
    "\n",
    "**1. ¿Qué desventaja trata de solucionar de W2V?**\n",
    "\n",
    "En el articulo [3], se habla acerca de los modelos de generacion de espacios vectoriales semanticos. Concretamente los propuestos por Mikolov con los llamados por el autor como **Metodos de ventana poco profunda**, los cuales utilizan el contexto local para generar vectores de representacion de palabras en una ventana de contexto. La desventaja que se plantea en el articulo es que mientras otros modelos como el LSA aprovechan la informacion estadistica contenida en todo el corpus, los modelos w2v no operan con las co-ocurrencias estadisticas del corpus, sino que se limitan a utilizar una ventana a traves de todo el corpus, desaprovechando la ventaja de la cantidad de repeticion de palabras en el corpus. \n",
    "\n",
    "**2. Describa en sus propias palabras y de manera general cual es la principal estrategia para lograrlo.**\n",
    "\n",
    "Para poder superar las limitaciones de w2v Pennington, Jeffrey y Socher proponen GloVe por *global vectors* vectores globales. GloVe busca superar las limitaciones de Word2Vec mediante la implementación de un modelo que combina la fuerza de los métodos basados en la matriz de co-ocurrencia (LSA), que toman en cuenta la estadistica del texto con la eficiencia de los modelos predictivos, (skip-grams) los cuales utilizan una ventana de contexto. GloVe utiliza una matriz global de co-ocurrencias para todo el corpus, capturando así la frecuencia con la que cada palabra aparece en el contexto de las otras. Este enfoque permite que GloVe no solo considere la información local de la ventana de contexto, sino también las estadísticas globales del corpus, solventando los problemas de cada modelo por separado, proporcionando una gran captura de las relaciones semánticas entre las palabras.\n",
    "\n",
    "**3. Explique en sus propias palabras las principales conclusiones de los experimentos. Comente si cree que se logró el objetivo.**\n",
    "\n",
    "Los experimentos realizados en el desarrollo de GloVe demuestran que el modelo es capaz de capturar subestructuras intrincadas del espacio semántico más eficazmente que los modelos anteriores, como Word2Vec o LSA, especialmente en tareas de analogía de palabras. Las pruebas que se hicieron en el articulo muestran que los embeddings de GloVe retienen una cantidad significativa de varianza semántica y sintáctica, lo cual era el objetivo principal. Creo firmemente que se logró el objetivo de combinar eficazmente la información global y local del corpus para producir representaciones vectoriales más informativas y precisas.\n",
    "\n",
    "**4. ¿Encuentra alguna relación entre Glove y las clasicas TCOR y DOR? ¿Cuáles?**\n",
    "\n",
    "Realmente si hay una relacion conceptual con TCOR Y DOR en el sentido de que todos estos enfoques se basan en la observación de cómo las palabras coexisten y se distribuyen en grandes corpus de texto. Sin embargo, GloVe innova sobre estas ideas al aplicar un modelo logarítmico y una función de ponderación que ayuda a mejorar la calidad de los embeddings al ajustar la influencia de las co-ocurrencias según su frecuencia, algo que no pasa en los métodos TCOR y DOR.\n",
    "\n",
    "**5. ¿Cual diría usted que es la principal conclusión y aportación del paper de Glove? ¿Qué crítica haría usted a este paper de Glove?**\n",
    "\n",
    "En el articulo de GloVe la principal aportación del paper de GloVe es demostrar que es posible mejorar significativamente la calidad de los embeddings de palabras integrando la información estadística global del corpus a través de una matriz de co-ocurrencia y combinarla con redes neuronales. Una posible crítica podría ser que, a pesar de su efectividad, la construcción y almacenamiento de grandes matrices de co-ocurrencia puede ser computacionalmente muy costosa y exigente en términos de memoria, lo que podría limitar su aplicabilidad en dispositivos con recursos limitados o con corpus extremadamente grandes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otros papers\n",
    "\n",
    "**Lea los papers de Directional Word2Vec, Fast Text, el de gnome-mining, así como el paper de CNNs de Kim y conteste.**\n",
    "\n",
    "**1. ¿Qué desventaja trata de solucionar FastText y cómo lo logra?**\n",
    "\n",
    "\n",
    "**2. ¿Cuál sería la principal desventaja de FastText vs Word2Vec?**\n",
    "\n",
    "\n",
    "**3. ¿Qué desventaja trata de solucionar el paper de Directional W2V y cómo lo logra? Describa brevemente las conclusiones de la sección experimental.**\n",
    "\n",
    "\n",
    "**4. ¿Qué se dice acerca del análisis de complejidad del Directional w2v?**\n",
    "\n",
    "\n",
    "**5. En el paper de gnome-mining, ¿Qué técnicas de NLP son usadas y con que objetivo intuitivo cada una?**\n",
    "\n",
    "\n",
    "**6. ¿En que problemas de clasificación evaluó Kim su CNN?**\n",
    "\n",
    "\n",
    "**7. En los resultados dónde estuvo involucrado algún método de clasificación con SVM, ¿Cómo fue el resultado respecto a CNNs? ¿Qué features usaba el método basado en SVM?**\n",
    "\n",
    "\n",
    "**8. En sus propias palabras, ¿Qué diferencia tienen las estrategias multi-channel y single-channel?, ¿Cuál recomienda Kim?**\n",
    "\n",
    "\n",
    "**9. ¿Cuál diría usted que es la principal conclusión y aportación del paper de Kim? ¿Qué crítica le haría usted a Kim?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 CNNs\n",
    "\n",
    "**Estudie superficialmente el siguiente [notebook](https://github.com/fagonzalezo/dl-tau-2017-2/blob/master/Handout-CNN-sentence-classification.ipynb). En esta tarea se le pro-porcionará el CNN-rand pero en Pytorch. Investigue lo necesario para completar el CNN-static y el CNN-non-static por usted mismo. Contruya la gŕafica de comparación de los tres.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "[1] Mikolov, Tomas, et al. \"Distributed representations of words and phrases and their compositionality.\" Advances in neural information processing systems 26 (2013).\n",
    "\n",
    "[2] Mikolov, Tomas, et al. \"Efficient estimation of word representations in vector space.\" arXiv preprint arXiv:1301.3781 (2013).\n",
    "\n",
    "[3] Pennington, Jeffrey, Richard Socher, and Christopher D. Manning. \"Glove: Global vectors for word representation.\" Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 2014."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PLN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
