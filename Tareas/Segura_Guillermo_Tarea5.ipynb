{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 5. Modelos de Lenguaje Nauronales\n",
    "\n",
    "Guillermo Segura Gómez\n",
    "\n",
    "## Ejercicio 1\n",
    "\n",
    "**1. Con base en la implementación mostrada en las prácticas del NLM, construya un modelo de lenguaje neuronal a nivel de carácter. Tomé en cuenta secuencias de tamaño 6 o más para el modelo, es decir hasta 5 caracteres o más en el contexto. Ponga al modelo a generar texto 3 veces, con un máximo de 300 caracteres. Escriba 5 ejemplos de oraciones y mídales el likelihood. Escriba un ejemplo de estructura morfológica (permutaciones con caracteres) similar al de estructura sintáctica del profesor con 5 o más caracteres de su gusto (e.g., \"ando \"). Calcule la perplejidad del modelo sobre los datos val.**\n",
    "\n",
    "### Modelos de lenguaje neuronales\n",
    "\n",
    "Los Modelos de Lenguaje Neuronal (NLM) son fundamentales en el campo del Procesamiento del Lenguaje Natural para generar texto, completar frases, y más (mucho más). Mientras que los modelos tradicionales a nivel de palabra capturan la estructura y semántica del lenguaje a partir de las palabras, los modelos a nivel de carácter ofrecen una mirada más profunda a la formación de palabras y estructuras sintácticas. Al enfocarse en caracteres, estos modelos pueden generalizar mejor en idiomas con una rica morfología o en casos donde la ortografía y la formación de palabras juegan un papel crucial. \n",
    "\n",
    "Construiremos un NLM a nivel de carácter, lo utilizaremos para generar texto, evaluaremos ejemplos de oraciones mediante el cálculo de su log-verosimilitud, exploraremos estructuras morfológicas mediante permutaciones de caracteres, y mediremos la perplejidad del modelo en un conjunto de datos de validación. \n",
    "\n",
    "Comenzamos siguiendo el procedimiento descrito en la práctica 5 de NLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General libraries\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "from typing import Tuple\n",
    "from argparse import Namespace\n",
    "from itertools import chain\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.util import ngrams\n",
    "from nltk import FreqDist\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#scikit-learn\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos los parámetros generadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1111\n",
    "random.seed(seed) #python seed\n",
    "np.random.seed(seed) #numpy seed\n",
    "torch.manual_seed(seed) #torch seed\n",
    "torch.backends.cudnn.benchmark = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializamos los datos en un data frame de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_text = \"/Users/guillermo_sego/Desktop/Segundo Semestre/PLN/Data/MexData/mex20_train.txt\"\n",
    "path_text_val = \"/Users/guillermo_sego/Desktop/Segundo Semestre/PLN/Data/MexData/mex20_val.txt\"\n",
    "\n",
    "X_train = pd.read_csv(path_text, sep = '\\r\\n', engine = 'python', header = None).loc[:, 0].values.tolist()\n",
    "X_val = pd.read_csv(path_text_val, sep = '\\r\\n', engine = 'python', header = None).loc[:, 0].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la clase `CaracterData` está diseñada para procesar datos textuales para tareas de modelado de lenguaje o similares, utilizando un enfoque basado en caracteres. El propósito de esta clase incluye varias funciones importantes para el preprocesamiento de datos textuales y su preparación para algoritmos de aprendizaje automático. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaracterData():\n",
    "    def __init__(self, N:int, vocab_max: int = None):\n",
    "        self.N = N  # Tamaño de la secuencia de caracteres, incluyendo el carácter objetivo\n",
    "        self.vocab_max = vocab_max  # Limitar el tamaño del vocabulario, si es necesario\n",
    "        self.UNK = '<unk>'  # Representación de caracteres desconocidos\n",
    "        self.vocab = set()\n",
    "\n",
    "    def get_vocab(self, corpus: list) -> set:\n",
    "        # Construye un conjunto de caracteres únicos a partir del corpus\n",
    "        unique_chars = set(chain(*corpus))\n",
    "        return unique_chars\n",
    "\n",
    "    def fit(self, corpus: list) -> None:\n",
    "        # Construye el vocabulario a partir del corpus\n",
    "        self.vocab = self.get_vocab(corpus)\n",
    "        \n",
    "        # Crea mapeos de caracteres a índices y viceversa\n",
    "        self.char2id = {char: idx for idx, char in enumerate(self.vocab)}\n",
    "        self.id2char = {idx: char for char, idx in self.char2id.items()}\n",
    "\n",
    "        # Incluir el token UNK en el vocabulario y los mapeos\n",
    "        self.char2id[self.UNK] = len(self.char2id)\n",
    "        self.id2char[len(self.id2char)] = self.UNK\n",
    "    \n",
    "    def get_vocab_size(self) -> int:\n",
    "        # Retorna el tamaño del conjunto vocab\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def transform(self, corpus: list) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        X_ngrams, y = [], []\n",
    "\n",
    "        for doc in corpus:\n",
    "            doc_ngrams = list(ngrams(doc, self.N))\n",
    "            for ngram in doc_ngrams:\n",
    "                # Usa el índice para UNK si el carácter no está en char2id\n",
    "                X_ngrams.append([self.char2id.get(char, self.char2id[self.UNK]) for char in ngram[:-1]])\n",
    "                y.append(self.char2id.get(ngram[-1], self.char2id[self.UNK]))\n",
    "\n",
    "        return np.array(X_ngrams), np.array(y)\n",
    "\n",
    "    def get_ngram_doc(self, doc: str) -> list:\n",
    "        # Genera n-gramas del documento\n",
    "        return list(ngrams(doc, self.N))\n",
    "\n",
    "    def replace_unk(self, doc_tokens: list) -> list:\n",
    "        # Reemplaza caracteres desconocidos por un índice especial, si es necesario\n",
    "        return [token if token in self.char2id else '<unk>' for token in doc_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "args.N = 6  # Tamaño de secuencia deseado\n",
    "\n",
    "# Creamos un objeto con la clase adaptada para caracteres\n",
    "caracter_data = CaracterData(args.N)\n",
    "\n",
    "# Ajustamos el modelo a nuestro conjunto de datos de entrenamiento para construir el vocabulario de caracteres\n",
    "# Nota: No es necensario pasar un tokenizador ya que estamos trabajando a nivel de carácter\n",
    "caracter_data.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora utilizamos los métodos de la clase que creamos. El método `transform` transforma los datos textuales proporcionados en un formato que es adecuado para el entrenamiento o la evaluación en modelos de aprendizaje automático."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_caracter_train, y_caracter_train = caracter_data.transform(X_train)\n",
    "X_caracter_val, y_caracter_val = caracter_data.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 425\n"
     ]
    }
   ],
   "source": [
    "print(f'Vocab size: {caracter_data.get_vocab_size()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora necesitamos pasar estos datos en tensores de PyTorch y, organizarlos en un DataLoader de PyTorch para facilitar el entrenamiento y la evaluación de modelos de aprendizaje profundo. PyTorch utiliza tensores, que son una generalización de matrices y vectores, como su estructura de datos principal para realizar operaciones de aprendizaje automático, especialmente en redes neuronales.\n",
    "\n",
    "Para el entrenamiento, es útil organizar los datos en lotes (batches), y PyTorch ofrece una herramienta llamada **DataLoader** para esto. Un DataLoader puede cargar los datos en lotes de un tamaño especificado y puede mezclar los datos para reducir el riesgo de sobreajuste. Para usar un DataLoader, primero debes organizar tus tensores en un Dataset, que es otra abstracción de PyTorch que facilita trabajar con conjuntos de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch size in args\n",
    "args.batch_size = 64\n",
    "# Num workers\n",
    "args.num_workers = 2\n",
    "\n",
    "# Convertimos los datos a tensores de pytorch\n",
    "train_dataset = TensorDataset(torch.tensor(X_caracter_train, dtype = torch.int64),\n",
    "                              torch.tensor(y_caracter_train, dtype = torch.int64))\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size = args.batch_size,\n",
    "                          num_workers=args.num_workers,\n",
    "                          shuffle = True)\n",
    "\n",
    "val_dataset = TensorDataset(torch.tensor(X_caracter_val, dtype = torch.int64),\n",
    "                            torch.tensor(y_caracter_val, dtype = torch.int64))\n",
    "\n",
    "val_loader = DataLoader(train_dataset,\n",
    "                          batch_size = args.batch_size,\n",
    "                          num_workers=args.num_workers,\n",
    "                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([64, 5])\n",
      "y shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(f'X shape: {batch[0].shape}')\n",
    "print(f'y shape: {batch[1].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parámetros que definen aspectos estructurales y de regularización de un modelo de red neuronal, que influyen en cómo el modelo aprenderá de los datos y generalizará a nuevos ejemplos no vistos. Utilizamos los parámetros del modelo propuesto por Bengio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab size\n",
    "args.vocab_size = caracter_data.get_vocab_size()\n",
    "\n",
    "# Dimension of word embeddings\n",
    "args.d = 50\n",
    "\n",
    "# Dimension for hidden layer\n",
    "args.d_h = 100\n",
    "\n",
    "# Dropout\n",
    "args.dropout = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la clase `NeuralLM`, que representa un Modelo de Lenguaje Neuronal, utilizando PyTorch. Esta clase hereda de `nn.Module`, que es la clase base para todos los módulos de red neuronal en PyTorch, proporcionando funcionalidades útiles como el seguimiento de parámetros, la GPU/CPU transferencia, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, window_size, dropout):\n",
    "        super(NeuralLM, self).__init__()\n",
    "        \n",
    "        # Tamaño de la ventana de entrada para el modelo, correspondiente al contexto de caracteres\n",
    "        self.window_size = window_size - 1  # Menos 1 porque el último carácter es el objetivo\n",
    "        \n",
    "        # Dimensión de los embeddings de caracteres\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Capa de embedding que convierte índices de caracteres en vectores densos\n",
    "        self.emb = nn.Embedding(vocab_size, self.embedding_dim)\n",
    "        \n",
    "        # Primera capa lineal que transforma la entrada de embeddings aplanada a una representación intermedia\n",
    "        self.fc1 = nn.Linear(self.embedding_dim * self.window_size, hidden_dim)\n",
    "        \n",
    "        # Capa de Dropout para reducir el sobreajuste\n",
    "        self.drop1 = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Segunda capa lineal que transforma la salida de la capa oculta en logits para cada carácter en el vocabulario\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Transforma los índices de caracteres en embeddings\n",
    "        x = self.emb(x)\n",
    "        \n",
    "        # Aplana los embeddings en un vector único para cada muestra\n",
    "        # print(self.window_size * self.embedding_dim)\n",
    "        x = x.view(-1, self.window_size * self.embedding_dim)\n",
    "        \n",
    "        # Aplica la primera capa lineal y una función de activación ReLU\n",
    "        h = F.relu(self.fc1(x))\n",
    "        \n",
    "        # Aplica Dropout a la representación de la capa oculta\n",
    "        h = self.drop1(h)\n",
    "        \n",
    "        # Genera los logits para cada carácter en el vocabulario\n",
    "        return self.fc2(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora necesitamos mas funciones para poder hacer la evaluación del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta función toma los logits (es decir, las salidas no normalizadas de un modelo) \n",
    "# y devuelve las predicciones de clase como índices.\n",
    "def get_preds(raw_logits):\n",
    "    # Calcula las probabilidades aplicando la función softmax a los logits.\n",
    "    # La operación detach() se usa para evitar que se calculen gradientes para estas operaciones,\n",
    "    # ya que solo se necesitan las probabilidades para hacer predicciones.\n",
    "    probs = F.softmax(raw_logits.detach(), dim=1)\n",
    "    \n",
    "    # Encuentra el índice de la mayor probabilidad en cada fila (es decir, para cada ejemplo en el lote),\n",
    "    # que corresponde a la clase predicha. Luego, convierte el tensor a un array de NumPy.\n",
    "    y_pred = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "\n",
    "# Evalúa el modelo en un conjunto de datos proporcionado y devuelve la precisión del modelo.\n",
    "def model_eval(data, model, gpu=False):\n",
    "    # Desactiva el cálculo de gradientes para acelerar las cosas y reducir el uso de memoria\n",
    "    # ya que no se necesita para la evaluación.\n",
    "    with torch.no_grad():\n",
    "        preds, tgts = [], []  # Listas para almacenar predicciones y etiquetas verdaderas\n",
    "        \n",
    "        # Itera sobre los lotes de datos en el DataLoader\n",
    "        for window_words, labels in data:\n",
    "            # Si se utiliza GPU, mueve los datos al dispositivo adecuado\n",
    "            if gpu:\n",
    "                window_words = window_words.cuda()\n",
    "                \n",
    "            # Obtiene los logits del modelo para el lote actual\n",
    "            outputs = model(window_words)\n",
    "            \n",
    "            # Obtiene las predicciones de clase para el lote actual utilizando la función get_preds\n",
    "            y_pred = get_preds(outputs)\n",
    "            \n",
    "            # Extrae las etiquetas verdaderas del lote actual y las convierte a un array de NumPy\n",
    "            tgt = labels.numpy()\n",
    "            \n",
    "            # Almacena las predicciones y las etiquetas verdaderas\n",
    "            tgts.append(tgt)\n",
    "            preds.append(y_pred)\n",
    "        \n",
    "        # Aplana las listas de listas para obtener una única lista de etiquetas y predicciones\n",
    "        tgts = [e for l in tgts for e in l]\n",
    "        preds = [e for l in preds for e in l]\n",
    "        \n",
    "        # Calcula y devuelve la precisión del modelo comparando las predicciones con las etiquetas verdaderas\n",
    "        return accuracy_score(tgts, preds)\n",
    "    \n",
    "\n",
    "# Guarda el estado actual del modelo y, si es el mejor modelo hasta el momento según \n",
    "# algún criterio, guarda una copia separada.\n",
    "def save_checkpoint(state, is_best, checkpoint_path, filename=\"checkpoint.pt\"):\n",
    "    # Construye la ruta completa del archivo donde se guardará el estado del modelo\n",
    "    filename = os.path.join(checkpoint_path, filename)\n",
    "    \n",
    "    # Guarda el estado del modelo en la ruta especificada\n",
    "    torch.save(state, filename)\n",
    "    \n",
    "    # Si el modelo actual es el \"mejor\" según algún criterio, guarda una copia separada\n",
    "    if is_best:\n",
    "        # Copia el archivo del checkpoint al archivo del \"mejor modelo\"\n",
    "        shutil.copyfile(filename, os.path.join(checkpoint_path, \"model_best.pt\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guillermo_sego/anaconda3/envs/PLN/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "# Model hyperparameters\n",
    "args. vocab_size = caracter_data.get_vocab_size()\n",
    "args.d = 100\n",
    "args.d_h = 200\n",
    "args.dropout = 0.1\n",
    "\n",
    "# Training hyperparameters\n",
    "args.lr = 2.3e-1\n",
    "args.num_epochs = 20\n",
    "args.patience = 20\n",
    "\n",
    "# Scheduler hyperparameters\n",
    "args.lr_patience = 10\n",
    "args.lr_factor = 0.5\n",
    "\n",
    "# Saving directoty\n",
    "args.savedir = 'model'\n",
    "os.makedirs(args.savedir, exist_ok=True)\n",
    "\n",
    "# Create model\n",
    "model = NeuralLM(\n",
    "    vocab_size=args.vocab_size,\n",
    "    embedding_dim=args.d,\n",
    "    hidden_dim=args.d_h,\n",
    "    window_size=args.N,\n",
    "    dropout=args.dropout\n",
    ")\n",
    "\n",
    "# Send to GPU\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "if args.use_gpu:\n",
    "    model.cuda()\n",
    "    \n",
    "# Loss, optimizer an scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr= args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, 'min',\n",
    "                patience=args.lr_patience,\n",
    "                verbose=True,\n",
    "                factor=args.lr_factor\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora realizamos el entrenamiento y validación del modelo de aprendizaje profundo utilizando PyTorch, incluyendo características como el early stopping y el ajuste dinámico de hiperparámetros (scheduler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.40929519959521143\n",
      "Epoch [1/20], Loss: 2.1188 - Val accuracy: 0.4563 - Epoch time: 1711170826.36\n",
      "Train acc: 0.45270056412023085\n",
      "Epoch [2/20], Loss: 1.9259 - Val accuracy: 0.4782 - Epoch time: 1711170844.05\n",
      "Train acc: 0.46749324455257946\n",
      "Epoch [3/20], Loss: 1.8610 - Val accuracy: 0.4906 - Epoch time: 1711170862.24\n",
      "Train acc: 0.4761725142106623\n",
      "Epoch [4/20], Loss: 1.8221 - Val accuracy: 0.4917 - Epoch time: 1711170880.25\n",
      "Train acc: 0.4824375861252261\n",
      "Epoch [5/20], Loss: 1.7942 - Val accuracy: 0.4979 - Epoch time: 1711170898.38\n",
      "Train acc: 0.487113378477306\n",
      "Epoch [6/20], Loss: 1.7733 - Val accuracy: 0.5036 - Epoch time: 1711170916.69\n",
      "Train acc: 0.49012574283007493\n",
      "Epoch [7/20], Loss: 1.7568 - Val accuracy: 0.5100 - Epoch time: 1711170934.65\n",
      "Train acc: 0.49334198174145205\n",
      "Epoch [8/20], Loss: 1.7444 - Val accuracy: 0.5134 - Epoch time: 1711170952.74\n",
      "Train acc: 0.495884425329429\n",
      "Epoch [9/20], Loss: 1.7321 - Val accuracy: 0.5144 - Epoch time: 1711170971.28\n",
      "Train acc: 0.49837640491775037\n",
      "Epoch [10/20], Loss: 1.7234 - Val accuracy: 0.5200 - Epoch time: 1711170992.45\n",
      "Train acc: 0.49999515545603307\n",
      "Epoch [11/20], Loss: 1.7137 - Val accuracy: 0.5222 - Epoch time: 1711171010.65\n",
      "Train acc: 0.5021372513134097\n",
      "Epoch [12/20], Loss: 1.7059 - Val accuracy: 0.5225 - Epoch time: 1711171029.02\n",
      "Train acc: 0.5152853436396521\n",
      "Epoch [13/20], Loss: 1.6543 - Val accuracy: 0.5352 - Epoch time: 1711171047.28\n",
      "Train acc: 0.5178798660752735\n",
      "Epoch [14/20], Loss: 1.6436 - Val accuracy: 0.5365 - Epoch time: 1711171065.70\n",
      "Train acc: 0.5183010722590646\n",
      "Epoch [15/20], Loss: 1.6405 - Val accuracy: 0.5379 - Epoch time: 1711171084.49\n",
      "Train acc: 0.518881879252433\n",
      "Epoch [16/20], Loss: 1.6370 - Val accuracy: 0.5387 - Epoch time: 1711171102.82\n",
      "Train acc: 0.5196218833433813\n",
      "Epoch [17/20], Loss: 1.6355 - Val accuracy: 0.5381 - Epoch time: 1711171120.85\n",
      "Train acc: 0.5204225788045819\n",
      "Epoch [18/20], Loss: 1.6321 - Val accuracy: 0.5407 - Epoch time: 1711171139.15\n",
      "Train acc: 0.5200440315218328\n",
      "Epoch [19/20], Loss: 1.6298 - Val accuracy: 0.5415 - Epoch time: 1711171157.53\n",
      "Train acc: 0.5211251722504522\n",
      "Epoch [20/20], Loss: 1.6272 - Val accuracy: 0.5409 - Epoch time: 1711171175.72\n",
      "--- 366.9564108848572 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "best_metric = 0\n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "for epoch in range(args.num_epochs):\n",
    "    epoch_stats_time = time.time()\n",
    "    loss_epoch = []\n",
    "    training_metric = []\n",
    "    model.train()\n",
    "    \n",
    "    for window_words, labels in train_loader:\n",
    "        if args.use_gpu:\n",
    "            window_words = window_words.cuda()\n",
    "            labels = labels.cuda()\n",
    "            \n",
    "        # Forward pass\n",
    "        outputs = model(window_words)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_epoch.append(loss.item())\n",
    "        \n",
    "        # Get training metrics\n",
    "        y_pred = get_preds(outputs)\n",
    "        tgt = labels.cpu().numpy()\n",
    "        training_metric.append(accuracy_score(tgt, y_pred))\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Get metric in training dataset\n",
    "    mean_epoch_metric = np.mean(training_metric)\n",
    "    train_metric_history.append(mean_epoch_metric)\n",
    "    \n",
    "    # Get metric in validation dataset\n",
    "    model.eval()\n",
    "    tuning_metric = model_eval(val_loader, model, gpu=args.use_gpu)\n",
    "    metric_history.append(mean_epoch_metric)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step(tuning_metric)\n",
    "    \n",
    "    # Check for metric improvement\n",
    "    is_improvement = tuning_metric > best_metric\n",
    "    if is_improvement:\n",
    "        best_metric = tuning_metric\n",
    "        n_no_improve = 0\n",
    "    else:\n",
    "        n_no_improve = 1\n",
    "        \n",
    "    save_checkpoint(\n",
    "        {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"scheduler\": scheduler.state_dict(),\n",
    "            \"best_metric\": best_metric,\n",
    "        },\n",
    "        is_improvement,\n",
    "        args.savedir,\n",
    "    )\n",
    "    \n",
    "    # Early stopping\n",
    "    if n_no_improve >= args.patience:\n",
    "        print(\"No improvement. Breaking out of loop.\")\n",
    "        \n",
    "    print(\"Train acc: {}\".format(mean_epoch_metric))\n",
    "    print(\"Epoch [{}/{}], Loss: {:.4f} - Val accuracy: {:.4f} - Epoch time: {:.2f}\"\n",
    "         .format(epoch+1, args.num_epochs, np.mean(loss_epoch), tuning_metric, (time.time())))\n",
    "    \n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación del modelo\n",
    "\n",
    "Utiizamos el modelo de lenguaje neuronal entrenado para generar texto de manera autónoma. Las siguientes funciones son adaptaciones para trabajar con caracteres de las funciones revisadas en la práctica 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(text, char2id):\n",
    "    # Convierte el texto en caracteres y usa '<unk>' para caracteres desconocidos.\n",
    "    all_chars = [char if char in char2id else '<unk>' for char in text]\n",
    "    \n",
    "    # Convierte los caracteres a sus índices numéricos correspondientes según el mapeo char2id.\n",
    "    char_ids = [char2id[char] for char in all_chars]\n",
    "    # print(len(char_ids))\n",
    "    return all_chars, char_ids\n",
    "\n",
    "def sample_next_word(logits, temperature=1.0):\n",
    "    # Convierte los logits a un array de numpy y ajusta la \"temperatura\" de la predicción.\n",
    "    logits = np.asarray(logits).astype(\"float64\")\n",
    "    preds = logits / temperature\n",
    "    \n",
    "    # Convierte los logits ajustados a probabilidades usando softmax.\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    \n",
    "    # Muestrea un índice de palabra de la distribución de probabilidades.\n",
    "    probas = np.random.multinomial(1, preds)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def predict_next_token(model, token_ids):\n",
    "    # Convierte la lista de índices de tokens a un tensor de PyTorch y agrega una dimensión de lote.\n",
    "    word_ids_tensor = torch.LongTensor(token_ids).unsqueeze(0)\n",
    "    \n",
    "    # Obtiene los logits de la predicción del modelo para la secuencia de tokens y los convierte a numpy.\n",
    "    y_raw_pred = model(word_ids_tensor).squeeze(0).detach().numpy()\n",
    "    \n",
    "    # Muestra el índice de la siguiente palabra de la distribución de logits.\n",
    "    y_pred = sample_next_word(y_raw_pred, 1.0)\n",
    "    return y_pred\n",
    "\n",
    "def generate_sentence(model, initial_text, char2id, nMax = 300):\n",
    "    # Obtiene tokens y sus índices del texto inicial.\n",
    "    all_tokens, window_word_ids = parse_text(initial_text, char2id)\n",
    "    \n",
    "    # Genera hasta 300 palabras adicionales.\n",
    "    for i in range(nMax):\n",
    "        # Predice el índice de la siguiente palabra utilizando el modelo.\n",
    "        y_pred = predict_next_token(model, window_word_ids)\n",
    "\n",
    "        next_word = caracter_data.id2char[y_pred]  # Convierte el índice de palabra predicho a texto.\n",
    "        all_tokens.append(next_word)  # Añade la palabra predicha a la lista de tokens.\n",
    "        \n",
    "        # Si se genera el token de fin de secuencia, detiene la generación.\n",
    "        if next_word == \"</s>\":\n",
    "            break\n",
    "        else:\n",
    "            # Actualiza la ventana de palabras para la siguiente predicción.\n",
    "            window_word_ids.pop(0)  # Elimina la primera palabra.\n",
    "            window_word_ids.append(y_pred)  # Añade la nueva palabra al final.\n",
    "    \n",
    "    # Une los tokens generados en una cadena de texto y la devuelve.\n",
    "    return \"\".join(all_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos las cadenas de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralLM(\n",
       "  (emb): Embedding(425, 100)\n",
       "  (fc1): Linear(in_features=500, out_features=200, bias=True)\n",
       "  (drop1): Dropout(p=0.1, inplace=False)\n",
       "  (fc2): Linear(in_features=200, out_features=425, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model with learned embeddings from scratch\n",
    "best_model = NeuralLM(\n",
    "    vocab_size=args.vocab_size,\n",
    "    embedding_dim=args.d,\n",
    "    hidden_dim=args.d_h,\n",
    "    window_size=args.N,\n",
    "    dropout=args.dropout\n",
    ")\n",
    "\n",
    "best_model.load_state_dict(torch.load(\"model/model_best.pt\")[\"state_dict\"])\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto generado: Cincontrica que “hale el #Bese enamente te plrapita no 10 por a la esas rece novia en un chuna pendeja para el mayar o albe... Grana chingo lo idgo la verga dequerose q adiendo prieron, se va a estaban cosasQueMéQué día que sean la el ninero camociando la pello, quieren en las cosas.😑. A no S pendejos ne\n"
     ]
    }
   ],
   "source": [
    "initial_text = \"Cinco\"\n",
    "generated_text = generate_sentence(\n",
    "    model=best_model,\n",
    "    initial_text=initial_text,\n",
    "    char2id=caracter_data.char2id,\n",
    "    nMax=300\n",
    ")\n",
    "\n",
    "print(\"Texto generado:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto generado: Reino goconocible raja de gazados de demato. Y sus putas. .0 vídoma. (lt) y que tu caniendo recuperro como loca de que decen del de mierda, luchona noches nocia! Que hacen de Fax La creer. Bueno fuertro eran a noviantes draga!.!2 de ser con que tabas son Jaje Plaeta. Phone, sus vaba no lleves y soy de mi\n"
     ]
    }
   ],
   "source": [
    "initial_text = \"Reino\"\n",
    "generated_text = generate_sentence(\n",
    "    model=best_model,\n",
    "    initial_text=initial_text,\n",
    "    char2id=caracter_data.char2id,\n",
    "    nMax=300\n",
    ")\n",
    "\n",
    "print(\"Texto generado:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto generado: Habiablo soy hace me diady... el podia, cosasse #Hombre gienda y no hago la lestodichamenlasea. No mame, oereeción en lo puto en Edf alcumudiado el Rolfos y despuestas nadie no hacen a ventar en bien a REIGOARAWO @USUARIO @USUARIO , el oficiendo en la solo que me vale, descumplas? Ingue eregó ellas su ed\n"
     ]
    }
   ],
   "source": [
    "initial_text = \"Habia\"\n",
    "generated_text = generate_sentence(\n",
    "    model=best_model,\n",
    "    initial_text=initial_text,\n",
    "    char2id=caracter_data.char2id,\n",
    "    nMax=300\n",
    ")\n",
    "\n",
    "print(\"Texto generado:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `log_likelihood` calcula la log-verosimilitud de una secuencia de texto dada bajo un modelo de lenguaje entrenado. La log-verosimilitud es una medida de cuán probable es que el modelo haya generado una secuencia de texto dada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(model, text, caracter_model):\n",
    "    # Transforma el texto\n",
    "    X, y = caracter_model.transform([text])\n",
    "    \n",
    "    # Convierte X en un tensor de PyTorch y agrega una dimensión de lote, preparándolo para el modelo.\n",
    "    X = torch.LongTensor(X).unsqueeze(0)\n",
    "    \n",
    "    # Obtiene los logits del modelo para el texto transformado y luego los desconecta del grafo de cómputo.\n",
    "    logits = model(X).detach()\n",
    "    \n",
    "    # Aplica softmax a los logits para obtener una distribución de probabilidades sobre el vocabulario.\n",
    "    probs = F.softmax(logits, dim=1).numpy()\n",
    "    \n",
    "    # Calcula la log-verosimilitud sumando los logaritmos de las probabilidades de las palabras reales (y)\n",
    "    # según lo predicho por el modelo.\n",
    "    return np.sum([np.log(probs[i][w]) for i, w in enumerate(y)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -69.76852\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model, \"Estamos en la clase de procesamiento de lenguaje\", caracter_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -51.269493\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model, \"Había una vez en un lugar muy lejano\", caracter_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -34.32611\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model, \"Eres un tonto viejo loco\", caracter_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -44.871674\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model, \"Eres muy buena persona mi estimado\", caracter_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -34.247272\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model, \"Mexico es un pais inseguro\", caracter_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La log-verosimilitud es una medida de cuán probable es que una secuencia de texto dada sea generada por un modelo de lenguaje. Valores más altos de log-verosimilitud indican que el modelo asigna una mayor probabilidad a la secuencia de texto observada, lo que sugiere que el modelo piensa que es más \"verosímil\" o probable.\n",
    "\n",
    "Cuando se calcula la log-verosimilitud, se suman los logaritmos de las probabilidades asignadas a las palabras reales. Dado que estas probabilidades están entre 0 y 1, sus logaritmos son negativos. Por lo tanto, una log-verosimilitud más alta (menos negativa) significa que las probabilidades asociadas a las palabras observadas son más altas, y el modelo considera que la secuencia es más probable. Una log-verosimilitud muy negativa indica que el modelo asigna muy bajas probabilidades a las palabras observadas, lo que sugiere que el modelo considera que esa secuencia de texto es poco probable.\n",
    "\n",
    "Debido al corpus con el que se construyó el modelo, las frases mas \"agresivas\" fueron mas probables ya que tienen un valor de log-verosimilitud mas cercano a cero.\n",
    "\n",
    "### Estructura morfológica\n",
    "\n",
    "Evaluamos la permutación de la palabra \"habia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "-13.483011 H i a a b\n",
      "-13.527386 H b i a a\n",
      "-13.591076 H b a a i\n",
      "-13.669549 H i a a b\n",
      "-14.227919 i a H a b\n",
      "--------------------------------------------------\n",
      "-29.35515 b a H i a\n",
      "-29.366308 a a i b H\n",
      "-29.491879 a a b i H\n",
      "-29.68933 a a i H b\n",
      "-30.166454 a a b i H\n"
     ]
    }
   ],
   "source": [
    "from itertools import permutations\n",
    "from random import shuffle\n",
    "\n",
    "word_list = \"Habia\"\n",
    "perms = [\" \".join(perm) for perm in permutations(word_list)]\n",
    "#print(len(perms))\n",
    "print(\"-\"*50)\n",
    "for p, t in sorted([(log_likelihood(best_model, text, caracter_data), text) for text in perms], reverse = True)[:5]:\n",
    "    print(p, t)\n",
    "print(\"-\"*50)  \n",
    "for p, t in sorted([(log_likelihood(best_model, text, caracter_data), text) for text in perms], reverse = True)[-5:]:\n",
    "    print(p, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2\n",
    "**2. Con base en la implementación mostrada en clase, construya un modelo de lenguaje neuronal a nivel de palabra, pero preinicializado con los embeddings proporcionados. Tomé en cuenta secuencias de tamaño 4 para el modelo, es decir hasta 3 palabras en el contexto. Después de haber entrenado el modelo, recupere las 10 palabras más similares a tres palabras de su gusto dadas. Ponga al modelo a generar texto a partir de tres secuencias de inicio de su gusto. Escriba 5 ejemplos de oraciones y mídales el likelihood. Proponga un ejemplo para ver estructuras sintácticas (permutaciones de palabras de alguna oración) buenas usando el likelihood a partir de una oración que usted proponga. Calcule la perplejidad del modelo sobre los datos val. Compárelo con la perplejidad del modelo de lenguaje sin embeddings preentrenados (el visto en clase). DISCUTA BREVEMENTE.**\n",
    "\n",
    "Para realizar este ejercicio, necesitamos construir y entrenar un modelo de lenguaje neuronal, utilizando embeddings preentrenados que se nos proporciona para este ejercicio.\n",
    "\n",
    "Necesitamos cargar los embeddings y agregarlos en el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "args.N = 4\n",
    "\n",
    "class NgramData():\n",
    "    # Inicialización de la clase NgramData\n",
    "    def __init__(self, N:int, vocab_max: int = 5000, tokenizer = None, embeddings_model = None):\n",
    "        # Asignación de un tokenizador personalizado o el predeterminado si no se proporciona\n",
    "        self.tokenizer = tokenizer if tokenizer else self.default_tokenizer\n",
    "        # Definición de signos de puntuación a ignorar\n",
    "        self.punct = set(['.', ',', ';', ':', '-', '_', '!', '¡', '?', '¿', '^', '<url>', '*', '@usuario'])\n",
    "        # Número de palabras en cada n-grama\n",
    "        self.N = N\n",
    "        # Tamaño máximo del vocabulario\n",
    "        self.vocab_max = vocab_max\n",
    "        # Token para palabras desconocidas\n",
    "        self.UNK = '<unk>'\n",
    "        # Token para indicar el inicio de una secuencia\n",
    "        self.SOS = '<s>'\n",
    "        # Token para indicar el final de una secuencia\n",
    "        self.EOS = '</s>'\n",
    "        # Modelo de embeddings (opcional)\n",
    "        self.embeddings_model = embeddings_model\n",
    "\n",
    "    # Función para obtener el tamaño del vocabulario\n",
    "    def get_vocab_size(self) -> int:\n",
    "        return len(self.vocab)\n",
    "\n",
    "    # Tokenizador predeterminado que divide el texto por espacios\n",
    "    def default_tokenizer(self, doc: str) -> list:\n",
    "        return doc.split(\" \")\n",
    "    \n",
    "    # Función para determinar si una palabra debe eliminarse (basada en puntuación o si es un número)\n",
    "    def remove_word(self, word: str) -> bool:\n",
    "        word = word.lower()\n",
    "        is_punct = word in self.punct\n",
    "        is_digit = word.isnumeric()\n",
    "        return is_punct or is_digit\n",
    "    \n",
    "    # Construye el vocabulario a partir de un corpus, excluyendo palabras según `remove_word` y limitando el tamaño\n",
    "    def get_vocab(self, corpus: list) -> set:\n",
    "        # Construcción de la distribución de frecuencia de palabras\n",
    "        freq_dist = FreqDist([w.lower() for sentence in corpus for w in self.tokenizer(sentence) if not self.remove_word(w)])\n",
    "        # Ordenar palabras por frecuencia y limitar el tamaño del vocabulario\n",
    "        sorted_words = self.sortFreqDict(freq_dist)[:self.vocab_max-3]\n",
    "        return set(sorted_words)\n",
    "    \n",
    "    # Ordena el diccionario de frecuencia de palabras\n",
    "    def sortFreqDict(self, freq_dist) -> list:\n",
    "        freq_dict = dict(freq_dist)\n",
    "        return sorted(freq_dict, key=freq_dict.get, reverse=True)\n",
    "    \n",
    "    # Ajusta el modelo al corpus, construyendo vocabulario, mapeos de palabras a ID y opcionalmente una matriz de embeddings\n",
    "    def fit(self, corpus: list) -> None:\n",
    "        self.vocab = self.get_vocab(corpus)\n",
    "        # Agregar tokens especiales al vocabulario\n",
    "        self.vocab.update({self.UNK, self.SOS, self.EOS})\n",
    "        \n",
    "        self.w2id = {}\n",
    "        self.id2w = {}\n",
    "        \n",
    "        # Opcional: inicialización de la matriz de embeddings\n",
    "        if self.embeddings_model is not None:\n",
    "            self.embedding_matrix = np.empty([len(self.vocab), self.embeddings_model.vector_size])\n",
    "            \n",
    "        id = 0\n",
    "        for doc in corpus:\n",
    "            for word in self.tokenizer(doc):\n",
    "                word_ = word.lower()\n",
    "                if word_ in self.vocab and word_ not in self.w2id:\n",
    "                    self.w2id[word_] = id\n",
    "                    self.id2w[id] = word_\n",
    "                    # Si se proporciona un modelo de embeddings, asignar vector o vector aleatorio si la palabra no está en el modelo\n",
    "                    if self.embeddings_model is not None:\n",
    "                        self.embedding_matrix[id] = self.embeddings_model[word_] if word_ in self.embeddings_model else np.random.rand(self.embeddings_model.vector_size)\n",
    "                    id += 1\n",
    "        \n",
    "        # Actualizar mapeos con tokens especiales\n",
    "        self.w2id.update({self.UNK: id, self.SOS: id+1, self.EOS: id+2})\n",
    "        self.id2w.update({id: self.UNK, id+1: self.SOS, id+2: self.EOS})\n",
    "\n",
    "    \n",
    "    # Transforma el corpus en secuencias de entrada (X_ngrams) y etiquetas objetivo (y) para el modelado\n",
    "    def transform(self, corpus: list) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        X_ngrams = []  # Lista para almacenar secuencias de entrada\n",
    "        y = []  # Lista para almacenar las etiquetas objetivo\n",
    "\n",
    "        # Iterar sobre cada documento en el corpus\n",
    "        for doc in corpus:\n",
    "            # Obtener n-gramas para el documento actual\n",
    "            doc_ngram = self.get_ngram_doc(doc)\n",
    "            # Iterar sobre cada ventana de palabras (n-grama) en el documento\n",
    "            for words_window in doc_ngram:\n",
    "                # Convertir palabras en IDs usando el mapeo palabra-ID\n",
    "                words_window_ids = [self.w2id[w] for w in words_window]\n",
    "                # Las primeras N-1 palabras son la entrada, la última palabra es la etiqueta objetivo\n",
    "                X_ngrams.append(list(words_window_ids[:-1]))\n",
    "                y.append(words_window_ids[-1])\n",
    "        # Convertir las listas en arrays de NumPy para su uso en modelos de aprendizaje automático\n",
    "        return np.array(X_ngrams), np.array(y)\n",
    "    \n",
    "    # Genera n-gramas a partir de un documento, preparándolo para la transformación\n",
    "    def get_ngram_doc(self, doc: str) -> list:\n",
    "        # Tokenizar el documento\n",
    "        doc_tokens = self.tokenizer(doc)\n",
    "        # Reemplazar tokens desconocidos por el token <unk>\n",
    "        doc_tokens = self.replace_unk(doc_tokens)\n",
    "        # Convertir todos los tokens a minúsculas\n",
    "        doc_tokens = [w.lower() for w in doc_tokens]\n",
    "        # Añadir tokens de inicio (<s>) y fin (</s>) al documento\n",
    "        doc_tokens = [self.SOS]*(self.N-1) + doc_tokens + [self.EOS]\n",
    "        # Generar y retornar n-gramas del documento procesado\n",
    "        return list(ngrams(doc_tokens, self.N))\n",
    "    \n",
    "    # Reemplaza tokens desconocidos en una lista de tokens por el token <unk>\n",
    "    def replace_unk(self, doc_tokens: list) -> list:\n",
    "        # Iterar sobre cada token en la lista de tokens\n",
    "        for i, token in enumerate(doc_tokens):\n",
    "            # Si el token no está en el vocabulario, reemplazarlo por <unk>\n",
    "            if token.lower() not in self.vocab:\n",
    "                doc_tokens[i] = self.UNK\n",
    "        # Retornar la lista de tokens con los desconocidos reemplazados\n",
    "        return doc_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesitamos ahora cargar los embeddings. Implementamos una función para esto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(file_path):\n",
    "    embeddings_dict = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embeddings_dict[word] = vector\n",
    "    return embeddings_dict\n",
    "\n",
    "embeddings_dict = load_embeddings(\"word2vec_col.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora necesitamos construir el modelo, integramos el embedding en la clase que construye el modelo como objeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = TweetTokenizer() # Inicializamos el tokenizador de tweets\n",
    "\n",
    "# Creamos un objeto con la clase que definimos\n",
    "ngram_data = NgramData(args.N, 5000, tk.tokenize)\n",
    "ngram_data.fit(X_train) # Construye el vocabulario\n",
    "\n",
    "# Transformación de datos\n",
    "X_ngram_train, y_ngram_train = ngram_data.transform(X_train)\n",
    "X_ngram_val, y_ngram_val = ngram_data.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([64, 3])\n",
      "y shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Pytorch \n",
    "\n",
    "args.batch_size = 64\n",
    "# Num workers\n",
    "args.num_workers = 2\n",
    "\n",
    "# Convertimos los datos a tensores de pytorch\n",
    "train_dataset = TensorDataset(torch.tensor(X_ngram_train, dtype = torch.int64),\n",
    "                              torch.tensor(y_ngram_train, dtype = torch.int64))\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size = args.batch_size,\n",
    "                          num_workers=args.num_workers,\n",
    "                          shuffle = True)\n",
    "\n",
    "val_dataset = TensorDataset(torch.tensor(X_ngram_val, dtype = torch.int64),\n",
    "                            torch.tensor(y_ngram_val, dtype = torch.int64))\n",
    "\n",
    "val_loader = DataLoader(train_dataset,\n",
    "                          batch_size = args.batch_size,\n",
    "                          num_workers=args.num_workers,\n",
    "                          shuffle = False)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "print(f'X shape: {batch[0].shape}')\n",
    "print(f'y shape: {batch[1].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab size\n",
    "args.vocab_size = ngram_data.get_vocab_size()\n",
    "\n",
    "# Dimension of word embeddings\n",
    "args.d = 100\n",
    "\n",
    "# Dimension for hidden layer\n",
    "args.d_h = 100\n",
    "\n",
    "# Dropout\n",
    "args.dropout = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparamos una matriz de embeddings que se pueda usar para inicializar la capa de embeddings en el modelo de PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = args.d\n",
    "vocab_size = args.vocab_size\n",
    "\n",
    "# Obteniendo el mapeo de palabra a ID desde tu instancia de NgramData\n",
    "word_to_id = ngram_data.w2id\n",
    "\n",
    "# Inicializa la matriz de embeddings con ceros\n",
    "embeddings_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "# Llena la matriz con los embeddings para cada palabra en el vocabulario\n",
    "for word, i in word_to_id.items():\n",
    "    embedding_vector = embeddings_dict.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embeddings_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLM(nn.Module):\n",
    "    def __init__(self, args, embeddings_matrix):\n",
    "        super(NeuralLM, self).__init__()\n",
    "        \n",
    "        # Tamaño de la ventana de entrada (contexto) para el modelo, basado en N-1\n",
    "        # donde N es el tamaño de los n-gramas considerados.\n",
    "        self.window_size = args.N-1\n",
    "        \n",
    "        # Dimensión de los embeddings de palabras, que transforman índices de palabras\n",
    "        # en vectores densos que capturan información semántica.\n",
    "        self.embedding_dim = args.d\n",
    "        \n",
    "        # Inicializa capa de embeddings\n",
    "        self.emb = nn.Embedding(args.vocab_size, self.embedding_dim)\n",
    "\n",
    "        # Convierte embeddings_matrix de NumPy array a PyTorch tensor\n",
    "        embeddings_tensor = torch.tensor(embeddings_matrix, dtype=torch.float)\n",
    "\n",
    "        self.emb.weight.data.copy_(embeddings_tensor)  # Establece los pesos de la capa de embedding\n",
    "        \n",
    "        # Otras capas\n",
    "        self.fc1 = nn.Linear(self.embedding_dim * self.window_size, args.d_h)\n",
    "        self.drop1 = nn.Dropout(p=args.dropout)\n",
    "        self.fc2 = nn.Linear(args.d_h, args.vocab_size, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Transforma los índices de palabras en x en embeddings de palabras.\n",
    "        x = self.emb(x)\n",
    "        \n",
    "        # Aplana los embeddings en un vector único para cada muestra en el lote,\n",
    "        # preparándolos para la entrada a la capa lineal.\n",
    "        x = x.view(-1, self.window_size * self.embedding_dim)\n",
    "        \n",
    "        # Pasa la entrada aplanada a través de la primera capa lineal y luego\n",
    "        # a través de una función de activación ReLU.\n",
    "        h = F.relu(self.fc1(x))\n",
    "        \n",
    "        # Aplica Dropout a la representación de la capa oculta para mejorar la generalización.\n",
    "        h = self.drop1(h)\n",
    "        \n",
    "        # La salida final se obtiene pasando la representación de la capa oculta después de Dropout\n",
    "        # a través de la segunda capa lineal, generando los logits para cada palabra en el vocabulario.\n",
    "        return self.fc2(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta función toma los logits (es decir, las salidas no normalizadas de un modelo) \n",
    "# y devuelve las predicciones de clase como índices.\n",
    "def get_preds(raw_logits):\n",
    "    # Calcula las probabilidades aplicando la función softmax a los logits.\n",
    "    # La operación detach() se usa para evitar que se calculen gradientes para estas operaciones,\n",
    "    # ya que solo se necesitan las probabilidades para hacer predicciones.\n",
    "    probs = F.softmax(raw_logits.detach(), dim=1)\n",
    "    \n",
    "    # Encuentra el índice de la mayor probabilidad en cada fila (es decir, para cada ejemplo en el lote),\n",
    "    # que corresponde a la clase predicha. Luego, convierte el tensor a un array de NumPy.\n",
    "    y_pred = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "\n",
    "# Evalúa el modelo en un conjunto de datos proporcionado y devuelve la precisión del modelo.\n",
    "def model_eval(data, model, gpu=False):\n",
    "    # Desactiva el cálculo de gradientes para acelerar las cosas y reducir el uso de memoria\n",
    "    # ya que no se necesita para la evaluación.\n",
    "    with torch.no_grad():\n",
    "        preds, tgts = [], []  # Listas para almacenar predicciones y etiquetas verdaderas\n",
    "        \n",
    "        # Itera sobre los lotes de datos en el DataLoader\n",
    "        for window_words, labels in data:\n",
    "            # Si se utiliza GPU, mueve los datos al dispositivo adecuado\n",
    "            if gpu:\n",
    "                window_words = window_words.cuda()\n",
    "                \n",
    "            # Obtiene los logits del modelo para el lote actual\n",
    "            outputs = model(window_words)\n",
    "            \n",
    "            # Obtiene las predicciones de clase para el lote actual utilizando la función get_preds\n",
    "            y_pred = get_preds(outputs)\n",
    "            \n",
    "            # Extrae las etiquetas verdaderas del lote actual y las convierte a un array de NumPy\n",
    "            tgt = labels.numpy()\n",
    "            \n",
    "            # Almacena las predicciones y las etiquetas verdaderas\n",
    "            tgts.append(tgt)\n",
    "            preds.append(y_pred)\n",
    "        \n",
    "        # Aplana las listas de listas para obtener una única lista de etiquetas y predicciones\n",
    "        tgts = [e for l in tgts for e in l]\n",
    "        preds = [e for l in preds for e in l]\n",
    "        \n",
    "        # Calcula y devuelve la precisión del modelo comparando las predicciones con las etiquetas verdaderas\n",
    "        return accuracy_score(tgts, preds)\n",
    "    \n",
    "\n",
    "# Guarda el estado actual del modelo y, si es el mejor modelo hasta el momento según \n",
    "# algún criterio, guarda una copia separada.\n",
    "def save_checkpoint(state, is_best, checkpoint_path, filename=\"checkpoint.pt\"):\n",
    "    # Construye la ruta completa del archivo donde se guardará el estado del modelo\n",
    "    filename = os.path.join(checkpoint_path, filename)\n",
    "    \n",
    "    # Guarda el estado del modelo en la ruta especificada\n",
    "    torch.save(state, filename)\n",
    "    \n",
    "    # Si el modelo actual es el \"mejor\" según algún criterio, guarda una copia separada\n",
    "    if is_best:\n",
    "        # Copia el archivo del checkpoint al archivo del \"mejor modelo\"\n",
    "        shutil.copyfile(filename, os.path.join(checkpoint_path, \"model_best.pt\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definición de modelo e hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guillermo_sego/anaconda3/envs/PLN/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "# Model hyperparameters\n",
    "args.vocab_size = ngram_data.get_vocab_size() \n",
    "args.d = 100\n",
    "args.d_h = 200\n",
    "args.dropout = 0.1\n",
    "\n",
    "# Training hyperparameters\n",
    "args.lr = 2.3e-1\n",
    "args.num_epochs = 20\n",
    "args.patience = 20\n",
    "\n",
    "# Scheduler hyperparameters\n",
    "args.lr_patience = 10\n",
    "args.lr_factor = 0.5\n",
    "\n",
    "# Saving directoty\n",
    "args.savedir = 'model'\n",
    "os.makedirs(args.savedir, exist_ok=True)\n",
    "\n",
    "# Create model\n",
    "model = NeuralLM(args, embeddings_matrix)\n",
    "\n",
    "# Send to GPU\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "if args.use_gpu:\n",
    "    model.cuda()\n",
    "    \n",
    "# Loss, optimizer an scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr= args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, 'min',\n",
    "                patience=args.lr_patience,\n",
    "                verbose=True,\n",
    "                factor=args.lr_factor\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento de la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.1483213779978307\n",
      "Epoch [1/20], Loss: 5.7387 - Val accuracy: 0.1709 - Epoch time: 1711171210.20\n",
      "Train acc: 0.16737229090507375\n",
      "Epoch [2/20], Loss: 5.2152 - Val accuracy: 0.1934 - Epoch time: 1711171223.80\n",
      "Train acc: 0.1714123950508175\n",
      "Epoch [3/20], Loss: 5.0171 - Val accuracy: 0.1873 - Epoch time: 1711171241.22\n",
      "Train acc: 0.17458158418029165\n",
      "Epoch [4/20], Loss: 4.8582 - Val accuracy: 0.1812 - Epoch time: 1711171254.90\n",
      "Train acc: 0.17931904802554935\n",
      "Epoch [5/20], Loss: 4.7290 - Val accuracy: 0.1404 - Epoch time: 1711171268.79\n",
      "Train acc: 0.1808769659141124\n",
      "Epoch [6/20], Loss: 4.6089 - Val accuracy: 0.1797 - Epoch time: 1711171282.47\n",
      "Train acc: 0.18392030390069494\n",
      "Epoch [7/20], Loss: 4.5016 - Val accuracy: 0.2164 - Epoch time: 1711171296.15\n",
      "Train acc: 0.18688110111276263\n",
      "Epoch [8/20], Loss: 4.4083 - Val accuracy: 0.1850 - Epoch time: 1711171309.87\n",
      "Train acc: 0.19044981269834896\n",
      "Epoch [9/20], Loss: 4.3219 - Val accuracy: 0.2199 - Epoch time: 1711171323.55\n",
      "Train acc: 0.19461514030048607\n",
      "Epoch [10/20], Loss: 4.2464 - Val accuracy: 0.2172 - Epoch time: 1711171337.23\n",
      "Train acc: 0.1979704384766802\n",
      "Epoch [11/20], Loss: 4.1756 - Val accuracy: 0.2308 - Epoch time: 1711171350.94\n",
      "Train acc: 0.1998324704736271\n",
      "Epoch [12/20], Loss: 4.1071 - Val accuracy: 0.2307 - Epoch time: 1711171364.77\n",
      "Train acc: 0.20521142992005786\n",
      "Epoch [13/20], Loss: 4.0484 - Val accuracy: 0.2381 - Epoch time: 1711171378.47\n",
      "Train acc: 0.20847916582975132\n",
      "Epoch [14/20], Loss: 3.9902 - Val accuracy: 0.2411 - Epoch time: 1711171392.41\n",
      "Train acc: 0.2126859207407705\n",
      "Epoch [15/20], Loss: 3.9460 - Val accuracy: 0.2546 - Epoch time: 1711171406.11\n",
      "Train acc: 0.21757528471859558\n",
      "Epoch [16/20], Loss: 3.8998 - Val accuracy: 0.2634 - Epoch time: 1711171420.00\n",
      "Train acc: 0.26259452958663076\n",
      "Epoch [17/20], Loss: 3.4593 - Val accuracy: 0.3114 - Epoch time: 1711171434.39\n",
      "Train acc: 0.27113608745430445\n",
      "Epoch [18/20], Loss: 3.3718 - Val accuracy: 0.3223 - Epoch time: 1711171448.18\n",
      "Train acc: 0.27528070140200056\n",
      "Epoch [19/20], Loss: 3.3307 - Val accuracy: 0.3123 - Epoch time: 1711171461.76\n",
      "Train acc: 0.2780833216165187\n",
      "Epoch [20/20], Loss: 3.3139 - Val accuracy: 0.3341 - Epoch time: 1711171476.16\n",
      "--- 279.70687222480774 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "best_metric = 0\n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "for epoch in range(args.num_epochs):\n",
    "    epoch_stats_time = time.time()\n",
    "    loss_epoch = []\n",
    "    training_metric = []\n",
    "    model.train()\n",
    "    \n",
    "    for window_words, labels in train_loader:\n",
    "        if args.use_gpu:\n",
    "            window_words = window_words.cuda()\n",
    "            labels = labels.cuda()\n",
    "            \n",
    "        #Forward pass\n",
    "        outputs = model(window_words)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_epoch.append(loss.item())\n",
    "        \n",
    "        #Get training metrics\n",
    "        y_pred = get_preds(outputs)\n",
    "        tgt = labels.cpu().numpy()\n",
    "        training_metric.append(accuracy_score(tgt, y_pred))\n",
    "        \n",
    "        #Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    #Get metric in training dataset\n",
    "    mean_epoch_metric = np.mean(training_metric)\n",
    "    train_metric_history.append(mean_epoch_metric)\n",
    "    \n",
    "    #Get metric in validation dataset\n",
    "    model.eval()\n",
    "    tuning_metric = model_eval(val_loader, model, gpu=args.use_gpu)\n",
    "    metric_history.append(mean_epoch_metric)\n",
    "    \n",
    "    #update scheduler\n",
    "    scheduler.step(tuning_metric)\n",
    "    \n",
    "    #Check for metric improvement\n",
    "    is_improvement = tuning_metric > best_metric\n",
    "    if is_improvement:\n",
    "        best_metric = tuning_metric\n",
    "        n_no_improve = 0\n",
    "    else:\n",
    "        n_no_improve = 1\n",
    "        \n",
    "    save_checkpoint(\n",
    "        {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"scheduler\": scheduler.state_dict(),\n",
    "            \"best_metric\": best_metric,\n",
    "        },\n",
    "        is_improvement,\n",
    "        args.savedir,\n",
    "    )\n",
    "    \n",
    "    #Early stopping\n",
    "    if n_no_improve >= args.patience:\n",
    "        print(\"No improvement. Breaking out of loop.\")\n",
    "        \n",
    "    print(\"Train acc: {}\".format(mean_epoch_metric))\n",
    "    print(\"Epoch [{}/{}], Loss: {:.4f} - Val accuracy: {:.4f} - Epoch time: {:.2f}\"\n",
    "         .format(epoch+1, args.num_epochs, np.mean(loss_epoch), tuning_metric, (time.time())))\n",
    "    \n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truncamos a 20 debido al tiempo de entrega de la tarea :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación del modelo\n",
    "Ahora evaluamos y comparamos el modelo con el desarrollado en clase. Primero necesitamos revisar las palabras mas similares en el contexto dado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_closest_words(embeddings, ngram_data, word, n):\n",
    "    word_id = torch.LongTensor([ngram_data.w2id[word]])\n",
    "    word_embed = embeddings(word_id)\n",
    "    dists = torch.norm(embeddings.weight - word_embed, dim = 1).detach()\n",
    "    lst = sorted(enumerate(dists.numpy()), key = lambda x: x[1])\n",
    "    for idx, difference in lst[1:n+1]:\n",
    "        print(ngram_data.id2w[idx], difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el mejor modelo que se haya guardado y proseguimos para hacer una visualización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "lindo 10.560313\n",
      "hermoso 15.729565\n",
      "feo 19.227892\n",
      "reconfortante 20.34947\n",
      "gracioso 21.105967\n",
      "perfecto 21.274488\n",
      "chistoso 21.696632\n",
      "chingon 21.848997\n",
      "chingón 22.155758\n",
      "guapo 22.157343\n"
     ]
    }
   ],
   "source": [
    "# Model with learned embeddings from scratch\n",
    "best_model = NeuralLM(args, embeddings_matrix)\n",
    "best_model.load_state_dict(torch.load(\"model/model_best.pt\")[\"state_dict\"])\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"Learned embeddings\")\n",
    "print(\"-\"*30)\n",
    "print_closest_words(best_model.emb, ngram_data, \"bonito\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, generamos texto con el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(text, tokenizer):\n",
    "    # Tokeniza el texto y convierte cada palabra a minúsculas. Si la palabra está en el vocabulario (w2id), la usa;\n",
    "    # de lo contrario, la reemplaza por el token \"<unk>\" para palabras desconocidas.\n",
    "    all_tokens = [w.lower() if w.lower() in ngram_data.w2id else \"<unk>\" for w in tokenizer.tokenize(text)]\n",
    "    \n",
    "    # Convierte los tokens a sus índices numéricos correspondientes según el mapeo w2id en ngram_data.\n",
    "    token_ids = [ngram_data.w2id[word.lower()] for word in all_tokens]\n",
    "    \n",
    "    return all_tokens, token_ids\n",
    "\n",
    "\n",
    "def sample_next_word(logits, temperature=1.0):\n",
    "    # Convierte los logits a un array de numpy y ajusta la \"temperatura\" de la predicción.\n",
    "    logits = np.asarray(logits).astype(\"float64\")\n",
    "    preds = logits / temperature\n",
    "    \n",
    "    # Convierte los logits ajustados a probabilidades usando softmax.\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    \n",
    "    # Muestrea un índice de palabra de la distribución de probabilidades.\n",
    "    probas = np.random.multinomial(1, preds)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def predict_next_token(model, token_ids):\n",
    "    # Convierte la lista de índices de tokens a un tensor de PyTorch y agrega una dimensión de lote.\n",
    "    word_ids_tensor = torch.LongTensor(token_ids).unsqueeze(0)\n",
    "    \n",
    "    # Obtiene los logits de la predicción del modelo para la secuencia de tokens y los convierte a numpy.\n",
    "    y_raw_pred = model(word_ids_tensor).squeeze(0).detach().numpy()\n",
    "    \n",
    "    # Muestra el índice de la siguiente palabra de la distribución de logits.\n",
    "    y_pred = sample_next_word(y_raw_pred, 1.0)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def generate_sentence(model, initial_text, tokenizer):\n",
    "    # Obtiene tokens y sus índices del texto inicial.\n",
    "    all_tokens, window_word_ids = parse_text(initial_text, tokenizer)\n",
    "    \n",
    "    # Genera hasta 100 palabras adicionales.\n",
    "    for i in range(100):\n",
    "        # Predice el índice de la siguiente palabra utilizando el modelo.\n",
    "        y_pred = predict_next_token(model, window_word_ids)\n",
    "        next_word = ngram_data.id2w[y_pred]  # Convierte el índice de palabra predicho a texto.\n",
    "        all_tokens.append(next_word)  # Añade la palabra predicha a la lista de tokens.\n",
    "        \n",
    "        # Si se genera el token de fin de secuencia, detiene la generación.\n",
    "        if next_word == \"</s>\":\n",
    "            break\n",
    "        else:\n",
    "            # Actualiza la ventana de palabras para la siguiente predicción.\n",
    "            window_word_ids.pop(0)  # Elimina la primera palabra.\n",
    "            window_word_ids.append(y_pred)  # Añade la nueva palabra al final.\n",
    "    \n",
    "    # Une los tokens generados en una cadena de texto y la devuelve.\n",
    "    return \" \".join(all_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "<s> <s> <s> vea al puto pendejo <unk> mi sangre <unk> <unk> … </s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = \"<s><s><s>\"\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"Learned embeddings\")\n",
    "print(\"-\"*30)\n",
    "print(generate_sentence(best_model, initial_tokens, tk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, medimos el valor del likelihood del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(model, text, ngram_model):\n",
    "    # Transforma el texto dado en n-gramas (X) y las etiquetas objetivo (y) utilizando el modelo n-gram.\n",
    "    X, y = ngram_data.transform([text])\n",
    "    \n",
    "    # Ignora los primeros dos n-gramas. Esto podría ser específico para cómo se estructura el texto o los n-gramas.\n",
    "    X, y = X[2:], y[2:]\n",
    "    \n",
    "    # Convierte X en un tensor de PyTorch y agrega una dimensión de lote, preparándolo para el modelo.\n",
    "    X = torch.LongTensor(X).unsqueeze(0)\n",
    "    \n",
    "    # Obtiene los logits del modelo para el texto transformado y luego los desconecta del grafo de cómputo.\n",
    "    logits = model(X).detach()\n",
    "    \n",
    "    # Aplica softmax a los logits para obtener una distribución de probabilidades sobre el vocabulario.\n",
    "    probs = F.softmax(logits, dim=1).numpy()\n",
    "    \n",
    "    # Calcula la log-verosimilitud sumando los logaritmos de las probabilidades de las palabras reales (y)\n",
    "    # según lo predicho por el modelo.\n",
    "    return np.sum([np.log(probs[i][w]) for i, w in enumerate(y)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -28.747715\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model, \"Estamos en la clase de procesamiento de lenguaje\", ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -24.890015\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model, \"Sergio Perez es el mejor piloto de f1\", ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -42.84663\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model, \"Mexico esta en decadencia por los narcos\", ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -40.24046\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model, \"Tonto, viejo malo, que te pasa\", ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -54.03869\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model, \"Horrible, todo esta muy mal, que horrible\", ngram_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estructura morfológica\n",
    "\n",
    "Evaluamos las permutaciones del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-22.08913 sino gano me voy a la chingada\n",
      "-24.907225 voy sino me gano a la chingada\n",
      "-25.02383 sino voy me gano a la chingada\n",
      "-25.140871 gano sino me voy a la chingada\n",
      "-25.212936 gano sino me voy la chingada a\n",
      "--------------------------------------------------\n",
      "-82.64427 a la voy gano chingada sino me\n",
      "-83.02557 a la gano voy sino chingada me\n",
      "-84.76806 a la gano voy chingada me sino\n",
      "-86.985146 a la voy gano chingada me sino\n",
      "-89.53535 la voy gano chingada a me sino\n"
     ]
    }
   ],
   "source": [
    "from itertools import permutations\n",
    "from random import shuffle\n",
    "\n",
    "word_list = \"sino gano me voy a la chingada\".split(\" \")\n",
    "perms = [\" \".join(perm) for perm in permutations(word_list)]\n",
    "#print(len(perms))\n",
    "print(\"-\"*50)\n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse = True)[:5]:\n",
    "    print(p, t)\n",
    "print(\"-\"*50)  \n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse = True)[-5:]:\n",
    "    print(p, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo comparamos con el modelo de la clase. Los valores de la perplejidad fueron mejores en el modelo con embeddings. La integración de embeddings preentrenados en el modelo de lenguaje neuronal muestra una mejora significativa en la coherencia y naturalidad de las secuencias de palabras generadas, como evidencian las perplejidades reducidas tanto en oraciones bien estructuradas como en aquellas con estructuras sintácticas menos convencionales. La disminución de la perplejidad con embeddings preentrenados sugiere una comprensión semántica más rica y una capacidad mejorada para capturar contextos y relaciones entre palabras, lo que resulta en un modelo más robusto y eficaz en la generación de texto natural. Este contraste resalta el valor de los embeddings preentrenados en enriquecer modelos de lenguaje con conocimiento semántico previo, facilitando así una generación de lenguaje más coherente y fluida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 3\n",
    "\n",
    "**3. (OPCIONAL: 30pts extra en ESTA tarea, calificación máxima para promediar con otras tareas: 130) A partir del modelo anterior haga un modelo de lenguaje que integre una conexión directa de la capa de embeddings hacía la salida, justo como lo proponía Bengio. Discuta sobre las diferencias en el proceso de entrenamiento y la perplejidad respecto al modelo anterior y el visto en clase.**\n",
    "\n",
    "Para poder agregar una capa extra al embedding hay que modidicar la clase **NeuralLM** para integrar la capa extra propuesta por el modelo de Bengio. En este modelo modificado, hay dos caminos desde la capa de embeddings hacia la salida:\n",
    "\n",
    "* Conexión Directa: Una conexión lineal directa desde los embeddings aplanados hasta la salida. Esto está representado por la capa self.direct_conn. Esta conexión intenta capturar las relaciones lineales entre las palabras y su contexto.\n",
    "\n",
    "* Ruta No Lineal: La ruta original que pasa a través de una capa oculta (y potencialmente más capas) antes de llegar a la salida. Esto permite que el modelo capture relaciones no lineales complejas.\n",
    "\n",
    "La salida final del modelo es la suma de las salidas de ambos caminos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLM(nn.Module):\n",
    "    def __init__(self, args, embeddings_matrix):\n",
    "        super(NeuralLM, self).__init__()\n",
    "        \n",
    "        self.window_size = args.N - 1\n",
    "        self.embedding_dim = args.d\n",
    "        \n",
    "        # Capa de embedding\n",
    "        self.emb = nn.Embedding(args.vocab_size, self.embedding_dim)\n",
    "        \n",
    "        # Convierte la matriz de NumPy a un tensor de PyTorch y asigna como pesos preentrenados\n",
    "        embeddings_tensor = torch.tensor(embeddings_matrix, dtype=torch.float32)\n",
    "        self.emb.weight = nn.Parameter(embeddings_tensor, requires_grad=False)  \n",
    "        \n",
    "        # Capa lineal para la conexión directa desde los embeddings a la salida\n",
    "        self.direct_conn = nn.Linear(self.embedding_dim * self.window_size, args.vocab_size)\n",
    "        \n",
    "        # Capas para la ruta no lineal\n",
    "        self.fc1 = nn.Linear(self.embedding_dim * self.window_size, args.d_h)\n",
    "        self.drop1 = nn.Dropout(p=args.dropout)\n",
    "        self.fc2 = nn.Linear(args.d_h, args.vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Transforma los índices de palabras en x en embeddings de palabras\n",
    "        embeddings = self.emb(x)\n",
    "        \n",
    "        # Aplana los embeddings para pasarlos a través de las capas lineales\n",
    "        flattened = embeddings.view(-1, self.window_size * self.embedding_dim)\n",
    "        \n",
    "        # Conexión directa de embeddings a salida\n",
    "        direct_output = self.direct_conn(flattened)\n",
    "        \n",
    "        # Ruta no lineal\n",
    "        h = F.relu(self.fc1(flattened))\n",
    "        h = self.drop1(h)\n",
    "        non_linear_output = self.fc2(h)\n",
    "        \n",
    "        # Suma las salidas de las conexiones directa y no lineal\n",
    "        combined_output = direct_output + non_linear_output\n",
    "        \n",
    "        return combined_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definición e hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guillermo_sego/anaconda3/envs/PLN/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "# Model hyperparameters\n",
    "args.vocab_size = ngram_data.get_vocab_size() \n",
    "args.d = 100\n",
    "args.d_h = 200\n",
    "args.dropout = 0.1\n",
    "\n",
    "# Training hyperparameters\n",
    "args.lr = 2.3e-1\n",
    "args.num_epochs = 20\n",
    "args.patience = 20\n",
    "\n",
    "# Scheduler hyperparameters\n",
    "args.lr_patience = 10\n",
    "args.lr_factor = 0.5\n",
    "\n",
    "# Saving directoty\n",
    "args.savedir = 'model'\n",
    "os.makedirs(args.savedir, exist_ok=True)\n",
    "\n",
    "# Create model\n",
    "model = NeuralLM(args, embeddings_matrix)\n",
    "\n",
    "# Send to GPU\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "if args.use_gpu:\n",
    "    model.cuda()\n",
    "    \n",
    "# Loss, optimizer an scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr= args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, 'min',\n",
    "                patience=args.lr_patience,\n",
    "                verbose=True,\n",
    "                factor=args.lr_factor\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.11172097828706866\n",
      "Epoch [1/20], Loss: 8.8886 - Val accuracy: 0.1493 - Epoch time: 1711171504.28\n",
      "Train acc: 0.12053024444622985\n",
      "Epoch [2/20], Loss: 7.8851 - Val accuracy: 0.1377 - Epoch time: 1711171524.59\n",
      "Train acc: 0.127338132205841\n",
      "Epoch [3/20], Loss: 7.6160 - Val accuracy: 0.1521 - Epoch time: 1711171547.32\n",
      "Train acc: 0.13876861718153696\n",
      "Epoch [4/20], Loss: 7.3455 - Val accuracy: 0.1675 - Epoch time: 1711171566.42\n",
      "Train acc: 0.15113623809906399\n",
      "Epoch [5/20], Loss: 7.1834 - Val accuracy: 0.1622 - Epoch time: 1711171589.28\n",
      "Train acc: 0.16253628027959668\n",
      "Epoch [6/20], Loss: 7.0548 - Val accuracy: 0.1890 - Epoch time: 1711171613.84\n",
      "Train acc: 0.1731149946772185\n",
      "Epoch [7/20], Loss: 6.9522 - Val accuracy: 0.1959 - Epoch time: 1711171637.29\n",
      "Train acc: 0.18045076678182623\n",
      "Epoch [8/20], Loss: 6.8860 - Val accuracy: 0.1979 - Epoch time: 1711171657.94\n",
      "Train acc: 0.18811670198449362\n",
      "Epoch [9/20], Loss: 6.8445 - Val accuracy: 0.1805 - Epoch time: 1711171679.59\n",
      "Train acc: 0.19489006698670305\n",
      "Epoch [10/20], Loss: 6.7720 - Val accuracy: 0.2239 - Epoch time: 1711171699.58\n",
      "Train acc: 0.19996554001124814\n",
      "Epoch [11/20], Loss: 6.7750 - Val accuracy: 0.2082 - Epoch time: 1711171720.00\n",
      "Train acc: 0.20621102066846103\n",
      "Epoch [12/20], Loss: 6.7051 - Val accuracy: 0.2347 - Epoch time: 1711171740.36\n",
      "Train acc: 0.21136087454304422\n",
      "Epoch [13/20], Loss: 6.6730 - Val accuracy: 0.2150 - Epoch time: 1711171760.05\n",
      "Train acc: 0.2820364911822601\n",
      "Epoch [14/20], Loss: 4.2028 - Val accuracy: 0.3274 - Epoch time: 1711171781.12\n",
      "Train acc: 0.2941351484353031\n",
      "Epoch [15/20], Loss: 3.9309 - Val accuracy: 0.3220 - Epoch time: 1711171804.22\n",
      "Train acc: 0.299402379685052\n",
      "Epoch [16/20], Loss: 3.8674 - Val accuracy: 0.3302 - Epoch time: 1711171827.04\n",
      "Train acc: 0.3017853914755152\n",
      "Epoch [17/20], Loss: 3.8353 - Val accuracy: 0.3427 - Epoch time: 1711171851.36\n",
      "Train acc: 0.3039349038886434\n",
      "Epoch [18/20], Loss: 3.8125 - Val accuracy: 0.3373 - Epoch time: 1711171871.70\n",
      "Train acc: 0.3063069311653879\n",
      "Epoch [19/20], Loss: 3.7894 - Val accuracy: 0.3341 - Epoch time: 1711171891.84\n",
      "Train acc: 0.3080685333226208\n",
      "Epoch [20/20], Loss: 3.7681 - Val accuracy: 0.3239 - Epoch time: 1711171911.97\n",
      "--- 426.13516330718994 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "best_metric = 0\n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "for epoch in range(args.num_epochs):\n",
    "    epoch_stats_time = time.time()\n",
    "    loss_epoch = []\n",
    "    training_metric = []\n",
    "    model.train()\n",
    "    \n",
    "    for window_words, labels in train_loader:\n",
    "        if args.use_gpu:\n",
    "            window_words = window_words.cuda()\n",
    "            labels = labels.cuda()\n",
    "            \n",
    "        #Forward pass\n",
    "        outputs = model(window_words)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_epoch.append(loss.item())\n",
    "        \n",
    "        #Get training metrics\n",
    "        y_pred = get_preds(outputs)\n",
    "        tgt = labels.cpu().numpy()\n",
    "        training_metric.append(accuracy_score(tgt, y_pred))\n",
    "        \n",
    "        #Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    #Get metric in training dataset\n",
    "    mean_epoch_metric = np.mean(training_metric)\n",
    "    train_metric_history.append(mean_epoch_metric)\n",
    "    \n",
    "    #Get metric in validation dataset\n",
    "    model.eval()\n",
    "    tuning_metric = model_eval(val_loader, model, gpu=args.use_gpu)\n",
    "    metric_history.append(mean_epoch_metric)\n",
    "    \n",
    "    #update scheduler\n",
    "    scheduler.step(tuning_metric)\n",
    "    \n",
    "    #Check for metric improvement\n",
    "    is_improvement = tuning_metric > best_metric\n",
    "    if is_improvement:\n",
    "        best_metric = tuning_metric\n",
    "        n_no_improve = 0\n",
    "    else:\n",
    "        n_no_improve = 1\n",
    "        \n",
    "    save_checkpoint(\n",
    "        {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"scheduler\": scheduler.state_dict(),\n",
    "            \"best_metric\": best_metric,\n",
    "        },\n",
    "        is_improvement,\n",
    "        args.savedir,\n",
    "    )\n",
    "    \n",
    "    #Early stopping\n",
    "    if n_no_improve >= args.patience:\n",
    "        print(\"No improvement. Breaking out of loop.\")\n",
    "        \n",
    "    print(\"Train acc: {}\".format(mean_epoch_metric))\n",
    "    print(\"Epoch [{}/{}], Loss: {:.4f} - Val accuracy: {:.4f} - Epoch time: {:.2f}\"\n",
    "         .format(epoch+1, args.num_epochs, np.mean(loss_epoch), tuning_metric, (time.time())))\n",
    "    \n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación del modelo\n",
    "Ahora comparamos el modelo con el realizado en el inciso anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "lindo 10.613454\n",
      "hermoso 15.747205\n",
      "feo 19.22297\n",
      "reconfortante 20.328833\n",
      "gracioso 21.1011\n",
      "perfecto 21.245314\n",
      "chistoso 21.689884\n",
      "chingon 21.850063\n",
      "chingón 22.149546\n",
      "genial 22.179726\n"
     ]
    }
   ],
   "source": [
    "# Model with learned embeddings from scratch\n",
    "best_model = NeuralLM(args, embeddings_matrix)\n",
    "best_model.load_state_dict(torch.load(\"model/model_best.pt\")[\"state_dict\"])\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"Learned embeddings\")\n",
    "print(\"-\"*30)\n",
    "print_closest_words(best_model.emb, ngram_data, \"bonito\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "<s> <s> <s> me queda claro <unk> </s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = \"<s><s><s>\"\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"Learned embeddings\")\n",
    "print(\"-\"*30)\n",
    "print(generate_sentence(best_model, initial_tokens, tk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -34.184837\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model, \"Estamos en la clase de procesamiento de lenguaje\", ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -49.329678\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model, \"Sergio Perez es el mejor piloto de f1\", ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -44.581127\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model, \"Mexico esta en decadencia por los narcos\", ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -53.303963\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model, \"Tonto, viejo malo, que te pasa\", ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -44.472637\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model, \"Horrible, todo esta muy mal, que horrible\", ngram_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparación estructura morfológica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "-38.66899 sino gano a la chingada me voy\n",
      "-38.979294 chingada sino gano a la me voy\n",
      "-40.775017 gano sino la chingada me voy a\n",
      "-42.651367 gano chingada a la me voy sino\n",
      "-42.77483 chingada sino gano la me voy a\n",
      "--------------------------------------------------\n",
      "-133.49619 me la a sino chingada voy gano\n",
      "-136.30128 la me a chingada sino voy gano\n",
      "-136.89542 me a chingada sino gano voy la\n",
      "-141.6242 me la a sino voy gano chingada\n",
      "-142.57162 me la a sino gano voy chingada\n"
     ]
    }
   ],
   "source": [
    "word_list = \"sino gano me voy a la chingada\".split(\" \")\n",
    "perms = [\" \".join(perm) for perm in permutations(word_list)]\n",
    "#print(len(perms))\n",
    "print(\"-\"*50)\n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse = True)[:5]:\n",
    "    print(p, t)\n",
    "print(\"-\"*50)  \n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse = True)[-5:]:\n",
    "    print(p, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discusión sobre el entrenamiento y la perplejidad:\n",
    "\n",
    "La comparación entre los modelos con y sin la conexión directa desde la capa de embeddings hacia la salida revela diferencias significativas en los valores de perplejidad, reflejando el impacto de la arquitectura del modelo en su capacidad para entender y generar secuencias de palabras coherentes.\n",
    "\n",
    "El modelo **con embeddings preentrenados**, pero **sin la conexión directa**, muestra perplejidades más bajas tanto para oraciones coherentes como para aquellas con estructuras menos convencionales. Esto indica una mejora en la capacidad del modelo para generar secuencias de palabras naturales y coherentes, probablemente debido a la rica información semántica proporcionada por los embeddings.\n",
    "\n",
    "Por otro lado, el modelo **con la conexión directa** muestra un aumento en los valores de perplejidad para ambos tipos de oraciones. Aunque esta arquitectura, inspirada en Bengio, tiene el potencial de capturar tanto relaciones lineales como no lineales directamente desde los embeddings, el aumento en la perplejidad sugiere que podría estar enfrentando desafíos, como el sobreajuste o la dificultad para integrar efectivamente esta información adicional en la generación de secuencias de palabras.\n",
    "\n",
    "- La **disminución de la perplejidad** en el modelo con embeddings, pero sin la conexión directa, subraya cómo una comprensión semántica más profunda facilitada por los embeddings puede mejorar la coherencia del texto generado.\n",
    "- El **aumento de la perplejidad** en el modelo con la conexión directa sugiere que la integración de esta ruta adicional en el modelo no está siendo tan efectiva como se esperaba, lo que podría deberse a varios factores, como la complejidad adicional en la arquitectura o la necesidad de una regulación más fina para evitar el sobreajuste.\n",
    "- Estas observaciones subrayan la importancia de equilibrar la riqueza semántica proporcionada por los embeddings preentrenados con la complejidad arquitectónica del modelo, asegurando que las adiciones a la arquitectura contribuyan efectivamente a la tarea de generación de lenguaje sin introducir dificultades adicionales en el aprendizaje."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PLN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
