{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 5. Modelos de Lenguaje Nauronales\n",
    "\n",
    "Guillermo Segura G√≥mez\n",
    "\n",
    "## Ejercicio 1\n",
    "\n",
    "**1. Con base en la implementaci√≥n mostrada en las pr√°cticas del NLM, construya un modelo de lenguaje neuronal a nivel de car√°cter. Tom√© en cuenta secuencias de tama√±o 6 o m√°s para el modelo, es decir hasta 5 caracteres o m√°s en el contexto. Ponga al modelo a generar texto 3 veces, con un m√°ximo de 300 caracteres. Escriba 5 ejemplos de oraciones y m√≠dales el likelihood. Escriba un ejemplo de estructura morfol√≥gica (permutaciones con caracteres) similar al de estructura sint√°ctica del profesor con 5 o m√°s caracteres de su gusto (e.g., \"ando \"). Calcule la perplejidad del modelo sobre los datos val.**\n",
    "\n",
    "### Modelos de lenguaje neuronales\n",
    "\n",
    "Los Modelos de Lenguaje Neuronal (NLM) son fundamentales en el campo del Procesamiento del Lenguaje Natural para generar texto, completar frases, y m√°s (mucho m√°s). Mientras que los modelos tradicionales a nivel de palabra capturan la estructura y sem√°ntica del lenguaje a partir de las palabras, los modelos a nivel de car√°cter ofrecen una mirada m√°s profunda a la formaci√≥n de palabras y estructuras sint√°cticas. Al enfocarse en caracteres, estos modelos pueden generalizar mejor en idiomas con una rica morfolog√≠a o en casos donde la ortograf√≠a y la formaci√≥n de palabras juegan un papel crucial. \n",
    "\n",
    "Construiremos un NLM a nivel de car√°cter, lo utilizaremos para generar texto, evaluaremos ejemplos de oraciones mediante el c√°lculo de su log-verosimilitud, exploraremos estructuras morfol√≥gicas mediante permutaciones de caracteres, y mediremos la perplejidad del modelo en un conjunto de datos de validaci√≥n. \n",
    "\n",
    "Comenzamos siguiendo el procedimiento descrito en la pr√°ctica 5 de NLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General libraries\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "from typing import Tuple\n",
    "from argparse import Namespace\n",
    "from itertools import chain\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.util import ngrams\n",
    "from nltk import FreqDist\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#scikit-learn\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos los par√°metros generadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1111\n",
    "random.seed(seed) #python seed\n",
    "np.random.seed(seed) #numpy seed\n",
    "torch.manual_seed(seed) #torch seed\n",
    "torch.backends.cudnn.benchmark = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializamos los datos en un data frame de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_text = \"/Users/guillermo_sego/Desktop/Segundo Semestre/PLN/Data/MexData/mex20_train.txt\"\n",
    "path_text_val = \"/Users/guillermo_sego/Desktop/Segundo Semestre/PLN/Data/MexData/mex20_val.txt\"\n",
    "\n",
    "X_train = pd.read_csv(path_text, sep = '\\r\\n', engine = 'python', header = None).loc[:, 0].values.tolist()\n",
    "X_val = pd.read_csv(path_text_val, sep = '\\r\\n', engine = 'python', header = None).loc[:, 0].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la clase `CaracterData` est√° dise√±ada para procesar datos textuales para tareas de modelado de lenguaje o similares, utilizando un enfoque basado en caracteres. El prop√≥sito de esta clase incluye varias funciones importantes para el preprocesamiento de datos textuales y su preparaci√≥n para algoritmos de aprendizaje autom√°tico. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaracterData():\n",
    "    def __init__(self, N:int, vocab_max: int = None):\n",
    "        self.N = N  # Tama√±o de la secuencia de caracteres, incluyendo el car√°cter objetivo\n",
    "        self.vocab_max = vocab_max  # Limitar el tama√±o del vocabulario, si es necesario\n",
    "        self.UNK = '<unk>'  # Representaci√≥n de caracteres desconocidos\n",
    "        self.vocab = set()\n",
    "\n",
    "    def get_vocab(self, corpus: list) -> set:\n",
    "        # Construye un conjunto de caracteres √∫nicos a partir del corpus\n",
    "        unique_chars = set(chain(*corpus))\n",
    "        return unique_chars\n",
    "\n",
    "    def fit(self, corpus: list) -> None:\n",
    "        # Construye el vocabulario a partir del corpus\n",
    "        self.vocab = self.get_vocab(corpus)\n",
    "        \n",
    "        # Crea mapeos de caracteres a √≠ndices y viceversa\n",
    "        self.char2id = {char: idx for idx, char in enumerate(self.vocab)}\n",
    "        self.id2char = {idx: char for char, idx in self.char2id.items()}\n",
    "\n",
    "        # Incluir el token UNK en el vocabulario y los mapeos\n",
    "        self.char2id[self.UNK] = len(self.char2id)\n",
    "        self.id2char[len(self.id2char)] = self.UNK\n",
    "    \n",
    "    def get_vocab_size(self) -> int:\n",
    "        # Retorna el tama√±o del conjunto vocab\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def transform(self, corpus: list) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        X_ngrams, y = [], []\n",
    "\n",
    "        for doc in corpus:\n",
    "            doc_ngrams = list(ngrams(doc, self.N))\n",
    "            for ngram in doc_ngrams:\n",
    "                # Usa el √≠ndice para UNK si el car√°cter no est√° en char2id\n",
    "                X_ngrams.append([self.char2id.get(char, self.char2id[self.UNK]) for char in ngram[:-1]])\n",
    "                y.append(self.char2id.get(ngram[-1], self.char2id[self.UNK]))\n",
    "\n",
    "        return np.array(X_ngrams), np.array(y)\n",
    "\n",
    "    def get_ngram_doc(self, doc: str) -> list:\n",
    "        # Genera n-gramas del documento\n",
    "        return list(ngrams(doc, self.N))\n",
    "\n",
    "    def replace_unk(self, doc_tokens: list) -> list:\n",
    "        # Reemplaza caracteres desconocidos por un √≠ndice especial, si es necesario\n",
    "        return [token if token in self.char2id else '<unk>' for token in doc_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "args.N = 6  # Tama√±o de secuencia deseado\n",
    "\n",
    "# Creamos un objeto con la clase adaptada para caracteres\n",
    "caracter_data = CaracterData(args.N)\n",
    "\n",
    "# Ajustamos el modelo a nuestro conjunto de datos de entrenamiento para construir el vocabulario de caracteres\n",
    "# Nota: No es necensario pasar un tokenizador ya que estamos trabajando a nivel de car√°cter\n",
    "caracter_data.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora utilizamos los m√©todos de la clase que creamos. El m√©todo `transform` transforma los datos textuales proporcionados en un formato que es adecuado para el entrenamiento o la evaluaci√≥n en modelos de aprendizaje autom√°tico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_caracter_train, y_caracter_train = caracter_data.transform(X_train)\n",
    "X_caracter_val, y_caracter_val = caracter_data.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 425\n"
     ]
    }
   ],
   "source": [
    "print(f'Vocab size: {caracter_data.get_vocab_size()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora necesitamos pasar estos datos en tensores de PyTorch y, organizarlos en un DataLoader de PyTorch para facilitar el entrenamiento y la evaluaci√≥n de modelos de aprendizaje profundo. PyTorch utiliza tensores, que son una generalizaci√≥n de matrices y vectores, como su estructura de datos principal para realizar operaciones de aprendizaje autom√°tico, especialmente en redes neuronales.\n",
    "\n",
    "Para el entrenamiento, es √∫til organizar los datos en lotes (batches), y PyTorch ofrece una herramienta llamada **DataLoader** para esto. Un DataLoader puede cargar los datos en lotes de un tama√±o especificado y puede mezclar los datos para reducir el riesgo de sobreajuste. Para usar un DataLoader, primero debes organizar tus tensores en un Dataset, que es otra abstracci√≥n de PyTorch que facilita trabajar con conjuntos de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch size in args\n",
    "args.batch_size = 64\n",
    "# Num workers\n",
    "args.num_workers = 2\n",
    "\n",
    "# Convertimos los datos a tensores de pytorch\n",
    "train_dataset = TensorDataset(torch.tensor(X_caracter_train, dtype = torch.int64),\n",
    "                              torch.tensor(y_caracter_train, dtype = torch.int64))\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size = args.batch_size,\n",
    "                          num_workers=args.num_workers,\n",
    "                          shuffle = True)\n",
    "\n",
    "val_dataset = TensorDataset(torch.tensor(X_caracter_val, dtype = torch.int64),\n",
    "                            torch.tensor(y_caracter_val, dtype = torch.int64))\n",
    "\n",
    "val_loader = DataLoader(train_dataset,\n",
    "                          batch_size = args.batch_size,\n",
    "                          num_workers=args.num_workers,\n",
    "                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([64, 5])\n",
      "y shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(f'X shape: {batch[0].shape}')\n",
    "print(f'y shape: {batch[1].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par√°metros que definen aspectos estructurales y de regularizaci√≥n de un modelo de red neuronal, que influyen en c√≥mo el modelo aprender√° de los datos y generalizar√° a nuevos ejemplos no vistos. Utilizamos los par√°metros del modelo propuesto por Bengio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab size\n",
    "args.vocab_size = caracter_data.get_vocab_size()\n",
    "\n",
    "# Dimension of word embeddings\n",
    "args.d = 50\n",
    "\n",
    "# Dimension for hidden layer\n",
    "args.d_h = 100\n",
    "\n",
    "# Dropout\n",
    "args.dropout = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la clase `NeuralLM`, que representa un Modelo de Lenguaje Neuronal, utilizando PyTorch. Esta clase hereda de `nn.Module`, que es la clase base para todos los m√≥dulos de red neuronal en PyTorch, proporcionando funcionalidades √∫tiles como el seguimiento de par√°metros, la GPU/CPU transferencia, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, window_size, dropout):\n",
    "        super(NeuralLM, self).__init__()\n",
    "        \n",
    "        # Tama√±o de la ventana de entrada para el modelo, correspondiente al contexto de caracteres\n",
    "        self.window_size = window_size - 1  # Menos 1 porque el √∫ltimo car√°cter es el objetivo\n",
    "        \n",
    "        # Dimensi√≥n de los embeddings de caracteres\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Capa de embedding que convierte √≠ndices de caracteres en vectores densos\n",
    "        self.emb = nn.Embedding(vocab_size, self.embedding_dim)\n",
    "        \n",
    "        # Primera capa lineal que transforma la entrada de embeddings aplanada a una representaci√≥n intermedia\n",
    "        self.fc1 = nn.Linear(self.embedding_dim * self.window_size, hidden_dim)\n",
    "        \n",
    "        # Capa de Dropout para reducir el sobreajuste\n",
    "        self.drop1 = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Segunda capa lineal que transforma la salida de la capa oculta en logits para cada car√°cter en el vocabulario\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Transforma los √≠ndices de caracteres en embeddings\n",
    "        x = self.emb(x)\n",
    "        \n",
    "        # Aplana los embeddings en un vector √∫nico para cada muestra\n",
    "        # print(self.window_size * self.embedding_dim)\n",
    "        x = x.view(-1, self.window_size * self.embedding_dim)\n",
    "        \n",
    "        # Aplica la primera capa lineal y una funci√≥n de activaci√≥n ReLU\n",
    "        h = F.relu(self.fc1(x))\n",
    "        \n",
    "        # Aplica Dropout a la representaci√≥n de la capa oculta\n",
    "        h = self.drop1(h)\n",
    "        \n",
    "        # Genera los logits para cada car√°cter en el vocabulario\n",
    "        return self.fc2(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora necesitamos mas funciones para poder hacer la evaluaci√≥n del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta funci√≥n toma los logits (es decir, las salidas no normalizadas de un modelo) \n",
    "# y devuelve las predicciones de clase como √≠ndices.\n",
    "def get_preds(raw_logits):\n",
    "    # Calcula las probabilidades aplicando la funci√≥n softmax a los logits.\n",
    "    # La operaci√≥n detach() se usa para evitar que se calculen gradientes para estas operaciones,\n",
    "    # ya que solo se necesitan las probabilidades para hacer predicciones.\n",
    "    probs = F.softmax(raw_logits.detach(), dim=1)\n",
    "    \n",
    "    # Encuentra el √≠ndice de la mayor probabilidad en cada fila (es decir, para cada ejemplo en el lote),\n",
    "    # que corresponde a la clase predicha. Luego, convierte el tensor a un array de NumPy.\n",
    "    y_pred = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "\n",
    "# Eval√∫a el modelo en un conjunto de datos proporcionado y devuelve la precisi√≥n del modelo.\n",
    "def model_eval(data, model, gpu=False):\n",
    "    # Desactiva el c√°lculo de gradientes para acelerar las cosas y reducir el uso de memoria\n",
    "    # ya que no se necesita para la evaluaci√≥n.\n",
    "    with torch.no_grad():\n",
    "        preds, tgts = [], []  # Listas para almacenar predicciones y etiquetas verdaderas\n",
    "        \n",
    "        # Itera sobre los lotes de datos en el DataLoader\n",
    "        for window_words, labels in data:\n",
    "            # Si se utiliza GPU, mueve los datos al dispositivo adecuado\n",
    "            if gpu:\n",
    "                window_words = window_words.cuda()\n",
    "                \n",
    "            # Obtiene los logits del modelo para el lote actual\n",
    "            outputs = model(window_words)\n",
    "            \n",
    "            # Obtiene las predicciones de clase para el lote actual utilizando la funci√≥n get_preds\n",
    "            y_pred = get_preds(outputs)\n",
    "            \n",
    "            # Extrae las etiquetas verdaderas del lote actual y las convierte a un array de NumPy\n",
    "            tgt = labels.numpy()\n",
    "            \n",
    "            # Almacena las predicciones y las etiquetas verdaderas\n",
    "            tgts.append(tgt)\n",
    "            preds.append(y_pred)\n",
    "        \n",
    "        # Aplana las listas de listas para obtener una √∫nica lista de etiquetas y predicciones\n",
    "        tgts = [e for l in tgts for e in l]\n",
    "        preds = [e for l in preds for e in l]\n",
    "        \n",
    "        # Calcula y devuelve la precisi√≥n del modelo comparando las predicciones con las etiquetas verdaderas\n",
    "        return accuracy_score(tgts, preds)\n",
    "    \n",
    "\n",
    "# Guarda el estado actual del modelo y, si es el mejor modelo hasta el momento seg√∫n \n",
    "# alg√∫n criterio, guarda una copia separada.\n",
    "def save_checkpoint(state, is_best, checkpoint_path, filename=\"checkpoint.pt\"):\n",
    "    # Construye la ruta completa del archivo donde se guardar√° el estado del modelo\n",
    "    filename = os.path.join(checkpoint_path, filename)\n",
    "    \n",
    "    # Guarda el estado del modelo en la ruta especificada\n",
    "    torch.save(state, filename)\n",
    "    \n",
    "    # Si el modelo actual es el \"mejor\" seg√∫n alg√∫n criterio, guarda una copia separada\n",
    "    if is_best:\n",
    "        # Copia el archivo del checkpoint al archivo del \"mejor modelo\"\n",
    "        shutil.copyfile(filename, os.path.join(checkpoint_path, \"model_best.pt\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guillermo_sego/anaconda3/envs/PLN/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "# Model hyperparameters\n",
    "args. vocab_size = caracter_data.get_vocab_size()\n",
    "args.d = 100\n",
    "args.d_h = 200\n",
    "args.dropout = 0.1\n",
    "\n",
    "# Training hyperparameters\n",
    "args.lr = 2.3e-1\n",
    "args.num_epochs = 20\n",
    "args.patience = 20\n",
    "\n",
    "# Scheduler hyperparameters\n",
    "args.lr_patience = 10\n",
    "args.lr_factor = 0.5\n",
    "\n",
    "# Saving directoty\n",
    "args.savedir = 'model'\n",
    "os.makedirs(args.savedir, exist_ok=True)\n",
    "\n",
    "# Create model\n",
    "model = NeuralLM(\n",
    "    vocab_size=args.vocab_size,\n",
    "    embedding_dim=args.d,\n",
    "    hidden_dim=args.d_h,\n",
    "    window_size=args.N,\n",
    "    dropout=args.dropout\n",
    ")\n",
    "\n",
    "# Send to GPU\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "if args.use_gpu:\n",
    "    model.cuda()\n",
    "    \n",
    "# Loss, optimizer an scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr= args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, 'min',\n",
    "                patience=args.lr_patience,\n",
    "                verbose=True,\n",
    "                factor=args.lr_factor\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora realizamos el entrenamiento y validaci√≥n del modelo de aprendizaje profundo utilizando PyTorch, incluyendo caracter√≠sticas como el early stopping y el ajuste din√°mico de hiperpar√°metros (scheduler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.40929519959521143\n",
      "Epoch [1/20], Loss: 2.1188 - Val accuracy: 0.4563 - Epoch time: 1711170826.36\n",
      "Train acc: 0.45270056412023085\n",
      "Epoch [2/20], Loss: 1.9259 - Val accuracy: 0.4782 - Epoch time: 1711170844.05\n",
      "Train acc: 0.46749324455257946\n",
      "Epoch [3/20], Loss: 1.8610 - Val accuracy: 0.4906 - Epoch time: 1711170862.24\n",
      "Train acc: 0.4761725142106623\n",
      "Epoch [4/20], Loss: 1.8221 - Val accuracy: 0.4917 - Epoch time: 1711170880.25\n",
      "Train acc: 0.4824375861252261\n",
      "Epoch [5/20], Loss: 1.7942 - Val accuracy: 0.4979 - Epoch time: 1711170898.38\n",
      "Train acc: 0.487113378477306\n",
      "Epoch [6/20], Loss: 1.7733 - Val accuracy: 0.5036 - Epoch time: 1711170916.69\n",
      "Train acc: 0.49012574283007493\n",
      "Epoch [7/20], Loss: 1.7568 - Val accuracy: 0.5100 - Epoch time: 1711170934.65\n",
      "Train acc: 0.49334198174145205\n",
      "Epoch [8/20], Loss: 1.7444 - Val accuracy: 0.5134 - Epoch time: 1711170952.74\n",
      "Train acc: 0.495884425329429\n",
      "Epoch [9/20], Loss: 1.7321 - Val accuracy: 0.5144 - Epoch time: 1711170971.28\n",
      "Train acc: 0.49837640491775037\n",
      "Epoch [10/20], Loss: 1.7234 - Val accuracy: 0.5200 - Epoch time: 1711170992.45\n",
      "Train acc: 0.49999515545603307\n",
      "Epoch [11/20], Loss: 1.7137 - Val accuracy: 0.5222 - Epoch time: 1711171010.65\n",
      "Train acc: 0.5021372513134097\n",
      "Epoch [12/20], Loss: 1.7059 - Val accuracy: 0.5225 - Epoch time: 1711171029.02\n",
      "Train acc: 0.5152853436396521\n",
      "Epoch [13/20], Loss: 1.6543 - Val accuracy: 0.5352 - Epoch time: 1711171047.28\n",
      "Train acc: 0.5178798660752735\n",
      "Epoch [14/20], Loss: 1.6436 - Val accuracy: 0.5365 - Epoch time: 1711171065.70\n",
      "Train acc: 0.5183010722590646\n",
      "Epoch [15/20], Loss: 1.6405 - Val accuracy: 0.5379 - Epoch time: 1711171084.49\n",
      "Train acc: 0.518881879252433\n",
      "Epoch [16/20], Loss: 1.6370 - Val accuracy: 0.5387 - Epoch time: 1711171102.82\n",
      "Train acc: 0.5196218833433813\n",
      "Epoch [17/20], Loss: 1.6355 - Val accuracy: 0.5381 - Epoch time: 1711171120.85\n",
      "Train acc: 0.5204225788045819\n",
      "Epoch [18/20], Loss: 1.6321 - Val accuracy: 0.5407 - Epoch time: 1711171139.15\n",
      "Train acc: 0.5200440315218328\n",
      "Epoch [19/20], Loss: 1.6298 - Val accuracy: 0.5415 - Epoch time: 1711171157.53\n",
      "Train acc: 0.5211251722504522\n",
      "Epoch [20/20], Loss: 1.6272 - Val accuracy: 0.5409 - Epoch time: 1711171175.72\n",
      "--- 366.9564108848572 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "best_metric = 0\n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "for epoch in range(args.num_epochs):\n",
    "    epoch_stats_time = time.time()\n",
    "    loss_epoch = []\n",
    "    training_metric = []\n",
    "    model.train()\n",
    "    \n",
    "    for window_words, labels in train_loader:\n",
    "        if args.use_gpu:\n",
    "            window_words = window_words.cuda()\n",
    "            labels = labels.cuda()\n",
    "            \n",
    "        # Forward pass\n",
    "        outputs = model(window_words)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_epoch.append(loss.item())\n",
    "        \n",
    "        # Get training metrics\n",
    "        y_pred = get_preds(outputs)\n",
    "        tgt = labels.cpu().numpy()\n",
    "        training_metric.append(accuracy_score(tgt, y_pred))\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Get metric in training dataset\n",
    "    mean_epoch_metric = np.mean(training_metric)\n",
    "    train_metric_history.append(mean_epoch_metric)\n",
    "    \n",
    "    # Get metric in validation dataset\n",
    "    model.eval()\n",
    "    tuning_metric = model_eval(val_loader, model, gpu=args.use_gpu)\n",
    "    metric_history.append(mean_epoch_metric)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step(tuning_metric)\n",
    "    \n",
    "    # Check for metric improvement\n",
    "    is_improvement = tuning_metric > best_metric\n",
    "    if is_improvement:\n",
    "        best_metric = tuning_metric\n",
    "        n_no_improve = 0\n",
    "    else:\n",
    "        n_no_improve = 1\n",
    "        \n",
    "    save_checkpoint(\n",
    "        {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"scheduler\": scheduler.state_dict(),\n",
    "            \"best_metric\": best_metric,\n",
    "        },\n",
    "        is_improvement,\n",
    "        args.savedir,\n",
    "    )\n",
    "    \n",
    "    # Early stopping\n",
    "    if n_no_improve >= args.patience:\n",
    "        print(\"No improvement. Breaking out of loop.\")\n",
    "        \n",
    "    print(\"Train acc: {}\".format(mean_epoch_metric))\n",
    "    print(\"Epoch [{}/{}], Loss: {:.4f} - Val accuracy: {:.4f} - Epoch time: {:.2f}\"\n",
    "         .format(epoch+1, args.num_epochs, np.mean(loss_epoch), tuning_metric, (time.time())))\n",
    "    \n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluaci√≥n del modelo\n",
    "\n",
    "Utiizamos el modelo de lenguaje neuronal entrenado para generar texto de manera aut√≥noma. Las siguientes funciones son adaptaciones para trabajar con caracteres de las funciones revisadas en la pr√°ctica 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(text, char2id):\n",
    "    # Convierte el texto en caracteres y usa '<unk>' para caracteres desconocidos.\n",
    "    all_chars = [char if char in char2id else '<unk>' for char in text]\n",
    "    \n",
    "    # Convierte los caracteres a sus √≠ndices num√©ricos correspondientes seg√∫n el mapeo char2id.\n",
    "    char_ids = [char2id[char] for char in all_chars]\n",
    "    # print(len(char_ids))\n",
    "    return all_chars, char_ids\n",
    "\n",
    "def sample_next_word(logits, temperature=1.0):\n",
    "    # Convierte los logits a un array de numpy y ajusta la \"temperatura\" de la predicci√≥n.\n",
    "    logits = np.asarray(logits).astype(\"float64\")\n",
    "    preds = logits / temperature\n",
    "    \n",
    "    # Convierte los logits ajustados a probabilidades usando softmax.\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    \n",
    "    # Muestrea un √≠ndice de palabra de la distribuci√≥n de probabilidades.\n",
    "    probas = np.random.multinomial(1, preds)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def predict_next_token(model, token_ids):\n",
    "    # Convierte la lista de √≠ndices de tokens a un tensor de PyTorch y agrega una dimensi√≥n de lote.\n",
    "    word_ids_tensor = torch.LongTensor(token_ids).unsqueeze(0)\n",
    "    \n",
    "    # Obtiene los logits de la predicci√≥n del modelo para la secuencia de tokens y los convierte a numpy.\n",
    "    y_raw_pred = model(word_ids_tensor).squeeze(0).detach().numpy()\n",
    "    \n",
    "    # Muestra el √≠ndice de la siguiente palabra de la distribuci√≥n de logits.\n",
    "    y_pred = sample_next_word(y_raw_pred, 1.0)\n",
    "    return y_pred\n",
    "\n",
    "def generate_sentence(model, initial_text, char2id, nMax = 300):\n",
    "    # Obtiene tokens y sus √≠ndices del texto inicial.\n",
    "    all_tokens, window_word_ids = parse_text(initial_text, char2id)\n",
    "    \n",
    "    # Genera hasta 300 palabras adicionales.\n",
    "    for i in range(nMax):\n",
    "        # Predice el √≠ndice de la siguiente palabra utilizando el modelo.\n",
    "        y_pred = predict_next_token(model, window_word_ids)\n",
    "\n",
    "        next_word = caracter_data.id2char[y_pred]  # Convierte el √≠ndice de palabra predicho a texto.\n",
    "        all_tokens.append(next_word)  # A√±ade la palabra predicha a la lista de tokens.\n",
    "        \n",
    "        # Si se genera el token de fin de secuencia, detiene la generaci√≥n.\n",
    "        if next_word == \"</s>\":\n",
    "            break\n",
    "        else:\n",
    "            # Actualiza la ventana de palabras para la siguiente predicci√≥n.\n",
    "            window_word_ids.pop(0)  # Elimina la primera palabra.\n",
    "            window_word_ids.append(y_pred)  # A√±ade la nueva palabra al final.\n",
    "    \n",
    "    # Une los tokens generados en una cadena de texto y la devuelve.\n",
    "    return \"\".join(all_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos las cadenas de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralLM(\n",
       "  (emb): Embedding(425, 100)\n",
       "  (fc1): Linear(in_features=500, out_features=200, bias=True)\n",
       "  (drop1): Dropout(p=0.1, inplace=False)\n",
       "  (fc2): Linear(in_features=200, out_features=425, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model with learned embeddings from scratch\n",
    "best_model = NeuralLM(\n",
    "    vocab_size=args.vocab_size,\n",
    "    embedding_dim=args.d,\n",
    "    hidden_dim=args.d_h,\n",
    "    window_size=args.N,\n",
    "    dropout=args.dropout\n",
    ")\n",
    "\n",
    "best_model.load_state_dict(torch.load(\"model/model_best.pt\")[\"state_dict\"])\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto generado: Cincontrica que ‚Äúhale el #Bese enamente te plrapita no 10 por a la esas rece novia en un chuna pendeja para el mayar o albe... Grana chingo lo idgo la verga dequerose q adiendo prieron, se va a estaban cosasQueM√©Qu√© d√≠a que sean la el ninero camociando la pello, quieren en las cosas.üòë. A no S pendejos ne\n"
     ]
    }
   ],
   "source": [
    "initial_text = \"Cinco\"\n",
    "generated_text = generate_sentence(\n",
    "    model=best_model,\n",
    "    initial_text=initial_text,\n",
    "    char2id=caracter_data.char2id,\n",
    "    nMax=300\n",
    ")\n",
    "\n",
    "print(\"Texto generado:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto generado: Reino goconocible raja de gazados de demato. Y sus putas. .0 v√≠doma. (lt) y que tu caniendo recuperro como loca de que decen del de mierda, luchona noches nocia! Que hacen de Fax La creer. Bueno fuertro eran a noviantes draga!.!2 de ser con que tabas son Jaje Plaeta. Phone, sus vaba no lleves y soy de mi\n"
     ]
    }
   ],
   "source": [
    "initial_text = \"Reino\"\n",
    "generated_text = generate_sentence(\n",
    "    model=best_model,\n",
    "    initial_text=initial_text,\n",
    "    char2id=caracter_data.char2id,\n",
    "    nMax=300\n",
    ")\n",
    "\n",
    "print(\"Texto generado:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto generado: Habiablo soy hace me diady... el podia, cosasse #Hombre gienda y no hago la lestodichamenlasea. No mame, oereeci√≥n en lo puto en Edf alcumudiado el Rolfos y despuestas nadie no hacen a ventar en bien a REIGOARAWO @USUARIO @USUARIO , el oficiendo en la solo que me vale, descumplas? Ingue ereg√≥ ellas su ed\n"
     ]
    }
   ],
   "source": [
    "initial_text = \"Habia\"\n",
    "generated_text = generate_sentence(\n",
    "    model=best_model,\n",
    "    initial_text=initial_text,\n",
    "    char2id=caracter_data.char2id,\n",
    "    nMax=300\n",
    ")\n",
    "\n",
    "print(\"Texto generado:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funci√≥n `log_likelihood` calcula la log-verosimilitud de una secuencia de texto dada bajo un modelo de lenguaje entrenado. La log-verosimilitud es una medida de cu√°n probable es que el modelo haya generado una secuencia de texto dada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(model, text, caracter_model):\n",
    "    # Transforma el texto\n",
    "    X, y = caracter_model.transform([text])\n",
    "    \n",
    "    # Convierte X en un tensor de PyTorch y agrega una dimensi√≥n de lote, prepar√°ndolo para el modelo.\n",
    "    X = torch.LongTensor(X).unsqueeze(0)\n",
    "    \n",
    "    # Obtiene los logits del modelo para el texto transformado y luego los desconecta del grafo de c√≥mputo.\n",
    "    logits = model(X).detach()\n",
    "    \n",
    "    # Aplica softmax a los logits para obtener una distribuci√≥n de probabilidades sobre el vocabulario.\n",
    "    probs = F.softmax(logits, dim=1).numpy()\n",
    "    \n",
    "    # Calcula la log-verosimilitud sumando los logaritmos de las probabilidades de las palabras reales (y)\n",
    "    # seg√∫n lo predicho por el modelo.\n",
    "    return np.sum([np.log(probs[i][w]) for i, w in enumerate(y)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -69.76852\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model, \"Estamos en la clase de procesamiento de lenguaje\", caracter_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -51.269493\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model, \"Hab√≠a una vez en un lugar muy lejano\", caracter_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -34.32611\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model, \"Eres un tonto viejo loco\", caracter_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -44.871674\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model, \"Eres muy buena persona mi estimado\", caracter_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -34.247272\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model, \"Mexico es un pais inseguro\", caracter_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La log-verosimilitud es una medida de cu√°n probable es que una secuencia de texto dada sea generada por un modelo de lenguaje. Valores m√°s altos de log-verosimilitud indican que el modelo asigna una mayor probabilidad a la secuencia de texto observada, lo que sugiere que el modelo piensa que es m√°s \"veros√≠mil\" o probable.\n",
    "\n",
    "Cuando se calcula la log-verosimilitud, se suman los logaritmos de las probabilidades asignadas a las palabras reales. Dado que estas probabilidades est√°n entre 0 y 1, sus logaritmos son negativos. Por lo tanto, una log-verosimilitud m√°s alta (menos negativa) significa que las probabilidades asociadas a las palabras observadas son m√°s altas, y el modelo considera que la secuencia es m√°s probable. Una log-verosimilitud muy negativa indica que el modelo asigna muy bajas probabilidades a las palabras observadas, lo que sugiere que el modelo considera que esa secuencia de texto es poco probable.\n",
    "\n",
    "Debido al corpus con el que se construy√≥ el modelo, las frases mas \"agresivas\" fueron mas probables ya que tienen un valor de log-verosimilitud mas cercano a cero.\n",
    "\n",
    "### Estructura morfol√≥gica\n",
    "\n",
    "Evaluamos la permutaci√≥n de la palabra \"habia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "-13.483011 H i a a b\n",
      "-13.527386 H b i a a\n",
      "-13.591076 H b a a i\n",
      "-13.669549 H i a a b\n",
      "-14.227919 i a H a b\n",
      "--------------------------------------------------\n",
      "-29.35515 b a H i a\n",
      "-29.366308 a a i b H\n",
      "-29.491879 a a b i H\n",
      "-29.68933 a a i H b\n",
      "-30.166454 a a b i H\n"
     ]
    }
   ],
   "source": [
    "from itertools import permutations\n",
    "from random import shuffle\n",
    "\n",
    "word_list = \"Habia\"\n",
    "perms = [\" \".join(perm) for perm in permutations(word_list)]\n",
    "#print(len(perms))\n",
    "print(\"-\"*50)\n",
    "for p, t in sorted([(log_likelihood(best_model, text, caracter_data), text) for text in perms], reverse = True)[:5]:\n",
    "    print(p, t)\n",
    "print(\"-\"*50)  \n",
    "for p, t in sorted([(log_likelihood(best_model, text, caracter_data), text) for text in perms], reverse = True)[-5:]:\n",
    "    print(p, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2\n",
    "**2. Con base en la implementaci√≥n mostrada en clase, construya un modelo de lenguaje neuronal a nivel de palabra, pero preinicializado con los embeddings proporcionados. Tom√© en cuenta secuencias de tama√±o 4 para el modelo, es decir hasta 3 palabras en el contexto. Despu√©s de haber entrenado el modelo, recupere las 10 palabras m√°s similares a tres palabras de su gusto dadas. Ponga al modelo a generar texto a partir de tres secuencias de inicio de su gusto. Escriba 5 ejemplos de oraciones y m√≠dales el likelihood. Proponga un ejemplo para ver estructuras sint√°cticas (permutaciones de palabras de alguna oraci√≥n) buenas usando el likelihood a partir de una oraci√≥n que usted proponga. Calcule la perplejidad del modelo sobre los datos val. Comp√°relo con la perplejidad del modelo de lenguaje sin embeddings preentrenados (el visto en clase). DISCUTA BREVEMENTE.**\n",
    "\n",
    "Para realizar este ejercicio, necesitamos construir y entrenar un modelo de lenguaje neuronal, utilizando embeddings preentrenados que se nos proporciona para este ejercicio.\n",
    "\n",
    "Necesitamos cargar los embeddings y agregarlos en el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "args.N = 4\n",
    "\n",
    "class NgramData():\n",
    "    # Inicializaci√≥n de la clase NgramData\n",
    "    def __init__(self, N:int, vocab_max: int = 5000, tokenizer = None, embeddings_model = None):\n",
    "        # Asignaci√≥n de un tokenizador personalizado o el predeterminado si no se proporciona\n",
    "        self.tokenizer = tokenizer if tokenizer else self.default_tokenizer\n",
    "        # Definici√≥n de signos de puntuaci√≥n a ignorar\n",
    "        self.punct = set(['.', ',', ';', ':', '-', '_', '!', '¬°', '?', '¬ø', '^', '<url>', '*', '@usuario'])\n",
    "        # N√∫mero de palabras en cada n-grama\n",
    "        self.N = N\n",
    "        # Tama√±o m√°ximo del vocabulario\n",
    "        self.vocab_max = vocab_max\n",
    "        # Token para palabras desconocidas\n",
    "        self.UNK = '<unk>'\n",
    "        # Token para indicar el inicio de una secuencia\n",
    "        self.SOS = '<s>'\n",
    "        # Token para indicar el final de una secuencia\n",
    "        self.EOS = '</s>'\n",
    "        # Modelo de embeddings (opcional)\n",
    "        self.embeddings_model = embeddings_model\n",
    "\n",
    "    # Funci√≥n para obtener el tama√±o del vocabulario\n",
    "    def get_vocab_size(self) -> int:\n",
    "        return len(self.vocab)\n",
    "\n",
    "    # Tokenizador predeterminado que divide el texto por espacios\n",
    "    def default_tokenizer(self, doc: str) -> list:\n",
    "        return doc.split(\" \")\n",
    "    \n",
    "    # Funci√≥n para determinar si una palabra debe eliminarse (basada en puntuaci√≥n o si es un n√∫mero)\n",
    "    def remove_word(self, word: str) -> bool:\n",
    "        word = word.lower()\n",
    "        is_punct = word in self.punct\n",
    "        is_digit = word.isnumeric()\n",
    "        return is_punct or is_digit\n",
    "    \n",
    "    # Construye el vocabulario a partir de un corpus, excluyendo palabras seg√∫n `remove_word` y limitando el tama√±o\n",
    "    def get_vocab(self, corpus: list) -> set:\n",
    "        # Construcci√≥n de la distribuci√≥n de frecuencia de palabras\n",
    "        freq_dist = FreqDist([w.lower() for sentence in corpus for w in self.tokenizer(sentence) if not self.remove_word(w)])\n",
    "        # Ordenar palabras por frecuencia y limitar el tama√±o del vocabulario\n",
    "        sorted_words = self.sortFreqDict(freq_dist)[:self.vocab_max-3]\n",
    "        return set(sorted_words)\n",
    "    \n",
    "    # Ordena el diccionario de frecuencia de palabras\n",
    "    def sortFreqDict(self, freq_dist) -> list:\n",
    "        freq_dict = dict(freq_dist)\n",
    "        return sorted(freq_dict, key=freq_dict.get, reverse=True)\n",
    "    \n",
    "    # Ajusta el modelo al corpus, construyendo vocabulario, mapeos de palabras a ID y opcionalmente una matriz de embeddings\n",
    "    def fit(self, corpus: list) -> None:\n",
    "        self.vocab = self.get_vocab(corpus)\n",
    "        # Agregar tokens especiales al vocabulario\n",
    "        self.vocab.update({self.UNK, self.SOS, self.EOS})\n",
    "        \n",
    "        self.w2id = {}\n",
    "        self.id2w = {}\n",
    "        \n",
    "        # Opcional: inicializaci√≥n de la matriz de embeddings\n",
    "        if self.embeddings_model is not None:\n",
    "            self.embedding_matrix = np.empty([len(self.vocab), self.embeddings_model.vector_size])\n",
    "            \n",
    "        id = 0\n",
    "        for doc in corpus:\n",
    "            for word in self.tokenizer(doc):\n",
    "                word_ = word.lower()\n",
    "                if word_ in self.vocab and word_ not in self.w2id:\n",
    "                    self.w2id[word_] = id\n",
    "                    self.id2w[id] = word_\n",
    "                    # Si se proporciona un modelo de embeddings, asignar vector o vector aleatorio si la palabra no est√° en el modelo\n",
    "                    if self.embeddings_model is not None:\n",
    "                        self.embedding_matrix[id] = self.embeddings_model[word_] if word_ in self.embeddings_model else np.random.rand(self.embeddings_model.vector_size)\n",
    "                    id += 1\n",
    "        \n",
    "        # Actualizar mapeos con tokens especiales\n",
    "        self.w2id.update({self.UNK: id, self.SOS: id+1, self.EOS: id+2})\n",
    "        self.id2w.update({id: self.UNK, id+1: self.SOS, id+2: self.EOS})\n",
    "\n",
    "    \n",
    "    # Transforma el corpus en secuencias de entrada (X_ngrams) y etiquetas objetivo (y) para el modelado\n",
    "    def transform(self, corpus: list) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        X_ngrams = []  # Lista para almacenar secuencias de entrada\n",
    "        y = []  # Lista para almacenar las etiquetas objetivo\n",
    "\n",
    "        # Iterar sobre cada documento en el corpus\n",
    "        for doc in corpus:\n",
    "            # Obtener n-gramas para el documento actual\n",
    "            doc_ngram = self.get_ngram_doc(doc)\n",
    "            # Iterar sobre cada ventana de palabras (n-grama) en el documento\n",
    "            for words_window in doc_ngram:\n",
    "                # Convertir palabras en IDs usando el mapeo palabra-ID\n",
    "                words_window_ids = [self.w2id[w] for w in words_window]\n",
    "                # Las primeras N-1 palabras son la entrada, la √∫ltima palabra es la etiqueta objetivo\n",
    "                X_ngrams.append(list(words_window_ids[:-1]))\n",
    "                y.append(words_window_ids[-1])\n",
    "        # Convertir las listas en arrays de NumPy para su uso en modelos de aprendizaje autom√°tico\n",
    "        return np.array(X_ngrams), np.array(y)\n",
    "    \n",
    "    # Genera n-gramas a partir de un documento, prepar√°ndolo para la transformaci√≥n\n",
    "    def get_ngram_doc(self, doc: str) -> list:\n",
    "        # Tokenizar el documento\n",
    "        doc_tokens = self.tokenizer(doc)\n",
    "        # Reemplazar tokens desconocidos por el token <unk>\n",
    "        doc_tokens = self.replace_unk(doc_tokens)\n",
    "        # Convertir todos los tokens a min√∫sculas\n",
    "        doc_tokens = [w.lower() for w in doc_tokens]\n",
    "        # A√±adir tokens de inicio (<s>) y fin (</s>) al documento\n",
    "        doc_tokens = [self.SOS]*(self.N-1) + doc_tokens + [self.EOS]\n",
    "        # Generar y retornar n-gramas del documento procesado\n",
    "        return list(ngrams(doc_tokens, self.N))\n",
    "    \n",
    "    # Reemplaza tokens desconocidos en una lista de tokens por el token <unk>\n",
    "    def replace_unk(self, doc_tokens: list) -> list:\n",
    "        # Iterar sobre cada token en la lista de tokens\n",
    "        for i, token in enumerate(doc_tokens):\n",
    "            # Si el token no est√° en el vocabulario, reemplazarlo por <unk>\n",
    "            if token.lower() not in self.vocab:\n",
    "                doc_tokens[i] = self.UNK\n",
    "        # Retornar la lista de tokens con los desconocidos reemplazados\n",
    "        return doc_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesitamos ahora cargar los embeddings. Implementamos una funci√≥n para esto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(file_path):\n",
    "    embeddings_dict = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embeddings_dict[word] = vector\n",
    "    return embeddings_dict\n",
    "\n",
    "embeddings_dict = load_embeddings(\"word2vec_col.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora necesitamos construir el modelo, integramos el embedding en la clase que construye el modelo como objeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = TweetTokenizer() # Inicializamos el tokenizador de tweets\n",
    "\n",
    "# Creamos un objeto con la clase que definimos\n",
    "ngram_data = NgramData(args.N, 5000, tk.tokenize)\n",
    "ngram_data.fit(X_train) # Construye el vocabulario\n",
    "\n",
    "# Transformaci√≥n de datos\n",
    "X_ngram_train, y_ngram_train = ngram_data.transform(X_train)\n",
    "X_ngram_val, y_ngram_val = ngram_data.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([64, 3])\n",
      "y shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Pytorch \n",
    "\n",
    "args.batch_size = 64\n",
    "# Num workers\n",
    "args.num_workers = 2\n",
    "\n",
    "# Convertimos los datos a tensores de pytorch\n",
    "train_dataset = TensorDataset(torch.tensor(X_ngram_train, dtype = torch.int64),\n",
    "                              torch.tensor(y_ngram_train, dtype = torch.int64))\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size = args.batch_size,\n",
    "                          num_workers=args.num_workers,\n",
    "                          shuffle = True)\n",
    "\n",
    "val_dataset = TensorDataset(torch.tensor(X_ngram_val, dtype = torch.int64),\n",
    "                            torch.tensor(y_ngram_val, dtype = torch.int64))\n",
    "\n",
    "val_loader = DataLoader(train_dataset,\n",
    "                          batch_size = args.batch_size,\n",
    "                          num_workers=args.num_workers,\n",
    "                          shuffle = False)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "print(f'X shape: {batch[0].shape}')\n",
    "print(f'y shape: {batch[1].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab size\n",
    "args.vocab_size = ngram_data.get_vocab_size()\n",
    "\n",
    "# Dimension of word embeddings\n",
    "args.d = 100\n",
    "\n",
    "# Dimension for hidden layer\n",
    "args.d_h = 100\n",
    "\n",
    "# Dropout\n",
    "args.dropout = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparamos una matriz de embeddings que se pueda usar para inicializar la capa de embeddings en el modelo de PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = args.d\n",
    "vocab_size = args.vocab_size\n",
    "\n",
    "# Obteniendo el mapeo de palabra a ID desde tu instancia de NgramData\n",
    "word_to_id = ngram_data.w2id\n",
    "\n",
    "# Inicializa la matriz de embeddings con ceros\n",
    "embeddings_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "# Llena la matriz con los embeddings para cada palabra en el vocabulario\n",
    "for word, i in word_to_id.items():\n",
    "    embedding_vector = embeddings_dict.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embeddings_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLM(nn.Module):\n",
    "    def __init__(self, args, embeddings_matrix):\n",
    "        super(NeuralLM, self).__init__()\n",
    "        \n",
    "        # Tama√±o de la ventana de entrada (contexto) para el modelo, basado en N-1\n",
    "        # donde N es el tama√±o de los n-gramas considerados.\n",
    "        self.window_size = args.N-1\n",
    "        \n",
    "        # Dimensi√≥n de los embeddings de palabras, que transforman √≠ndices de palabras\n",
    "        # en vectores densos que capturan informaci√≥n sem√°ntica.\n",
    "        self.embedding_dim = args.d\n",
    "        \n",
    "        # Inicializa capa de embeddings\n",
    "        self.emb = nn.Embedding(args.vocab_size, self.embedding_dim)\n",
    "\n",
    "        # Convierte embeddings_matrix de NumPy array a PyTorch tensor\n",
    "        embeddings_tensor = torch.tensor(embeddings_matrix, dtype=torch.float)\n",
    "\n",
    "        self.emb.weight.data.copy_(embeddings_tensor)  # Establece los pesos de la capa de embedding\n",
    "        \n",
    "        # Otras capas\n",
    "        self.fc1 = nn.Linear(self.embedding_dim * self.window_size, args.d_h)\n",
    "        self.drop1 = nn.Dropout(p=args.dropout)\n",
    "        self.fc2 = nn.Linear(args.d_h, args.vocab_size, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Transforma los √≠ndices de palabras en x en embeddings de palabras.\n",
    "        x = self.emb(x)\n",
    "        \n",
    "        # Aplana los embeddings en un vector √∫nico para cada muestra en el lote,\n",
    "        # prepar√°ndolos para la entrada a la capa lineal.\n",
    "        x = x.view(-1, self.window_size * self.embedding_dim)\n",
    "        \n",
    "        # Pasa la entrada aplanada a trav√©s de la primera capa lineal y luego\n",
    "        # a trav√©s de una funci√≥n de activaci√≥n ReLU.\n",
    "        h = F.relu(self.fc1(x))\n",
    "        \n",
    "        # Aplica Dropout a la representaci√≥n de la capa oculta para mejorar la generalizaci√≥n.\n",
    "        h = self.drop1(h)\n",
    "        \n",
    "        # La salida final se obtiene pasando la representaci√≥n de la capa oculta despu√©s de Dropout\n",
    "        # a trav√©s de la segunda capa lineal, generando los logits para cada palabra en el vocabulario.\n",
    "        return self.fc2(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta funci√≥n toma los logits (es decir, las salidas no normalizadas de un modelo) \n",
    "# y devuelve las predicciones de clase como √≠ndices.\n",
    "def get_preds(raw_logits):\n",
    "    # Calcula las probabilidades aplicando la funci√≥n softmax a los logits.\n",
    "    # La operaci√≥n detach() se usa para evitar que se calculen gradientes para estas operaciones,\n",
    "    # ya que solo se necesitan las probabilidades para hacer predicciones.\n",
    "    probs = F.softmax(raw_logits.detach(), dim=1)\n",
    "    \n",
    "    # Encuentra el √≠ndice de la mayor probabilidad en cada fila (es decir, para cada ejemplo en el lote),\n",
    "    # que corresponde a la clase predicha. Luego, convierte el tensor a un array de NumPy.\n",
    "    y_pred = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "\n",
    "# Eval√∫a el modelo en un conjunto de datos proporcionado y devuelve la precisi√≥n del modelo.\n",
    "def model_eval(data, model, gpu=False):\n",
    "    # Desactiva el c√°lculo de gradientes para acelerar las cosas y reducir el uso de memoria\n",
    "    # ya que no se necesita para la evaluaci√≥n.\n",
    "    with torch.no_grad():\n",
    "        preds, tgts = [], []  # Listas para almacenar predicciones y etiquetas verdaderas\n",
    "        \n",
    "        # Itera sobre los lotes de datos en el DataLoader\n",
    "        for window_words, labels in data:\n",
    "            # Si se utiliza GPU, mueve los datos al dispositivo adecuado\n",
    "            if gpu:\n",
    "                window_words = window_words.cuda()\n",
    "                \n",
    "            # Obtiene los logits del modelo para el lote actual\n",
    "            outputs = model(window_words)\n",
    "            \n",
    "            # Obtiene las predicciones de clase para el lote actual utilizando la funci√≥n get_preds\n",
    "            y_pred = get_preds(outputs)\n",
    "            \n",
    "            # Extrae las etiquetas verdaderas del lote actual y las convierte a un array de NumPy\n",
    "            tgt = labels.numpy()\n",
    "            \n",
    "            # Almacena las predicciones y las etiquetas verdaderas\n",
    "            tgts.append(tgt)\n",
    "            preds.append(y_pred)\n",
    "        \n",
    "        # Aplana las listas de listas para obtener una √∫nica lista de etiquetas y predicciones\n",
    "        tgts = [e for l in tgts for e in l]\n",
    "        preds = [e for l in preds for e in l]\n",
    "        \n",
    "        # Calcula y devuelve la precisi√≥n del modelo comparando las predicciones con las etiquetas verdaderas\n",
    "        return accuracy_score(tgts, preds)\n",
    "    \n",
    "\n",
    "# Guarda el estado actual del modelo y, si es el mejor modelo hasta el momento seg√∫n \n",
    "# alg√∫n criterio, guarda una copia separada.\n",
    "def save_checkpoint(state, is_best, checkpoint_path, filename=\"checkpoint.pt\"):\n",
    "    # Construye la ruta completa del archivo donde se guardar√° el estado del modelo\n",
    "    filename = os.path.join(checkpoint_path, filename)\n",
    "    \n",
    "    # Guarda el estado del modelo en la ruta especificada\n",
    "    torch.save(state, filename)\n",
    "    \n",
    "    # Si el modelo actual es el \"mejor\" seg√∫n alg√∫n criterio, guarda una copia separada\n",
    "    if is_best:\n",
    "        # Copia el archivo del checkpoint al archivo del \"mejor modelo\"\n",
    "        shutil.copyfile(filename, os.path.join(checkpoint_path, \"model_best.pt\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definici√≥n de modelo e hiperpar√°metros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guillermo_sego/anaconda3/envs/PLN/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "# Model hyperparameters\n",
    "args.vocab_size = ngram_data.get_vocab_size() \n",
    "args.d = 100\n",
    "args.d_h = 200\n",
    "args.dropout = 0.1\n",
    "\n",
    "# Training hyperparameters\n",
    "args.lr = 2.3e-1\n",
    "args.num_epochs = 20\n",
    "args.patience = 20\n",
    "\n",
    "# Scheduler hyperparameters\n",
    "args.lr_patience = 10\n",
    "args.lr_factor = 0.5\n",
    "\n",
    "# Saving directoty\n",
    "args.savedir = 'model'\n",
    "os.makedirs(args.savedir, exist_ok=True)\n",
    "\n",
    "# Create model\n",
    "model = NeuralLM(args, embeddings_matrix)\n",
    "\n",
    "# Send to GPU\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "if args.use_gpu:\n",
    "    model.cuda()\n",
    "    \n",
    "# Loss, optimizer an scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr= args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, 'min',\n",
    "                patience=args.lr_patience,\n",
    "                verbose=True,\n",
    "                factor=args.lr_factor\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento de la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.1483213779978307\n",
      "Epoch [1/20], Loss: 5.7387 - Val accuracy: 0.1709 - Epoch time: 1711171210.20\n",
      "Train acc: 0.16737229090507375\n",
      "Epoch [2/20], Loss: 5.2152 - Val accuracy: 0.1934 - Epoch time: 1711171223.80\n",
      "Train acc: 0.1714123950508175\n",
      "Epoch [3/20], Loss: 5.0171 - Val accuracy: 0.1873 - Epoch time: 1711171241.22\n",
      "Train acc: 0.17458158418029165\n",
      "Epoch [4/20], Loss: 4.8582 - Val accuracy: 0.1812 - Epoch time: 1711171254.90\n",
      "Train acc: 0.17931904802554935\n",
      "Epoch [5/20], Loss: 4.7290 - Val accuracy: 0.1404 - Epoch time: 1711171268.79\n",
      "Train acc: 0.1808769659141124\n",
      "Epoch [6/20], Loss: 4.6089 - Val accuracy: 0.1797 - Epoch time: 1711171282.47\n",
      "Train acc: 0.18392030390069494\n",
      "Epoch [7/20], Loss: 4.5016 - Val accuracy: 0.2164 - Epoch time: 1711171296.15\n",
      "Train acc: 0.18688110111276263\n",
      "Epoch [8/20], Loss: 4.4083 - Val accuracy: 0.1850 - Epoch time: 1711171309.87\n",
      "Train acc: 0.19044981269834896\n",
      "Epoch [9/20], Loss: 4.3219 - Val accuracy: 0.2199 - Epoch time: 1711171323.55\n",
      "Train acc: 0.19461514030048607\n",
      "Epoch [10/20], Loss: 4.2464 - Val accuracy: 0.2172 - Epoch time: 1711171337.23\n",
      "Train acc: 0.1979704384766802\n",
      "Epoch [11/20], Loss: 4.1756 - Val accuracy: 0.2308 - Epoch time: 1711171350.94\n",
      "Train acc: 0.1998324704736271\n",
      "Epoch [12/20], Loss: 4.1071 - Val accuracy: 0.2307 - Epoch time: 1711171364.77\n",
      "Train acc: 0.20521142992005786\n",
      "Epoch [13/20], Loss: 4.0484 - Val accuracy: 0.2381 - Epoch time: 1711171378.47\n",
      "Train acc: 0.20847916582975132\n",
      "Epoch [14/20], Loss: 3.9902 - Val accuracy: 0.2411 - Epoch time: 1711171392.41\n",
      "Train acc: 0.2126859207407705\n",
      "Epoch [15/20], Loss: 3.9460 - Val accuracy: 0.2546 - Epoch time: 1711171406.11\n",
      "Train acc: 0.21757528471859558\n",
      "Epoch [16/20], Loss: 3.8998 - Val accuracy: 0.2634 - Epoch time: 1711171420.00\n",
      "Train acc: 0.26259452958663076\n",
      "Epoch [17/20], Loss: 3.4593 - Val accuracy: 0.3114 - Epoch time: 1711171434.39\n",
      "Train acc: 0.27113608745430445\n",
      "Epoch [18/20], Loss: 3.3718 - Val accuracy: 0.3223 - Epoch time: 1711171448.18\n",
      "Train acc: 0.27528070140200056\n",
      "Epoch [19/20], Loss: 3.3307 - Val accuracy: 0.3123 - Epoch time: 1711171461.76\n",
      "Train acc: 0.2780833216165187\n",
      "Epoch [20/20], Loss: 3.3139 - Val accuracy: 0.3341 - Epoch time: 1711171476.16\n",
      "--- 279.70687222480774 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "best_metric = 0\n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "for epoch in range(args.num_epochs):\n",
    "    epoch_stats_time = time.time()\n",
    "    loss_epoch = []\n",
    "    training_metric = []\n",
    "    model.train()\n",
    "    \n",
    "    for window_words, labels in train_loader:\n",
    "        if args.use_gpu:\n",
    "            window_words = window_words.cuda()\n",
    "            labels = labels.cuda()\n",
    "            \n",
    "        #Forward pass\n",
    "        outputs = model(window_words)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_epoch.append(loss.item())\n",
    "        \n",
    "        #Get training metrics\n",
    "        y_pred = get_preds(outputs)\n",
    "        tgt = labels.cpu().numpy()\n",
    "        training_metric.append(accuracy_score(tgt, y_pred))\n",
    "        \n",
    "        #Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    #Get metric in training dataset\n",
    "    mean_epoch_metric = np.mean(training_metric)\n",
    "    train_metric_history.append(mean_epoch_metric)\n",
    "    \n",
    "    #Get metric in validation dataset\n",
    "    model.eval()\n",
    "    tuning_metric = model_eval(val_loader, model, gpu=args.use_gpu)\n",
    "    metric_history.append(mean_epoch_metric)\n",
    "    \n",
    "    #update scheduler\n",
    "    scheduler.step(tuning_metric)\n",
    "    \n",
    "    #Check for metric improvement\n",
    "    is_improvement = tuning_metric > best_metric\n",
    "    if is_improvement:\n",
    "        best_metric = tuning_metric\n",
    "        n_no_improve = 0\n",
    "    else:\n",
    "        n_no_improve = 1\n",
    "        \n",
    "    save_checkpoint(\n",
    "        {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"scheduler\": scheduler.state_dict(),\n",
    "            \"best_metric\": best_metric,\n",
    "        },\n",
    "        is_improvement,\n",
    "        args.savedir,\n",
    "    )\n",
    "    \n",
    "    #Early stopping\n",
    "    if n_no_improve >= args.patience:\n",
    "        print(\"No improvement. Breaking out of loop.\")\n",
    "        \n",
    "    print(\"Train acc: {}\".format(mean_epoch_metric))\n",
    "    print(\"Epoch [{}/{}], Loss: {:.4f} - Val accuracy: {:.4f} - Epoch time: {:.2f}\"\n",
    "         .format(epoch+1, args.num_epochs, np.mean(loss_epoch), tuning_metric, (time.time())))\n",
    "    \n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truncamos a 20 debido al tiempo de entrega de la tarea :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluaci√≥n del modelo\n",
    "Ahora evaluamos y comparamos el modelo con el desarrollado en clase. Primero necesitamos revisar las palabras mas similares en el contexto dado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_closest_words(embeddings, ngram_data, word, n):\n",
    "    word_id = torch.LongTensor([ngram_data.w2id[word]])\n",
    "    word_embed = embeddings(word_id)\n",
    "    dists = torch.norm(embeddings.weight - word_embed, dim = 1).detach()\n",
    "    lst = sorted(enumerate(dists.numpy()), key = lambda x: x[1])\n",
    "    for idx, difference in lst[1:n+1]:\n",
    "        print(ngram_data.id2w[idx], difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el mejor modelo que se haya guardado y proseguimos para hacer una visualizaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "lindo 10.560313\n",
      "hermoso 15.729565\n",
      "feo 19.227892\n",
      "reconfortante 20.34947\n",
      "gracioso 21.105967\n",
      "perfecto 21.274488\n",
      "chistoso 21.696632\n",
      "chingon 21.848997\n",
      "ching√≥n 22.155758\n",
      "guapo 22.157343\n"
     ]
    }
   ],
   "source": [
    "# Model with learned embeddings from scratch\n",
    "best_model = NeuralLM(args, embeddings_matrix)\n",
    "best_model.load_state_dict(torch.load(\"model/model_best.pt\")[\"state_dict\"])\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"Learned embeddings\")\n",
    "print(\"-\"*30)\n",
    "print_closest_words(best_model.emb, ngram_data, \"bonito\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, generamos texto con el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(text, tokenizer):\n",
    "    # Tokeniza el texto y convierte cada palabra a min√∫sculas. Si la palabra est√° en el vocabulario (w2id), la usa;\n",
    "    # de lo contrario, la reemplaza por el token \"<unk>\" para palabras desconocidas.\n",
    "    all_tokens = [w.lower() if w.lower() in ngram_data.w2id else \"<unk>\" for w in tokenizer.tokenize(text)]\n",
    "    \n",
    "    # Convierte los tokens a sus √≠ndices num√©ricos correspondientes seg√∫n el mapeo w2id en ngram_data.\n",
    "    token_ids = [ngram_data.w2id[word.lower()] for word in all_tokens]\n",
    "    \n",
    "    return all_tokens, token_ids\n",
    "\n",
    "\n",
    "def sample_next_word(logits, temperature=1.0):\n",
    "    # Convierte los logits a un array de numpy y ajusta la \"temperatura\" de la predicci√≥n.\n",
    "    logits = np.asarray(logits).astype(\"float64\")\n",
    "    preds = logits / temperature\n",
    "    \n",
    "    # Convierte los logits ajustados a probabilidades usando softmax.\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    \n",
    "    # Muestrea un √≠ndice de palabra de la distribuci√≥n de probabilidades.\n",
    "    probas = np.random.multinomial(1, preds)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def predict_next_token(model, token_ids):\n",
    "    # Convierte la lista de √≠ndices de tokens a un tensor de PyTorch y agrega una dimensi√≥n de lote.\n",
    "    word_ids_tensor = torch.LongTensor(token_ids).unsqueeze(0)\n",
    "    \n",
    "    # Obtiene los logits de la predicci√≥n del modelo para la secuencia de tokens y los convierte a numpy.\n",
    "    y_raw_pred = model(word_ids_tensor).squeeze(0).detach().numpy()\n",
    "    \n",
    "    # Muestra el √≠ndice de la siguiente palabra de la distribuci√≥n de logits.\n",
    "    y_pred = sample_next_word(y_raw_pred, 1.0)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def generate_sentence(model, initial_text, tokenizer):\n",
    "    # Obtiene tokens y sus √≠ndices del texto inicial.\n",
    "    all_tokens, window_word_ids = parse_text(initial_text, tokenizer)\n",
    "    \n",
    "    # Genera hasta 100 palabras adicionales.\n",
    "    for i in range(100):\n",
    "        # Predice el √≠ndice de la siguiente palabra utilizando el modelo.\n",
    "        y_pred = predict_next_token(model, window_word_ids)\n",
    "        next_word = ngram_data.id2w[y_pred]  # Convierte el √≠ndice de palabra predicho a texto.\n",
    "        all_tokens.append(next_word)  # A√±ade la palabra predicha a la lista de tokens.\n",
    "        \n",
    "        # Si se genera el token de fin de secuencia, detiene la generaci√≥n.\n",
    "        if next_word == \"</s>\":\n",
    "            break\n",
    "        else:\n",
    "            # Actualiza la ventana de palabras para la siguiente predicci√≥n.\n",
    "            window_word_ids.pop(0)  # Elimina la primera palabra.\n",
    "            window_word_ids.append(y_pred)  # A√±ade la nueva palabra al final.\n",
    "    \n",
    "    # Une los tokens generados en una cadena de texto y la devuelve.\n",
    "    return \" \".join(all_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "<s> <s> <s> vea al puto pendejo <unk> mi sangre <unk> <unk> ‚Ä¶ </s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = \"<s><s><s>\"\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"Learned embeddings\")\n",
    "print(\"-\"*30)\n",
    "print(generate_sentence(best_model, initial_tokens, tk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, medimos el valor del likelihood del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(model, text, ngram_model):\n",
    "    # Transforma el texto dado en n-gramas (X) y las etiquetas objetivo (y) utilizando el modelo n-gram.\n",
    "    X, y = ngram_data.transform([text])\n",
    "    \n",
    "    # Ignora los primeros dos n-gramas. Esto podr√≠a ser espec√≠fico para c√≥mo se estructura el texto o los n-gramas.\n",
    "    X, y = X[2:], y[2:]\n",
    "    \n",
    "    # Convierte X en un tensor de PyTorch y agrega una dimensi√≥n de lote, prepar√°ndolo para el modelo.\n",
    "    X = torch.LongTensor(X).unsqueeze(0)\n",
    "    \n",
    "    # Obtiene los logits del modelo para el texto transformado y luego los desconecta del grafo de c√≥mputo.\n",
    "    logits = model(X).detach()\n",
    "    \n",
    "    # Aplica softmax a los logits para obtener una distribuci√≥n de probabilidades sobre el vocabulario.\n",
    "    probs = F.softmax(logits, dim=1).numpy()\n",
    "    \n",
    "    # Calcula la log-verosimilitud sumando los logaritmos de las probabilidades de las palabras reales (y)\n",
    "    # seg√∫n lo predicho por el modelo.\n",
    "    return np.sum([np.log(probs[i][w]) for i, w in enumerate(y)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -28.747715\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model, \"Estamos en la clase de procesamiento de lenguaje\", ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -24.890015\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model, \"Sergio Perez es el mejor piloto de f1\", ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -42.84663\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model, \"Mexico esta en decadencia por los narcos\", ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -40.24046\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model, \"Tonto, viejo malo, que te pasa\", ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -54.03869\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model, \"Horrible, todo esta muy mal, que horrible\", ngram_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estructura morfol√≥gica\n",
    "\n",
    "Evaluamos las permutaciones del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-22.08913 sino gano me voy a la chingada\n",
      "-24.907225 voy sino me gano a la chingada\n",
      "-25.02383 sino voy me gano a la chingada\n",
      "-25.140871 gano sino me voy a la chingada\n",
      "-25.212936 gano sino me voy la chingada a\n",
      "--------------------------------------------------\n",
      "-82.64427 a la voy gano chingada sino me\n",
      "-83.02557 a la gano voy sino chingada me\n",
      "-84.76806 a la gano voy chingada me sino\n",
      "-86.985146 a la voy gano chingada me sino\n",
      "-89.53535 la voy gano chingada a me sino\n"
     ]
    }
   ],
   "source": [
    "from itertools import permutations\n",
    "from random import shuffle\n",
    "\n",
    "word_list = \"sino gano me voy a la chingada\".split(\" \")\n",
    "perms = [\" \".join(perm) for perm in permutations(word_list)]\n",
    "#print(len(perms))\n",
    "print(\"-\"*50)\n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse = True)[:5]:\n",
    "    print(p, t)\n",
    "print(\"-\"*50)  \n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse = True)[-5:]:\n",
    "    print(p, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo comparamos con el modelo de la clase. Los valores de la perplejidad fueron mejores en el modelo con embeddings. La integraci√≥n de embeddings preentrenados en el modelo de lenguaje neuronal muestra una mejora significativa en la coherencia y naturalidad de las secuencias de palabras generadas, como evidencian las perplejidades reducidas tanto en oraciones bien estructuradas como en aquellas con estructuras sint√°cticas menos convencionales. La disminuci√≥n de la perplejidad con embeddings preentrenados sugiere una comprensi√≥n sem√°ntica m√°s rica y una capacidad mejorada para capturar contextos y relaciones entre palabras, lo que resulta en un modelo m√°s robusto y eficaz en la generaci√≥n de texto natural. Este contraste resalta el valor de los embeddings preentrenados en enriquecer modelos de lenguaje con conocimiento sem√°ntico previo, facilitando as√≠ una generaci√≥n de lenguaje m√°s coherente y fluida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 3\n",
    "\n",
    "**3. (OPCIONAL: 30pts extra en ESTA tarea, calificaci√≥n m√°xima para promediar con otras tareas: 130) A partir del modelo anterior haga un modelo de lenguaje que integre una conexi√≥n directa de la capa de embeddings hac√≠a la salida, justo como lo propon√≠a Bengio. Discuta sobre las diferencias en el proceso de entrenamiento y la perplejidad respecto al modelo anterior y el visto en clase.**\n",
    "\n",
    "Para poder agregar una capa extra al embedding hay que modidicar la clase **NeuralLM** para integrar la capa extra propuesta por el modelo de Bengio. En este modelo modificado, hay dos caminos desde la capa de embeddings hacia la salida:\n",
    "\n",
    "* Conexi√≥n Directa: Una conexi√≥n lineal directa desde los embeddings aplanados hasta la salida. Esto est√° representado por la capa self.direct_conn. Esta conexi√≥n intenta capturar las relaciones lineales entre las palabras y su contexto.\n",
    "\n",
    "* Ruta No Lineal: La ruta original que pasa a trav√©s de una capa oculta (y potencialmente m√°s capas) antes de llegar a la salida. Esto permite que el modelo capture relaciones no lineales complejas.\n",
    "\n",
    "La salida final del modelo es la suma de las salidas de ambos caminos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLM(nn.Module):\n",
    "    def __init__(self, args, embeddings_matrix):\n",
    "        super(NeuralLM, self).__init__()\n",
    "        \n",
    "        self.window_size = args.N - 1\n",
    "        self.embedding_dim = args.d\n",
    "        \n",
    "        # Capa de embedding\n",
    "        self.emb = nn.Embedding(args.vocab_size, self.embedding_dim)\n",
    "        \n",
    "        # Convierte la matriz de NumPy a un tensor de PyTorch y asigna como pesos preentrenados\n",
    "        embeddings_tensor = torch.tensor(embeddings_matrix, dtype=torch.float32)\n",
    "        self.emb.weight = nn.Parameter(embeddings_tensor, requires_grad=False)  \n",
    "        \n",
    "        # Capa lineal para la conexi√≥n directa desde los embeddings a la salida\n",
    "        self.direct_conn = nn.Linear(self.embedding_dim * self.window_size, args.vocab_size)\n",
    "        \n",
    "        # Capas para la ruta no lineal\n",
    "        self.fc1 = nn.Linear(self.embedding_dim * self.window_size, args.d_h)\n",
    "        self.drop1 = nn.Dropout(p=args.dropout)\n",
    "        self.fc2 = nn.Linear(args.d_h, args.vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Transforma los √≠ndices de palabras en x en embeddings de palabras\n",
    "        embeddings = self.emb(x)\n",
    "        \n",
    "        # Aplana los embeddings para pasarlos a trav√©s de las capas lineales\n",
    "        flattened = embeddings.view(-1, self.window_size * self.embedding_dim)\n",
    "        \n",
    "        # Conexi√≥n directa de embeddings a salida\n",
    "        direct_output = self.direct_conn(flattened)\n",
    "        \n",
    "        # Ruta no lineal\n",
    "        h = F.relu(self.fc1(flattened))\n",
    "        h = self.drop1(h)\n",
    "        non_linear_output = self.fc2(h)\n",
    "        \n",
    "        # Suma las salidas de las conexiones directa y no lineal\n",
    "        combined_output = direct_output + non_linear_output\n",
    "        \n",
    "        return combined_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definici√≥n e hiperpar√°metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guillermo_sego/anaconda3/envs/PLN/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "# Model hyperparameters\n",
    "args.vocab_size = ngram_data.get_vocab_size() \n",
    "args.d = 100\n",
    "args.d_h = 200\n",
    "args.dropout = 0.1\n",
    "\n",
    "# Training hyperparameters\n",
    "args.lr = 2.3e-1\n",
    "args.num_epochs = 20\n",
    "args.patience = 20\n",
    "\n",
    "# Scheduler hyperparameters\n",
    "args.lr_patience = 10\n",
    "args.lr_factor = 0.5\n",
    "\n",
    "# Saving directoty\n",
    "args.savedir = 'model'\n",
    "os.makedirs(args.savedir, exist_ok=True)\n",
    "\n",
    "# Create model\n",
    "model = NeuralLM(args, embeddings_matrix)\n",
    "\n",
    "# Send to GPU\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "if args.use_gpu:\n",
    "    model.cuda()\n",
    "    \n",
    "# Loss, optimizer an scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr= args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, 'min',\n",
    "                patience=args.lr_patience,\n",
    "                verbose=True,\n",
    "                factor=args.lr_factor\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.11172097828706866\n",
      "Epoch [1/20], Loss: 8.8886 - Val accuracy: 0.1493 - Epoch time: 1711171504.28\n",
      "Train acc: 0.12053024444622985\n",
      "Epoch [2/20], Loss: 7.8851 - Val accuracy: 0.1377 - Epoch time: 1711171524.59\n",
      "Train acc: 0.127338132205841\n",
      "Epoch [3/20], Loss: 7.6160 - Val accuracy: 0.1521 - Epoch time: 1711171547.32\n",
      "Train acc: 0.13876861718153696\n",
      "Epoch [4/20], Loss: 7.3455 - Val accuracy: 0.1675 - Epoch time: 1711171566.42\n",
      "Train acc: 0.15113623809906399\n",
      "Epoch [5/20], Loss: 7.1834 - Val accuracy: 0.1622 - Epoch time: 1711171589.28\n",
      "Train acc: 0.16253628027959668\n",
      "Epoch [6/20], Loss: 7.0548 - Val accuracy: 0.1890 - Epoch time: 1711171613.84\n",
      "Train acc: 0.1731149946772185\n",
      "Epoch [7/20], Loss: 6.9522 - Val accuracy: 0.1959 - Epoch time: 1711171637.29\n",
      "Train acc: 0.18045076678182623\n",
      "Epoch [8/20], Loss: 6.8860 - Val accuracy: 0.1979 - Epoch time: 1711171657.94\n",
      "Train acc: 0.18811670198449362\n",
      "Epoch [9/20], Loss: 6.8445 - Val accuracy: 0.1805 - Epoch time: 1711171679.59\n",
      "Train acc: 0.19489006698670305\n",
      "Epoch [10/20], Loss: 6.7720 - Val accuracy: 0.2239 - Epoch time: 1711171699.58\n",
      "Train acc: 0.19996554001124814\n",
      "Epoch [11/20], Loss: 6.7750 - Val accuracy: 0.2082 - Epoch time: 1711171720.00\n",
      "Train acc: 0.20621102066846103\n",
      "Epoch [12/20], Loss: 6.7051 - Val accuracy: 0.2347 - Epoch time: 1711171740.36\n",
      "Train acc: 0.21136087454304422\n",
      "Epoch [13/20], Loss: 6.6730 - Val accuracy: 0.2150 - Epoch time: 1711171760.05\n",
      "Train acc: 0.2820364911822601\n",
      "Epoch [14/20], Loss: 4.2028 - Val accuracy: 0.3274 - Epoch time: 1711171781.12\n",
      "Train acc: 0.2941351484353031\n",
      "Epoch [15/20], Loss: 3.9309 - Val accuracy: 0.3220 - Epoch time: 1711171804.22\n",
      "Train acc: 0.299402379685052\n",
      "Epoch [16/20], Loss: 3.8674 - Val accuracy: 0.3302 - Epoch time: 1711171827.04\n",
      "Train acc: 0.3017853914755152\n",
      "Epoch [17/20], Loss: 3.8353 - Val accuracy: 0.3427 - Epoch time: 1711171851.36\n",
      "Train acc: 0.3039349038886434\n",
      "Epoch [18/20], Loss: 3.8125 - Val accuracy: 0.3373 - Epoch time: 1711171871.70\n",
      "Train acc: 0.3063069311653879\n",
      "Epoch [19/20], Loss: 3.7894 - Val accuracy: 0.3341 - Epoch time: 1711171891.84\n",
      "Train acc: 0.3080685333226208\n",
      "Epoch [20/20], Loss: 3.7681 - Val accuracy: 0.3239 - Epoch time: 1711171911.97\n",
      "--- 426.13516330718994 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "best_metric = 0\n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "for epoch in range(args.num_epochs):\n",
    "    epoch_stats_time = time.time()\n",
    "    loss_epoch = []\n",
    "    training_metric = []\n",
    "    model.train()\n",
    "    \n",
    "    for window_words, labels in train_loader:\n",
    "        if args.use_gpu:\n",
    "            window_words = window_words.cuda()\n",
    "            labels = labels.cuda()\n",
    "            \n",
    "        #Forward pass\n",
    "        outputs = model(window_words)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_epoch.append(loss.item())\n",
    "        \n",
    "        #Get training metrics\n",
    "        y_pred = get_preds(outputs)\n",
    "        tgt = labels.cpu().numpy()\n",
    "        training_metric.append(accuracy_score(tgt, y_pred))\n",
    "        \n",
    "        #Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    #Get metric in training dataset\n",
    "    mean_epoch_metric = np.mean(training_metric)\n",
    "    train_metric_history.append(mean_epoch_metric)\n",
    "    \n",
    "    #Get metric in validation dataset\n",
    "    model.eval()\n",
    "    tuning_metric = model_eval(val_loader, model, gpu=args.use_gpu)\n",
    "    metric_history.append(mean_epoch_metric)\n",
    "    \n",
    "    #update scheduler\n",
    "    scheduler.step(tuning_metric)\n",
    "    \n",
    "    #Check for metric improvement\n",
    "    is_improvement = tuning_metric > best_metric\n",
    "    if is_improvement:\n",
    "        best_metric = tuning_metric\n",
    "        n_no_improve = 0\n",
    "    else:\n",
    "        n_no_improve = 1\n",
    "        \n",
    "    save_checkpoint(\n",
    "        {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"scheduler\": scheduler.state_dict(),\n",
    "            \"best_metric\": best_metric,\n",
    "        },\n",
    "        is_improvement,\n",
    "        args.savedir,\n",
    "    )\n",
    "    \n",
    "    #Early stopping\n",
    "    if n_no_improve >= args.patience:\n",
    "        print(\"No improvement. Breaking out of loop.\")\n",
    "        \n",
    "    print(\"Train acc: {}\".format(mean_epoch_metric))\n",
    "    print(\"Epoch [{}/{}], Loss: {:.4f} - Val accuracy: {:.4f} - Epoch time: {:.2f}\"\n",
    "         .format(epoch+1, args.num_epochs, np.mean(loss_epoch), tuning_metric, (time.time())))\n",
    "    \n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluaci√≥n del modelo\n",
    "Ahora comparamos el modelo con el realizado en el inciso anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "lindo 10.613454\n",
      "hermoso 15.747205\n",
      "feo 19.22297\n",
      "reconfortante 20.328833\n",
      "gracioso 21.1011\n",
      "perfecto 21.245314\n",
      "chistoso 21.689884\n",
      "chingon 21.850063\n",
      "ching√≥n 22.149546\n",
      "genial 22.179726\n"
     ]
    }
   ],
   "source": [
    "# Model with learned embeddings from scratch\n",
    "best_model = NeuralLM(args, embeddings_matrix)\n",
    "best_model.load_state_dict(torch.load(\"model/model_best.pt\")[\"state_dict\"])\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"Learned embeddings\")\n",
    "print(\"-\"*30)\n",
    "print_closest_words(best_model.emb, ngram_data, \"bonito\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "<s> <s> <s> me queda claro <unk> </s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = \"<s><s><s>\"\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"Learned embeddings\")\n",
    "print(\"-\"*30)\n",
    "print(generate_sentence(best_model, initial_tokens, tk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -34.184837\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model, \"Estamos en la clase de procesamiento de lenguaje\", ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -49.329678\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model, \"Sergio Perez es el mejor piloto de f1\", ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -44.581127\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model, \"Mexico esta en decadencia por los narcos\", ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -53.303963\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model, \"Tonto, viejo malo, que te pasa\", ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -44.472637\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model, \"Horrible, todo esta muy mal, que horrible\", ngram_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaci√≥n estructura morfol√≥gica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "-38.66899 sino gano a la chingada me voy\n",
      "-38.979294 chingada sino gano a la me voy\n",
      "-40.775017 gano sino la chingada me voy a\n",
      "-42.651367 gano chingada a la me voy sino\n",
      "-42.77483 chingada sino gano la me voy a\n",
      "--------------------------------------------------\n",
      "-133.49619 me la a sino chingada voy gano\n",
      "-136.30128 la me a chingada sino voy gano\n",
      "-136.89542 me a chingada sino gano voy la\n",
      "-141.6242 me la a sino voy gano chingada\n",
      "-142.57162 me la a sino gano voy chingada\n"
     ]
    }
   ],
   "source": [
    "word_list = \"sino gano me voy a la chingada\".split(\" \")\n",
    "perms = [\" \".join(perm) for perm in permutations(word_list)]\n",
    "#print(len(perms))\n",
    "print(\"-\"*50)\n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse = True)[:5]:\n",
    "    print(p, t)\n",
    "print(\"-\"*50)  \n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse = True)[-5:]:\n",
    "    print(p, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discusi√≥n sobre el entrenamiento y la perplejidad:\n",
    "\n",
    "La comparaci√≥n entre los modelos con y sin la conexi√≥n directa desde la capa de embeddings hacia la salida revela diferencias significativas en los valores de perplejidad, reflejando el impacto de la arquitectura del modelo en su capacidad para entender y generar secuencias de palabras coherentes.\n",
    "\n",
    "El modelo **con embeddings preentrenados**, pero **sin la conexi√≥n directa**, muestra perplejidades m√°s bajas tanto para oraciones coherentes como para aquellas con estructuras menos convencionales. Esto indica una mejora en la capacidad del modelo para generar secuencias de palabras naturales y coherentes, probablemente debido a la rica informaci√≥n sem√°ntica proporcionada por los embeddings.\n",
    "\n",
    "Por otro lado, el modelo **con la conexi√≥n directa** muestra un aumento en los valores de perplejidad para ambos tipos de oraciones. Aunque esta arquitectura, inspirada en Bengio, tiene el potencial de capturar tanto relaciones lineales como no lineales directamente desde los embeddings, el aumento en la perplejidad sugiere que podr√≠a estar enfrentando desaf√≠os, como el sobreajuste o la dificultad para integrar efectivamente esta informaci√≥n adicional en la generaci√≥n de secuencias de palabras.\n",
    "\n",
    "- La **disminuci√≥n de la perplejidad** en el modelo con embeddings, pero sin la conexi√≥n directa, subraya c√≥mo una comprensi√≥n sem√°ntica m√°s profunda facilitada por los embeddings puede mejorar la coherencia del texto generado.\n",
    "- El **aumento de la perplejidad** en el modelo con la conexi√≥n directa sugiere que la integraci√≥n de esta ruta adicional en el modelo no est√° siendo tan efectiva como se esperaba, lo que podr√≠a deberse a varios factores, como la complejidad adicional en la arquitectura o la necesidad de una regulaci√≥n m√°s fina para evitar el sobreajuste.\n",
    "- Estas observaciones subrayan la importancia de equilibrar la riqueza sem√°ntica proporcionada por los embeddings preentrenados con la complejidad arquitect√≥nica del modelo, asegurando que las adiciones a la arquitectura contribuyan efectivamente a la tarea de generaci√≥n de lenguaje sin introducir dificultades adicionales en el aprendizaje."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PLN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
