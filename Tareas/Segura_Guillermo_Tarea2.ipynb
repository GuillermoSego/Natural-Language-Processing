{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 2. Minería de texto básica\n",
    "\n",
    "Guillermo Segura Gómez\n",
    "\n",
    "## Bolsas de Palabras, Bigramas y Emociones\n",
    "\n",
    "Representa los documentos y clasifica con SVM similar a la Práctica 3, pero con diferentes\n",
    "pesados de términos.\n",
    "\n",
    "1. Evalué BoW con pesado binario.\n",
    "2. Evalué BoW con pesado frecuencia.\n",
    "3. Evalué BoW con pesado tfidf.\n",
    "4. Evalué BoW con pesado binario normalizado l2 (no use sklearn).\n",
    "5. Evalué BoW con pesado frecuencia normalizado l2 (no use sklearn).\n",
    "6. Evalué BoW con pesado tfidf normalizado l2 (no use sklearn).\n",
    "7. Ponga una tabla comparativa a modo de resumen con las seis entradas anteriores.\n",
    "8. De las configuraciones anteriores elija la mejor y evalúela con más y menos términos\n",
    "(e.g., 1000 y 7000). Ponga una tabla dónde compare las tres configuraciones.\n",
    "9. Utilice el recurso léxico del Consejo Nacional de Investigación de Canadá llamado\n",
    "\"EmoLex\" (https://www.saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm) para\n",
    "construir una \"Bolsa de Emociones\" de los Tweets de agresividad (Debe usar EmoLex\n",
    "en Español). Para esto, una estrategia sencilla sería enmascarar cada palabra con su\n",
    "emoción, y después construir la Bolsa de Emociones (BoE).\n",
    "10. Evalúa tú BoE clasificando con SVM. Ponga una tabla comparativa a modo de resumen\n",
    "con los tres pesados, normalize cada uno si lo cree conveniente.\n",
    "\n",
    "---\n",
    "\n",
    "Para las bolsas de palabras que vamos a crear, vamos a utilizar el corpus [MEX-A3T](https://sites.google.com/view/mex-a3t/home?authuser=0) que evalua la agresividad en un conjunto de tweets. Tenemos dos conjuntos de datos, los de entrenamiento y los de prueba o validación. Cada conjunto contiene dos archivos, uno es una serie de documentos donde cada documento es un tweet; el segundo archivo consiste en las etiquetas, donde 0 corresponde a si es agresivo el tweet y 1 si no lo es. \n",
    "\n",
    "La bolsa de palabras es un modelo que simplifica el contenido textual al considerar solo la ocurrencia de palabras, ignorando su orden y contexto. En este modelo, un texto se representa como un vector, donde cada dimensión (columna) corresponde a una palabra del vocabulario de todos los textos considerados (filas), el valor en cada dimensión depende del esquema de pesado que se elija. \n",
    "\n",
    "Para construir una bolsa de palabras de manera manual, lo que tenemos que hacer es primero, construir el vocabulario completo del corpus. Del vocabulario es posible descartar las palabras menos frecuentes para tener una matriz de dimensionalidad mas baja con la cual trabajar. Una vez construido el vocabulario, podemos comenzar con la implementación de la bolsa de palabras en una matriz, respetando que cada fila tiene un documento y cada columna corresponde a un término del vocabulario. Según el esquema de pesado se va llenando la matriz. Es bástante útil construir un diccionario con la siguiente estructura {'palabra':'valor'}, de esta manera sabemos exactamente el lugar o columna donde corresponde la palabra encontrada. Además es de fácil y rápido acceso ya que un diccionario en python es una tabla hash. \n",
    "\n",
    "De esta manera comenzamos importando los documentos. Para procesarlos, construir el diccionario y las diferentes funciones de bolsas de palabras. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que extrae el texto de dos archivos. Uno el de los documentos, otro el de las etiquetas\n",
    "def get_text_from_file(path_corpus, path_truth):\n",
    "\n",
    "    tr_text = []\n",
    "    tr_labels = []\n",
    "\n",
    "    with open(path_corpus, \"r\") as f_corpus, open(path_truth, \"r\") as f_truth:\n",
    "        for tweet in f_corpus:\n",
    "            tr_text += [tweet]\n",
    "        for label in f_truth:\n",
    "            tr_labels += [label]\n",
    "\n",
    "    return tr_text, tr_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_text = \"/Users/guillermo_sego/Desktop/Segundo Semestre/PLN/Data/MexData/mex20_train.txt\"\n",
    "path_labels = \"/Users/guillermo_sego/Desktop/Segundo Semestre/PLN/Data/MexData/mex20_train_labels.txt\"\n",
    "\n",
    "path_text_val = \"/Users/guillermo_sego/Desktop/Segundo Semestre/PLN/Data/MexData/mex20_val.txt\"\n",
    "path_labels_val = \"/Users/guillermo_sego/Desktop/Segundo Semestre/PLN/Data/MexData/mex20_val_labels.txt\"\n",
    "\n",
    "tr_text, tr_labels = get_text_from_file(path_text, path_labels) # Importamos los datos de entrenamiento\n",
    "val_text, val_labels = get_text_from_file(path_text_val, path_labels_val) # Importamos los datos de test o validación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construcción del vocabulario\n",
    "Ahora construimos el vocabulario de la bow. Necesitamos tokenizar el total del corpus y guardarlo en una lista para poder construir el vocabulario total. Utilizamos el método de **TweetTokenizer** de la clase *tokenize* de la librería nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tamaño del corpus es: 97473\n",
      "El tamaño del vocabulario es: 15194\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer() # Inicializar tokenizer\n",
    "\n",
    "corpus_palabras = []\n",
    "\n",
    "for doc in tr_text:\n",
    "    corpus_palabras += tokenizer.tokenize(doc)\n",
    "\n",
    "fdist = nltk.FreqDist(corpus_palabras)\n",
    "\n",
    "print(f\"El tamaño del corpus es:\", len(corpus_palabras))\n",
    "print(f\"El tamaño del vocabulario es:\", len(fdist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora necesitamos ordenar las frecuencias del vocabulario para poder trabajar con los tokens mas comunes. Tomamos los cincomil términos mas frecuentes del vocabulario, ya que 15194 es bastante para trabajar, además de que los términos menos frecuentes no tienen tanta contribución. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3016, ','), (2915, 'de'), (2829, 'que'), (2604, '.'), (2031, 'la')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Función que ordena un arreglo\n",
    "def  SortFrecuency(freqdist):\n",
    "    # List comprenhension\n",
    "    aux = [(freqdist[key], key) for key in freqdist]\n",
    "    aux.sort() # Ordena la lista\n",
    "    aux.reverse() # Cambiar el orden\n",
    "\n",
    "    return aux\n",
    "\n",
    "voc = SortFrecuency(fdist)\n",
    "voc = voc[:5000]\n",
    "voc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora necesitamos realizar un diccionario al que el valor de acceso sea la palabra. Esto con los fines explicados anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "992"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_indices = dict()\n",
    "count = 0\n",
    "\n",
    "for weight, word in voc:\n",
    "    dict_indices[word] = count\n",
    "    count += 1\n",
    "\n",
    "dict_indices[\"presidente\"] # Ejemplo de uso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Evalué BoW con pesado binario.\n",
    "\n",
    "La función que construye la bolsa de palabras con pesado binario únicamente, coloca 1 en la casilla donde se encontro la palabra. Se queda en 0 si no lo encuentra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # Manejo de vectores\n",
    "\n",
    "def build_bow_binary(tr_text, vocabulary, dict_indices):\n",
    "    # Construcción de matriz para la bolsa de palabras\n",
    "    # En cada fila vemos los documentos que estamos procesando\n",
    "    # En las columnas el tamaño del vocabulario que estamos creando\n",
    "    BOW = np.zeros((len(tr_text),len(vocabulary)), dtype = int)\n",
    "\n",
    "    for idx, tr in enumerate(tr_text):\n",
    "\n",
    "        # Cada documento tr lo tokenizamos\n",
    "        fdist_doc = nltk.FreqDist(tokenizer.tokenize(tr))\n",
    "\n",
    "        # Contamos cada palabra\n",
    "        for word in fdist_doc:\n",
    "            # Nos aseguramos que las palabras estan en el diccionario final\n",
    "            if word in dict_indices:\n",
    "                BOW[idx, dict_indices[word]] = 1 # Esquema de pesado binario\n",
    "\n",
    "    return BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOW_tr = build_bow_binary(tr_text, voc, dict_indices)\n",
    "BOW_tr[:10][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Evalué BoW con pesado frecuencia.\n",
    "\n",
    "Para cambiar el esquema de pesado, ahora contamos el número de palabras en cada documento. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bow_frecuency(tr_text, vocabulary, dict_indices):\n",
    "    # Construcción de matriz para la bolsa de palabras\n",
    "    BOW = np.zeros((len(tr_text), len(vocabulary)), dtype=int)\n",
    "\n",
    "    # Iteramos sobre cada documento en tr_text\n",
    "    for idx, tr in enumerate(tr_text):\n",
    "\n",
    "        # Cada documento tr lo tokenizamos\n",
    "        tokens = nltk.FreqDist(tokenizer.tokenize(tr))\n",
    "\n",
    "        # Calculamos la frecuencia de cada palabra en el documento\n",
    "        fdist_doc = nltk.FreqDist(tokens)\n",
    "\n",
    "        # Iteramos sobre cada palabra y su frecuencia en el documento\n",
    "        for word, freq in fdist_doc.items():\n",
    "            # Nos aseguramos que las palabras estan en el diccionario final\n",
    "            if word in dict_indices:\n",
    "                BOW[idx, dict_indices[word]] = freq  # Esquema de pesado de frecuencias\n",
    "\n",
    "    return BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOW_tr = build_bow_frecuency(tr_text, voc, dict_indices)\n",
    "BOW_tr[:10][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evalué BoW con pesado tfidf.\n",
    "\n",
    "**TF-IDF** es una técnica de pesado, en este caso para la bolsa de palabras, consiste en dos partes:\n",
    "\n",
    "- **TF (Term Frequency)**: Mide cuán frecuente es una palabra en un documento. Hay varias maneras de calcularlo, pero una forma común es simplemente contar el número de veces que la palabra aparece en el documento y dividirlo por el número total de palabras en el documento. Esto normaliza la frecuencia de la palabra.\n",
    "\n",
    "- **IDF (Inverse Document Frequency)**: Mide la importancia de la palabra en todo el conjunto de documentos. Se calcula tomando el logaritmo del número total de documentos dividido por el número de documentos que contienen la palabra. Esto da más peso a las palabras que son raras en todo el conjunto de documentos.\n",
    "\n",
    "El valor TF-IDF es simplemente el producto de TF e IDF. Este valor será alto para palabras que aparecen frecuentemente en un documento, pero no en muchos documentos, lo que significa que dichas palabras son potencialmente más relevantes para el documento.\n",
    "\n",
    "Para el esquema TFIDF necesitamos calcular primero el TF y posterior y el IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bow_tfidf(tr_text, vocabulary, dict_indices):\n",
    "    # Construcción de matriz para la bolsa de palabras\n",
    "    BOW = np.zeros((len(tr_text), len(vocabulary)), dtype=float) # Usamos float ya que tfidf contiene valores flotantes\n",
    "\n",
    "    # Calculamos el IDF para cada palabra en el vocabulario\n",
    "    # Necesitamos saber el número de documentos que contiene cada palabra. Realizamos esto con un diccionario\n",
    "    doc_count = {word: 0 for word in dict_indices} # Inicializar diccionario\n",
    "    for tr in tr_text:\n",
    "        tokens = set(tokenizer.tokenize(tr))  # Convertimos a set para obtener palabras únicas\n",
    "        for token in tokens:\n",
    "            if token in dict_indices:  # Si la palabra está en el vocabulario\n",
    "                doc_count[token] += 1\n",
    "\n",
    "    # Iteramos sobre cada documento en tr_text\n",
    "    for idx, tr in enumerate(tr_text):\n",
    "\n",
    "        # Cada documento tr lo tokenizamos\n",
    "        tokens = nltk.FreqDist(tokenizer.tokenize(tr))\n",
    "\n",
    "        # Calculamos la frecuencia de cada palabra en el documento\n",
    "        fdist_doc = nltk.FreqDist(tokens)\n",
    "\n",
    "        # Iteramos sobre cada palabra y su frecuencia en el documento\n",
    "        for word, freq in fdist_doc.items():\n",
    "            if word in dict_indices:\n",
    "                tf = freq / len(tokens)  # TF: Frecuencia de la palabra / total de palabras en el documento\n",
    "                idf = np.log(len(tr_text) / doc_count[word])  # IDF: log(Total de documentos / úmero de documentos que contienen la palabra)\n",
    "                BOW[idx, dict_indices[word]] = tf * idf\n",
    "\n",
    "    return BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.07149705, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.08579646, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.05046851, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.04289823, 0.04530089, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.06471556, ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOW_tr = build_bow_tfidf(tr_text, voc, dict_indices)\n",
    "BOW_tr[:10][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evalué BoW con pesado binario normalizado l2 (no use sklearn).\n",
    "\n",
    "La normalización L2, ajusta cada vector de características (en este caso, cada fila en la matriz BoW) para que su norma (o longitud) sea 1. Esto se hace dividiendo cada elemento en el vector por la raíz cuadrada de la suma de los cuadrados de todos los elementos en el vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bow_binary_norm(tr_text, vocabulary, dict_indices):\n",
    "    # Construcción de matriz para la bolsa de palabras\n",
    "    # En cada fila vemos los documentos que estamos procesando\n",
    "    # En las columnas el tamaño del vocabulario que estamos creando\n",
    "    BOW = np.zeros((len(tr_text),len(vocabulary)), dtype = float)\n",
    "\n",
    "    for idx, tr in enumerate(tr_text):\n",
    "\n",
    "        # Cada documento tr lo tokenizamos\n",
    "        fdist_doc = nltk.FreqDist(tokenizer.tokenize(tr))\n",
    "\n",
    "        # Contamos cada palabra\n",
    "        for word in fdist_doc:\n",
    "            # Nos aseguramos que las palabras estan en el diccionario final\n",
    "            if word in dict_indices:\n",
    "                BOW[idx, dict_indices[word]] = 1 # Esquema de pesado binario\n",
    "\n",
    "    # Normalización L2 de la matriz BoW\n",
    "    norm = np.linalg.norm(BOW, axis=1, keepdims=True)  # Calcular la norma L2 para cada fila (documento)\n",
    "    norm[norm == 0] = 1e-10  # Asegurar que no haya divisiones por cero\n",
    "    BOW = BOW / norm  # Dividir cada elemento por la norma de su fila\n",
    "\n",
    "    return BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.20412415, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.31622777, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.24253563, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.23570226, 0.23570226, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.30151134, ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOW_tr = build_bow_binary_norm(tr_text, voc, dict_indices)\n",
    "BOW_tr[:10][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evalué BoW con pesado frecuencia normalizado l2 (no use sklearn).\n",
    "\n",
    "Para normalizar, hacemos lo mismo que hicimos con el esquema binario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bow_frecuency_norm(tr_text, vocabulary, dict_indices):\n",
    "    # Construcción de matriz para la bolsa de palabras\n",
    "    BOW = np.zeros((len(tr_text), len(vocabulary)), dtype=float)\n",
    "\n",
    "    # Iteramos sobre cada documento en tr_text\n",
    "    for idx, tr in enumerate(tr_text):\n",
    "\n",
    "        # Cada documento tr lo tokenizamos\n",
    "        tokens = nltk.FreqDist(tokenizer.tokenize(tr))\n",
    "\n",
    "        # Calculamos la frecuencia de cada palabra en el documento\n",
    "        fdist_doc = nltk.FreqDist(tokens)\n",
    "\n",
    "        # Iteramos sobre cada palabra y su frecuencia en el documento\n",
    "        for word, freq in fdist_doc.items():\n",
    "            # Nos aseguramos que las palabras estan en el diccionario final\n",
    "            if word in dict_indices:\n",
    "                BOW[idx, dict_indices[word]] = freq  # Esquema de pesado de frecuencias\n",
    "\n",
    "    # Normalización L2 de la matriz BoW\n",
    "    norm = np.linalg.norm(BOW, axis=1, keepdims=True)  # Calcular la norma L2 para cada fila (documento)\n",
    "    norm[norm == 0] = 1e-10  # Asegurar que no haya divisiones por cero\n",
    "    BOW = BOW / norm  # Dividir cada elemento por la norma de su fila\n",
    "\n",
    "    return BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.32444284, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.31622777, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.2236068 , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.21821789, 0.21821789, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.18569534, ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOW_tr = build_bow_frecuency_norm(tr_text, voc, dict_indices)\n",
    "BOW_tr[:10][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evalué BoW con pesado tfidf normalizado l2 (no use sklearn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bow_tfidf_norm(tr_text, vocabulary, dict_indices):\n",
    "    # Construcción de matriz para la bolsa de palabras\n",
    "    BOW = np.zeros((len(tr_text), len(vocabulary)), dtype=float) # Usamos float ya que tfidf contiene valores flotantes\n",
    "\n",
    "    # Calculamos el IDF para cada palabra en el vocabulario\n",
    "    # Necesitamos saber el número de documentos que contiene cada palabra. Realizamos esto con un diccionario\n",
    "    doc_count = {word: 0 for word in dict_indices} # Inicializar diccionario\n",
    "    for tr in tr_text:\n",
    "        tokens = set(tokenizer.tokenize(tr))  # Convertimos a set para obtener palabras únicas\n",
    "        for token in tokens:\n",
    "            if token in dict_indices:  # Si la palabra está en el vocabulario\n",
    "                doc_count[token] += 1\n",
    "\n",
    "    # Iteramos sobre cada documento en tr_text\n",
    "    for idx, tr in enumerate(tr_text):\n",
    "\n",
    "        # Cada documento tr lo tokenizamos\n",
    "        tokens = nltk.FreqDist(tokenizer.tokenize(tr))\n",
    "\n",
    "        # Calculamos la frecuencia de cada palabra en el documento\n",
    "        fdist_doc = nltk.FreqDist(tokens)\n",
    "\n",
    "        # Iteramos sobre cada palabra y su frecuencia en el documento\n",
    "        for word, freq in fdist_doc.items():\n",
    "            if word in dict_indices:\n",
    "                tf = freq / len(tokens)  # TF: Frecuencia de la palabra / total de palabras en el documento\n",
    "                idf = np.log(len(tr_text) / doc_count[word])  # IDF: log(Total de documentos / úmero de documentos que contienen la palabra)\n",
    "                BOW[idx, dict_indices[word]] = tf * idf\n",
    "    \n",
    "    # Normalización L2 de la matriz BoW\n",
    "    norm = np.linalg.norm(BOW, axis=1, keepdims=True)  # Calcular la norma L2 para cada fila (documento)\n",
    "    norm[norm == 0] = 1e-10  # Asegurar que no haya divisiones por cero\n",
    "    BOW = BOW / norm  # Dividir cada elemento por la norma de su fila\n",
    "\n",
    "    return BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.06525231, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.06492909, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.04781418, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.05435931, 0.05740389, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.06506392, ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOW_tr = build_bow_tfidf_norm(tr_text, voc, dict_indices)\n",
    "BOW_tr[:10][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Ponga una tabla comparativa a modo de resumen con las seis entradas anteriores.\n",
    "\n",
    "Con las funciones construidas, ahora necesitamos clasificar las matrices. Para esto podemos construir una función que realice la clasificación y llamarla las veces que sea necesario. Para implementar la clasificación haremos uso de la libería `Scikit-learn` (importado generalmente como `sklearn`) la cual es una biblioteca de Python muy popular para machine learning. Ofrece una amplia gama de algoritmos tanto para aprendizaje supervisado (como clasificación y regresión) como no supervisado (como agrupamiento y reducción de dimensionalidad), junto con herramientas para la selección de modelos, preprocesamiento de datos, evaluación de modelos y muchas otras utilidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_recall_fscore_support, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suprimir advertencias\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Función para clasificar una bolsa de palabras\n",
    "def clasificar_bow(BOW_tr, tr_labels, BOW_val, val_labels):\n",
    "\n",
    "    parameters = {'C': [0.05, .12, .25, .5, 1, 2, 4]}\n",
    "    \n",
    "    # Máquina de soporte vectorial con balance de clases\n",
    "    srv = svm.LinearSVC(class_weight='balanced', dual=False, max_iter=10000)\n",
    "    grid = GridSearchCV(estimator=srv, param_grid=parameters, n_jobs=8, scoring=\"f1_macro\", cv=5)\n",
    "\n",
    "    # Entrenamiento y búsqueda de hiperparámetros\n",
    "    grid.fit(BOW_tr, tr_labels)\n",
    "\n",
    "    # Predicciones sobre el conjunto de validación\n",
    "    y_pred = grid.predict(BOW_val)\n",
    "\n",
    "    # Cálculo de métricas\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(val_labels, y_pred, average=\"macro\")\n",
    "\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1_score:.4f}\")\n",
    "\n",
    "    return precision, recall, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando: Binario\n",
      "Precision: 0.7692\n",
      "Recall: 0.7809\n",
      "F1-Score: 0.7744\n",
      "Procesando: Frecuencia\n",
      "Precision: 0.7892\n",
      "Recall: 0.7999\n",
      "F1-Score: 0.7941\n",
      "Procesando: TF-IDF\n",
      "Precision: 0.7932\n",
      "Recall: 0.8040\n",
      "F1-Score: 0.7982\n",
      "Procesando: Binario Normalizado\n",
      "Precision: 0.7732\n",
      "Recall: 0.7868\n",
      "F1-Score: 0.7792\n",
      "Procesando: Frecuencia Normalizada\n",
      "Precision: 0.7950\n",
      "Recall: 0.8158\n",
      "F1-Score: 0.8037\n",
      "Procesando: TF-IDF Normalizado\n",
      "Precision: 0.7818\n",
      "Recall: 0.8057\n",
      "F1-Score: 0.7913\n",
      "                        Precision    Recall  F1-Score\n",
      "Binario                  0.769177  0.780867  0.774444\n",
      "Frecuencia               0.789162  0.799878  0.794089\n",
      "TF-IDF                   0.793168  0.804033  0.798166\n",
      "Binario Normalizado      0.773209  0.786784  0.779227\n",
      "Frecuencia Normalizada   0.794952  0.815804  0.803692\n",
      "TF-IDF Normalizado       0.781761  0.805668  0.791269\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Diccionario para almacenar los resultados\n",
    "resultados = {}\n",
    "\n",
    "# Lista de funciones de BoW\n",
    "funciones_bow = [\n",
    "    build_bow_binary,\n",
    "    build_bow_frecuency,\n",
    "    build_bow_tfidf,\n",
    "    build_bow_binary_norm,\n",
    "    build_bow_frecuency_norm,\n",
    "    build_bow_tfidf_norm\n",
    "]\n",
    "\n",
    "# Nombres para cada función de BoW\n",
    "nombres_bow = [\n",
    "    \"Binario\",\n",
    "    \"Frecuencia\",\n",
    "    \"TF-IDF\",\n",
    "    \"Binario Normalizado\",\n",
    "    \"Frecuencia Normalizada\",\n",
    "    \"TF-IDF Normalizado\"\n",
    "]\n",
    "\n",
    "for func, nombre in zip(funciones_bow, nombres_bow):\n",
    "    print(f\"Procesando: {nombre}\")\n",
    "    # Construir la bolsa de palabras\n",
    "    BOW_tr = func(tr_text, voc, dict_indices)\n",
    "    BOW_val = func(val_text, voc, dict_indices)\n",
    "\n",
    "    # Clasificar y obtener métricas\n",
    "    precision, recall, f1 = clasificar_bow(BOW_tr, tr_labels, BOW_val, val_labels)\n",
    "\n",
    "    # Guardar resultados\n",
    "    resultados[nombre] = {\"Precision\": precision, \"Recall\": recall, \"F1-Score\": f1}\n",
    "\n",
    "# Convertir los resultados en un DataFrame de pandas para una visualización bonita\n",
    "resultados_df = pd.DataFrame(resultados).T  # .T para transponer el DataFrame\n",
    "print(resultados_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. De las configuraciones anteriores elija la mejor y evalúela con más y menos términos\n",
    "(e.g., 1000 y 7000). Ponga una tabla dónde compare las tres configuraciones.\n",
    "\n",
    "---\n",
    "\n",
    "Para implementar esto necesitamos construir vocabularios de distinta longitud. Las tres funciones que tuvieron mejor precisión fueron: TFIDF, Frecuencia normalizada, y frecuencia. Sin embargo vamos a probar el TFIDF, Frecuencia normalizada y TFIDF Normalizada, debido al al to valor del recall. \n",
    "\n",
    "Ya tenemos construido y tokenizado el corpus completo en la lista **fdist**, simplemente construimos nuevos vocabularios y llamamos a las funciones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que construye un vocabulario y su diccionario de indices de diferente longitud en base a un corpus\n",
    "def BuildVocabulary(fdist, Length):\n",
    "    vocName = SortFrecuency(fdist)\n",
    "    vocName = vocName[:Length]\n",
    "\n",
    "    dict_indices = dict()\n",
    "    count = 0\n",
    "\n",
    "    for weight, word in vocName:\n",
    "        dict_indices[word] = count\n",
    "        count += 1\n",
    "\n",
    "    return vocName, dict_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construimos vocabularios y diccionarios de diferente longitud\n",
    "voc10mil, dict_indices10mil = BuildVocabulary(fdist, 10000)\n",
    "voc7mil, dict_indices7mil = BuildVocabulary(fdist, 7000)\n",
    "voc2mil, dict_indices2mil = BuildVocabulary(fdist, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora llamamos a la función que clasifica las bolsas de palabras e imprimimos los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista con los vocabularios\n",
    "vocs = [voc2mil, voc7mil, voc10mil]\n",
    "\n",
    "# Lista con los diccionarios de índices\n",
    "diccionarios_indices = [dict_indices2mil, dict_indices7mil, dict_indices10mil]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando: TF-IDF - 2000 palabras\n",
      "Precision: 0.7818\n",
      "Recall: 0.8057\n",
      "F1-Score: 0.7913\n",
      "Procesando: Frecuencia Normalizada - 2000 palabras\n",
      "Precision: 0.7840\n",
      "Recall: 0.8104\n",
      "F1-Score: 0.7942\n",
      "Procesando: TF-IDF Normalizado - 2000 palabras\n",
      "Precision: 0.7802\n",
      "Recall: 0.8062\n",
      "F1-Score: 0.7902\n",
      "Procesando: TF-IDF - 7000 palabras\n",
      "Precision: 0.7932\n",
      "Recall: 0.8040\n",
      "F1-Score: 0.7982\n",
      "Procesando: Frecuencia Normalizada - 7000 palabras\n",
      "Precision: 0.7949\n",
      "Recall: 0.8140\n",
      "F1-Score: 0.8031\n",
      "Procesando: TF-IDF Normalizado - 7000 palabras\n",
      "Precision: 0.7910\n",
      "Recall: 0.8081\n",
      "F1-Score: 0.7984\n",
      "Procesando: TF-IDF - 10000 palabras\n",
      "Precision: 0.7956\n",
      "Recall: 0.8017\n",
      "F1-Score: 0.7985\n",
      "Procesando: Frecuencia Normalizada - 10000 palabras\n",
      "Precision: 0.7950\n",
      "Recall: 0.8158\n",
      "F1-Score: 0.8037\n",
      "Procesando: TF-IDF Normalizado - 10000 palabras\n",
      "Precision: 0.7816\n",
      "Recall: 0.8039\n",
      "F1-Score: 0.7906\n",
      "                                         Precision    Recall  F1-Score\n",
      "TF-IDF - 2000 palabras                    0.781761  0.805668  0.791269\n",
      "Frecuencia Normalizada - 2000 palabras    0.783978  0.810389  0.794180\n",
      "TF-IDF Normalizado - 2000 palabras        0.780177  0.806234  0.790221\n",
      "TF-IDF - 7000 palabras                    0.793168  0.804033  0.798166\n",
      "Frecuencia Normalizada - 7000 palabras    0.794920  0.814041  0.803079\n",
      "TF-IDF Normalizado - 7000 palabras        0.790996  0.808124  0.798426\n",
      "TF-IDF - 10000 palabras                   0.795603  0.801704  0.798522\n",
      "Frecuencia Normalizada - 10000 palabras   0.794952  0.815804  0.803692\n",
      "TF-IDF Normalizado - 10000 palabras       0.781584  0.803906  0.790637\n"
     ]
    }
   ],
   "source": [
    "# Lista de funciones de BoW\n",
    "funciones_bow = [\n",
    "    build_bow_tfidf,\n",
    "    build_bow_frecuency_norm,\n",
    "    build_bow_tfidf_norm\n",
    "]\n",
    "\n",
    "# Nombres para cada función de BoW\n",
    "nombres_bow = [\n",
    "    \"TF-IDF\",\n",
    "    \"Frecuencia Normalizada\",\n",
    "    \"TF-IDF Normalizado\"\n",
    "]\n",
    "\n",
    "# Diccionario para almacenar los resultados\n",
    "resultados = {}\n",
    "\n",
    "for voc, dict_indices in zip(vocs, diccionarios_indices):  # Usar zip para iterar en paralelo\n",
    "    voc_size = len(voc)  # Obtener el tamaño del vocabulario actual\n",
    "    \n",
    "    for func, nombre in zip(funciones_bow, nombres_bow):\n",
    "        bow_name = f\"{nombre} - {voc_size} palabras\"  # Crear un nombre único que incluya el tamaño del vocabulario\n",
    "        print(f\"Procesando: {bow_name}\")\n",
    "        \n",
    "        # Construir la bolsa de palabras\n",
    "        BOW_tr = func(tr_text, voc, dict_indices)\n",
    "        BOW_val = func(val_text, voc, dict_indices)\n",
    "\n",
    "        # Clasificar y obtener métricas\n",
    "        precision, recall, f1 = clasificar_bow(BOW_tr, tr_labels, BOW_val, val_labels)\n",
    "\n",
    "        # Guardar resultados, incluyendo el tamaño del vocabulario en la clave\n",
    "        resultados[bow_name] = {\"Precision\": precision, \"Recall\": recall, \"F1-Score\": f1}\n",
    "    \n",
    "\n",
    "# Convertir los resultados en un DataFrame de pandas para una visualización bonita\n",
    "resultados_df = pd.DataFrame(resultados).T  # .T para transponer el DataFrame\n",
    "print(resultados_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Utilice el recurso léxico del Consejo Nacional de Investigación de Canadá \"EmoLex\" \n",
    "con los tres pesados, normalize cada uno si lo cree conveniente.\n",
    "Para construir una \"Bolsa de Emociones\" de los Tweets de agresividad (Debe usar EmoLex\n",
    "en Español). Para esto, una estrategia sencilla sería enmascarar cada palabra con su\n",
    "emoción, y después construir la Bolsa de Emociones (BoE).\n",
    "\n",
    "### 10. Evalúa tú BoE clasificando con SVM. \n",
    "Ponga una tabla comparativa a modo de resumen\n",
    "\n",
    "---\n",
    "\n",
    "La Bolsa de Emociones es similar a la Bolsa de Palabras (BoW) en el procesamiento de lenguaje natural, pero en lugar de contar la frecuencia de cada palabra en un documento, cuenta la presencia o frecuencia de emociones asociadas a las palabras del documento. En el contexto de los tweets, cada tweet se transforma en un vector donde cada dimensión representa una emoción específica (como alegría, tristeza, ira, etc.), y el valor en esa dimensión puede ser binario (indicando la presencia/ausencia de esa emoción en el tweet) o una frecuencia (indicando cuántas veces se mencionan palabras asociadas a esa emoción en el tweet). Lo primero que tenemos que hacer es descargar la bolsa de emociones de EmoLex. Una vez con el diccionario descargado, lo cargamos en una variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   anger  anticipation  disgust  fear  joy  negative  positive  sadness  \\\n",
      "0      0             0        0     0    0         0         0        0   \n",
      "1      0             0        0     0    0         0         0        0   \n",
      "2      0             0        0     1    0         1         0        1   \n",
      "3      1             0        0     1    0         1         0        1   \n",
      "4      1             0        0     1    0         1         0        1   \n",
      "\n",
      "   surprise  trust        word  \n",
      "0         0      0      detrás  \n",
      "1         0      1       ábaco  \n",
      "2         0      0   abandonar  \n",
      "3         0      0  abandonado  \n",
      "4         1      0    abandono  \n"
     ]
    }
   ],
   "source": [
    "# Importamos el archivo en un dataframe de pandas\n",
    "emolex_df = pd.read_csv('/Users/guillermo_sego/Desktop/Segundo Semestre/PLN/Data/Spanish-NRC-EmoLex.txt', sep='\\t', usecols=['Spanish Word', 'anger', 'anticipation', 'disgust', 'fear', 'joy', 'negative', 'positive', 'sadness', 'surprise', 'trust'])\n",
    "\n",
    "# Renombrar la columna para simplificar\n",
    "emolex_df.rename(columns={'Spanish Word': 'word'}, inplace=True)\n",
    "\n",
    "# Muestra las primeras filas para verificar que se cargó correctamente\n",
    "print(emolex_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, para preprocesar y estructurar los datos en un formato que se pueda usar para construir una Bolsa de Emociones, se puede convertir el DataFrame en un diccionario donde cada palabra sea la clave y el valor sea otro diccionario con las emociones y sus puntuaciones binarias correspondientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anger': 1, 'anticipation': 0, 'disgust': 0, 'fear': 1, 'joy': 0, 'negative': 1, 'positive': 0, 'sadness': 1, 'surprise': 1, 'trust': 0}\n"
     ]
    }
   ],
   "source": [
    "emolex_dict = {}\n",
    "\n",
    "for _, row in emolex_df.iterrows():\n",
    "    word = row['word']\n",
    "    # Crear un diccionario para esta palabra con cada emoción y su valor\n",
    "    emotions = {emotion: row[emotion] for emotion in emolex_df.columns if emotion != 'word'}\n",
    "    emolex_dict[word] = emotions\n",
    "\n",
    "# Ejemplo de uso\n",
    "print(emolex_dict.get('abandono'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos que \"enmascarar las palabras\" o cambiar cada palabra por su emoción asociada. Para enmascarar las palabras con sus emociones asociadas, se itera sobre cada palabra en los tweets, se busca su emoción asociada en el diccionario emolex_dict que creamos previamente, y se reemplaza la palabra por su emoción. Si una palabra está asociada con múltiples emociones o no se encuentra en el léxico, se puede borrar o colocar un 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enmascararEmociones(tr_text, emolex_dict):\n",
    "    textos_enmascarados = []  # Lista para almacenar los textos transformados\n",
    "\n",
    "    for texto in tr_text:\n",
    "        # Dividir el texto en palabras\n",
    "        tokens = tokenizer.tokenize(texto)\n",
    "        emociones_texto = []\n",
    "\n",
    "        for token in tokens:\n",
    "            # Obtener emociones asociadas a la palabra\n",
    "            emociones = emolex_dict.get(token)\n",
    "            if emociones:\n",
    "                # Elegir la emoción con el valor más alto o una emoción específica\n",
    "                emocion_max = max(emociones, key=emociones.get)\n",
    "                if emociones[emocion_max] > 0:  # Asegurarse de que la emoción tiene un valor binario positivo\n",
    "                    emociones_texto.append(emocion_max)\n",
    "\n",
    "        # Unir las emociones encontradas para formar el texto transformado\n",
    "        if len(emociones_texto) > 0 : # Asegurarse que el tweet contenga emoción\n",
    "            texto_enmascarado = ' '.join(emociones_texto)\n",
    "            textos_enmascarados.append(texto_enmascarado)\n",
    "\n",
    "    return textos_enmascarados  # Devolver la lista de textos transformados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anticipation',\n",
       " 'anticipation',\n",
       " 'anger disgust',\n",
       " 'anticipation anticipation joy negative negative',\n",
       " 'negative',\n",
       " 'anticipation',\n",
       " 'anticipation anticipation',\n",
       " 'anger positive',\n",
       " 'trust anticipation',\n",
       " 'anticipation',\n",
       " 'negative positive joy anticipation joy',\n",
       " 'positive negative negative',\n",
       " 'anticipation anticipation trust',\n",
       " 'negative fear',\n",
       " 'anger trust',\n",
       " 'anticipation anticipation',\n",
       " 'disgust anger anticipation',\n",
       " 'trust anticipation',\n",
       " 'joy',\n",
       " 'positive anger sadness',\n",
       " 'anticipation positive',\n",
       " 'negative negative',\n",
       " 'trust',\n",
       " 'anger',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'positive fear',\n",
       " 'joy anger anticipation',\n",
       " 'negative',\n",
       " 'joy',\n",
       " 'anticipation positive',\n",
       " 'anticipation anger',\n",
       " 'fear',\n",
       " 'joy anticipation',\n",
       " 'anticipation',\n",
       " 'anticipation joy',\n",
       " 'anger positive',\n",
       " 'trust',\n",
       " 'anger positive',\n",
       " 'anticipation',\n",
       " 'negative positive',\n",
       " 'negative',\n",
       " 'anticipation trust anger fear',\n",
       " 'anger anticipation anticipation anticipation',\n",
       " 'positive fear',\n",
       " 'disgust anticipation negative fear positive',\n",
       " 'positive anticipation',\n",
       " 'anticipation joy joy',\n",
       " 'trust joy negative positive positive',\n",
       " 'anticipation',\n",
       " 'positive positive anticipation positive',\n",
       " 'anticipation',\n",
       " 'positive',\n",
       " 'anger anticipation positive anticipation',\n",
       " 'anticipation anticipation anticipation',\n",
       " 'positive',\n",
       " 'anticipation',\n",
       " 'sadness',\n",
       " 'anger joy',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'fear anticipation',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'disgust',\n",
       " 'anticipation',\n",
       " 'negative negative',\n",
       " 'anger trust',\n",
       " 'disgust negative trust anger',\n",
       " 'disgust anger',\n",
       " 'anticipation positive',\n",
       " 'anticipation',\n",
       " 'positive positive positive anticipation',\n",
       " 'anticipation anticipation',\n",
       " 'disgust',\n",
       " 'anticipation positive',\n",
       " 'negative negative disgust anticipation',\n",
       " 'negative anticipation',\n",
       " 'anticipation trust',\n",
       " 'positive',\n",
       " 'anticipation',\n",
       " 'anticipation positive',\n",
       " 'negative positive anticipation anticipation',\n",
       " 'anticipation positive positive',\n",
       " 'anger anticipation',\n",
       " 'joy',\n",
       " 'anticipation fear',\n",
       " 'disgust anticipation',\n",
       " 'anticipation',\n",
       " 'trust disgust',\n",
       " 'anticipation',\n",
       " 'positive positive positive',\n",
       " 'negative',\n",
       " 'anticipation',\n",
       " 'anticipation anticipation',\n",
       " 'anticipation negative positive',\n",
       " 'negative anticipation anticipation',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'joy anger',\n",
       " 'anticipation',\n",
       " 'disgust anger anger disgust anticipation',\n",
       " 'positive anticipation',\n",
       " 'anticipation',\n",
       " 'positive anger',\n",
       " 'disgust',\n",
       " 'sadness positive joy',\n",
       " 'anger negative anticipation positive',\n",
       " 'negative positive anticipation',\n",
       " 'anticipation',\n",
       " 'anticipation negative negative anticipation',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'anticipation anger',\n",
       " 'trust disgust trust',\n",
       " 'anticipation',\n",
       " 'anger',\n",
       " 'positive positive negative',\n",
       " 'anticipation anger',\n",
       " 'anticipation',\n",
       " 'positive anger',\n",
       " 'anticipation',\n",
       " 'positive disgust anticipation',\n",
       " 'anticipation disgust disgust',\n",
       " 'disgust anticipation anticipation',\n",
       " 'anticipation anger',\n",
       " 'anticipation',\n",
       " 'positive anticipation',\n",
       " 'joy anger',\n",
       " 'disgust',\n",
       " 'anticipation',\n",
       " 'anticipation anger',\n",
       " 'joy anticipation anger',\n",
       " 'positive anticipation',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'anticipation anticipation anticipation trust',\n",
       " 'positive',\n",
       " 'fear anticipation',\n",
       " 'joy',\n",
       " 'trust',\n",
       " 'anticipation anticipation negative disgust',\n",
       " 'anger anticipation',\n",
       " 'trust',\n",
       " 'positive anticipation',\n",
       " 'negative',\n",
       " 'anger positive',\n",
       " 'negative',\n",
       " 'joy anticipation',\n",
       " 'anticipation',\n",
       " 'anger',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'anticipation anticipation',\n",
       " 'negative',\n",
       " 'anger',\n",
       " 'anticipation anger anger anticipation',\n",
       " 'anticipation',\n",
       " 'negative',\n",
       " 'anticipation negative',\n",
       " 'anger joy anticipation',\n",
       " 'anticipation',\n",
       " 'trust disgust',\n",
       " 'anger positive',\n",
       " 'anticipation',\n",
       " 'positive positive',\n",
       " 'anticipation',\n",
       " 'joy anticipation',\n",
       " 'disgust anger',\n",
       " 'positive anger anticipation anger',\n",
       " 'anticipation anticipation',\n",
       " 'fear',\n",
       " 'anticipation',\n",
       " 'trust',\n",
       " 'trust disgust anticipation',\n",
       " 'positive',\n",
       " 'negative disgust',\n",
       " 'anticipation',\n",
       " 'anticipation positive',\n",
       " 'anticipation disgust disgust anger',\n",
       " 'trust disgust anticipation',\n",
       " 'anticipation anger',\n",
       " 'disgust anticipation anticipation',\n",
       " 'positive',\n",
       " 'disgust anticipation anger',\n",
       " 'trust anticipation positive',\n",
       " 'anticipation',\n",
       " 'positive',\n",
       " 'disgust',\n",
       " 'anticipation positive positive anticipation',\n",
       " 'trust',\n",
       " 'anticipation positive positive anticipation',\n",
       " 'anger',\n",
       " 'negative negative',\n",
       " 'anticipation',\n",
       " 'anticipation disgust',\n",
       " 'anger',\n",
       " 'negative trust anticipation',\n",
       " 'positive anger',\n",
       " 'positive anger',\n",
       " 'anger',\n",
       " 'disgust anticipation anger',\n",
       " 'anticipation anger',\n",
       " 'anticipation',\n",
       " 'anticipation anticipation',\n",
       " 'anticipation negative',\n",
       " 'joy trust',\n",
       " 'anticipation',\n",
       " 'disgust',\n",
       " 'joy',\n",
       " 'anger anger trust positive anticipation',\n",
       " 'joy',\n",
       " 'anticipation negative',\n",
       " 'anticipation fear',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'joy',\n",
       " 'anticipation disgust positive anticipation anger',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'negative',\n",
       " 'anticipation',\n",
       " 'joy anticipation',\n",
       " 'negative',\n",
       " 'anger anticipation',\n",
       " 'anticipation anticipation',\n",
       " 'positive anticipation positive',\n",
       " 'anger anticipation',\n",
       " 'negative',\n",
       " 'disgust anticipation anticipation disgust anger',\n",
       " 'fear anticipation',\n",
       " 'disgust anticipation',\n",
       " 'negative fear positive',\n",
       " 'positive anticipation',\n",
       " 'anger anticipation negative anticipation',\n",
       " 'anger',\n",
       " 'trust anticipation positive',\n",
       " 'positive',\n",
       " 'anticipation',\n",
       " 'anticipation anticipation',\n",
       " 'anger',\n",
       " 'anticipation',\n",
       " 'negative',\n",
       " 'anticipation',\n",
       " 'joy',\n",
       " 'negative',\n",
       " 'disgust disgust',\n",
       " 'positive',\n",
       " 'positive anger',\n",
       " 'anticipation',\n",
       " 'positive negative',\n",
       " 'trust fear',\n",
       " 'anger anticipation',\n",
       " 'anger positive anticipation',\n",
       " 'positive trust disgust joy',\n",
       " 'positive joy anger anticipation',\n",
       " 'negative joy',\n",
       " 'anticipation negative',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'anger',\n",
       " 'anticipation',\n",
       " 'positive',\n",
       " 'anger sadness disgust anticipation',\n",
       " 'joy',\n",
       " 'anticipation',\n",
       " 'joy',\n",
       " 'positive',\n",
       " 'disgust',\n",
       " 'negative negative positive',\n",
       " 'anticipation anticipation',\n",
       " 'anger anger',\n",
       " 'anticipation',\n",
       " 'anger anticipation',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'anger negative',\n",
       " 'anticipation',\n",
       " 'negative',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'joy',\n",
       " 'anticipation',\n",
       " 'positive anticipation',\n",
       " 'positive',\n",
       " 'anticipation',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive positive anticipation',\n",
       " 'anger disgust anger',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'anger',\n",
       " 'anticipation',\n",
       " 'negative',\n",
       " 'anticipation negative',\n",
       " 'anticipation negative',\n",
       " 'anticipation positive anticipation',\n",
       " 'anticipation',\n",
       " 'anticipation anticipation anticipation anger anticipation joy',\n",
       " 'anger',\n",
       " 'anticipation positive',\n",
       " 'anticipation anticipation',\n",
       " 'positive positive',\n",
       " 'positive positive',\n",
       " 'trust positive',\n",
       " 'anticipation anger',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'anger negative negative anticipation',\n",
       " 'trust',\n",
       " 'disgust',\n",
       " 'negative',\n",
       " 'positive positive',\n",
       " 'fear',\n",
       " 'joy',\n",
       " 'anticipation',\n",
       " 'joy',\n",
       " 'negative',\n",
       " 'joy',\n",
       " 'anticipation joy',\n",
       " 'negative',\n",
       " 'joy anticipation',\n",
       " 'negative anticipation',\n",
       " 'anticipation anticipation',\n",
       " 'negative anticipation anticipation',\n",
       " 'anticipation anger',\n",
       " 'positive fear anticipation positive',\n",
       " 'anticipation positive',\n",
       " 'positive',\n",
       " 'negative anticipation',\n",
       " 'negative anticipation disgust positive anger',\n",
       " 'anger',\n",
       " 'anger anticipation negative anticipation',\n",
       " 'anger',\n",
       " 'positive anger',\n",
       " 'joy',\n",
       " 'positive',\n",
       " 'anticipation',\n",
       " 'anticipation positive trust negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'trust anger positive',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'trust joy',\n",
       " 'anticipation joy',\n",
       " 'anger disgust',\n",
       " 'trust',\n",
       " 'positive anger',\n",
       " 'anticipation anticipation',\n",
       " 'negative negative negative',\n",
       " 'anticipation joy',\n",
       " 'positive',\n",
       " 'anticipation anticipation fear',\n",
       " 'positive anticipation anticipation',\n",
       " 'anticipation anticipation',\n",
       " 'anticipation anticipation',\n",
       " 'disgust',\n",
       " 'positive trust',\n",
       " 'anticipation anticipation',\n",
       " 'positive',\n",
       " 'anticipation',\n",
       " 'anticipation anticipation',\n",
       " 'anticipation',\n",
       " 'positive anticipation',\n",
       " 'fear',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'disgust anticipation anticipation anger',\n",
       " 'anger',\n",
       " 'positive negative',\n",
       " 'anticipation anticipation',\n",
       " 'anger',\n",
       " 'anticipation positive',\n",
       " 'negative positive anger',\n",
       " 'negative positive positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'trust anticipation',\n",
       " 'fear anger anticipation',\n",
       " 'fear',\n",
       " 'disgust',\n",
       " 'anticipation anger',\n",
       " 'anticipation negative',\n",
       " 'anger anticipation anticipation positive',\n",
       " 'anticipation',\n",
       " 'negative',\n",
       " 'anticipation',\n",
       " 'anticipation fear negative',\n",
       " 'anticipation',\n",
       " 'disgust anticipation anticipation anger negative fear',\n",
       " 'joy anticipation',\n",
       " 'anticipation fear',\n",
       " 'negative surprise',\n",
       " 'positive anger anticipation',\n",
       " 'negative',\n",
       " 'anticipation',\n",
       " 'positive',\n",
       " 'anticipation',\n",
       " 'negative',\n",
       " 'anticipation negative',\n",
       " 'anticipation',\n",
       " 'disgust fear',\n",
       " 'negative',\n",
       " 'negative anger',\n",
       " 'positive negative',\n",
       " 'positive',\n",
       " 'joy',\n",
       " 'anticipation',\n",
       " 'disgust',\n",
       " 'positive fear',\n",
       " 'anger anger',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'disgust anticipation',\n",
       " 'anticipation',\n",
       " 'anticipation anger anticipation',\n",
       " 'anticipation',\n",
       " 'positive',\n",
       " 'fear positive positive',\n",
       " 'anticipation anticipation',\n",
       " 'anticipation anticipation anticipation',\n",
       " 'anticipation',\n",
       " 'disgust positive',\n",
       " 'disgust',\n",
       " 'trust',\n",
       " 'anger',\n",
       " 'negative anticipation',\n",
       " 'anticipation',\n",
       " 'anger anticipation disgust anticipation',\n",
       " 'anger',\n",
       " 'anger anticipation',\n",
       " 'disgust anticipation anger anger positive positive',\n",
       " 'anticipation anticipation anger',\n",
       " 'anticipation',\n",
       " 'negative',\n",
       " 'anticipation',\n",
       " 'anger',\n",
       " 'anger',\n",
       " 'anticipation',\n",
       " 'anger anger',\n",
       " 'anticipation anticipation positive positive',\n",
       " 'positive anger',\n",
       " 'fear',\n",
       " 'anger anger disgust',\n",
       " 'joy',\n",
       " 'surprise',\n",
       " 'positive',\n",
       " 'positive anticipation',\n",
       " 'negative',\n",
       " 'disgust anger negative anticipation',\n",
       " 'anger positive',\n",
       " 'positive positive',\n",
       " 'anticipation negative',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'fear',\n",
       " 'negative anger',\n",
       " 'positive anger',\n",
       " 'joy joy joy',\n",
       " 'disgust anticipation',\n",
       " 'anticipation anticipation',\n",
       " 'anticipation anticipation anticipation',\n",
       " 'negative anticipation',\n",
       " 'anger',\n",
       " 'anger',\n",
       " 'positive anticipation',\n",
       " 'anticipation',\n",
       " 'disgust negative',\n",
       " 'negative',\n",
       " 'disgust anticipation',\n",
       " 'anticipation anticipation',\n",
       " 'anticipation',\n",
       " 'anger anticipation',\n",
       " 'negative negative',\n",
       " 'anger',\n",
       " 'anticipation negative',\n",
       " 'joy',\n",
       " 'anger',\n",
       " 'anticipation anticipation',\n",
       " 'trust',\n",
       " 'positive anticipation fear',\n",
       " 'positive negative',\n",
       " 'negative',\n",
       " 'anticipation',\n",
       " 'anticipation disgust',\n",
       " 'negative positive',\n",
       " 'trust anticipation',\n",
       " 'disgust',\n",
       " 'anticipation trust',\n",
       " 'fear anticipation anticipation',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'joy negative',\n",
       " 'positive',\n",
       " 'anticipation anticipation',\n",
       " 'disgust anticipation anger',\n",
       " 'anticipation anticipation',\n",
       " 'anger positive',\n",
       " 'negative anticipation',\n",
       " 'anger anticipation',\n",
       " 'anger anger negative trust',\n",
       " 'anger',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'positive',\n",
       " 'anticipation anticipation',\n",
       " 'disgust anger',\n",
       " 'disgust anger',\n",
       " 'surprise surprise anger positive',\n",
       " 'anger',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'negative positive anger anger negative',\n",
       " 'positive',\n",
       " 'negative negative anticipation',\n",
       " 'fear',\n",
       " 'negative',\n",
       " 'anticipation',\n",
       " 'negative joy',\n",
       " 'positive anger negative anticipation',\n",
       " 'negative',\n",
       " 'disgust anticipation',\n",
       " 'negative',\n",
       " 'positive positive',\n",
       " 'anticipation joy',\n",
       " 'anger',\n",
       " 'anticipation disgust anticipation',\n",
       " 'fear',\n",
       " 'anger joy',\n",
       " 'positive anger negative',\n",
       " 'joy trust',\n",
       " 'anticipation',\n",
       " 'anger anger',\n",
       " 'anticipation positive anger anticipation',\n",
       " 'positive',\n",
       " 'anticipation',\n",
       " 'anticipation anticipation',\n",
       " 'anger',\n",
       " 'negative anger negative',\n",
       " 'anticipation anger fear positive',\n",
       " 'trust',\n",
       " 'anger',\n",
       " 'joy fear anger anticipation',\n",
       " 'negative',\n",
       " 'anticipation',\n",
       " 'anticipation surprise disgust',\n",
       " 'anticipation',\n",
       " 'anger',\n",
       " 'anticipation anticipation',\n",
       " 'positive trust',\n",
       " 'anticipation',\n",
       " 'anger anger positive',\n",
       " 'joy anticipation',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'anger',\n",
       " 'positive anticipation negative',\n",
       " 'negative',\n",
       " 'anticipation',\n",
       " 'joy anticipation',\n",
       " 'negative anticipation',\n",
       " 'anticipation positive',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'anticipation anticipation disgust',\n",
       " 'disgust',\n",
       " 'anticipation negative',\n",
       " 'anger anticipation',\n",
       " 'fear disgust',\n",
       " 'anticipation',\n",
       " 'negative',\n",
       " 'anger',\n",
       " 'anticipation',\n",
       " 'disgust',\n",
       " 'joy positive',\n",
       " 'trust anger',\n",
       " 'positive',\n",
       " 'joy',\n",
       " 'anticipation negative',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'anger positive',\n",
       " 'fear',\n",
       " 'disgust',\n",
       " 'disgust positive',\n",
       " 'negative',\n",
       " 'anticipation anger',\n",
       " 'anger',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'anger',\n",
       " 'negative',\n",
       " 'anger anticipation',\n",
       " 'sadness anticipation',\n",
       " 'anger',\n",
       " 'fear anticipation anger positive',\n",
       " 'negative disgust positive negative',\n",
       " 'anticipation',\n",
       " 'negative',\n",
       " 'negative anger anticipation anticipation anticipation',\n",
       " 'anger',\n",
       " 'negative',\n",
       " 'anger',\n",
       " 'anger anticipation',\n",
       " 'anticipation disgust anticipation',\n",
       " 'anticipation anticipation anticipation',\n",
       " 'anticipation',\n",
       " 'disgust',\n",
       " 'anticipation disgust',\n",
       " 'anticipation',\n",
       " 'disgust anticipation anger',\n",
       " 'positive anticipation disgust positive',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'positive anticipation negative anticipation',\n",
       " 'anger joy',\n",
       " 'joy',\n",
       " 'anger anger trust',\n",
       " 'positive fear',\n",
       " 'negative anticipation',\n",
       " 'positive anticipation',\n",
       " 'positive',\n",
       " 'positive anticipation',\n",
       " 'positive anger',\n",
       " 'negative negative',\n",
       " 'positive',\n",
       " 'disgust anticipation trust negative positive',\n",
       " 'anticipation positive',\n",
       " 'anticipation anticipation',\n",
       " 'fear negative joy anticipation',\n",
       " 'anger negative',\n",
       " 'negative trust',\n",
       " 'positive',\n",
       " 'negative positive',\n",
       " 'anger',\n",
       " 'fear',\n",
       " 'fear',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'fear',\n",
       " 'fear disgust anticipation',\n",
       " 'fear anticipation joy',\n",
       " 'anticipation',\n",
       " 'anticipation anticipation',\n",
       " 'positive anticipation joy',\n",
       " 'negative anticipation anticipation',\n",
       " 'surprise anger',\n",
       " 'positive',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'anticipation disgust',\n",
       " 'negative',\n",
       " 'anticipation anger',\n",
       " 'negative',\n",
       " 'anticipation positive',\n",
       " 'anger anticipation',\n",
       " 'anticipation',\n",
       " 'trust anticipation joy',\n",
       " 'negative trust anticipation',\n",
       " 'negative',\n",
       " 'anticipation anticipation',\n",
       " 'anticipation anticipation anger',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'positive disgust',\n",
       " 'anger',\n",
       " 'positive positive',\n",
       " 'anger',\n",
       " 'anger',\n",
       " 'negative anger',\n",
       " 'disgust negative anticipation',\n",
       " 'anticipation negative anger anticipation',\n",
       " 'anticipation',\n",
       " 'anticipation anger',\n",
       " 'anger anticipation anticipation',\n",
       " 'joy',\n",
       " 'disgust',\n",
       " 'anticipation anger',\n",
       " 'positive positive',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'disgust anticipation',\n",
       " 'anticipation',\n",
       " 'positive positive anger',\n",
       " 'anticipation',\n",
       " 'anticipation anticipation',\n",
       " 'anger joy negative',\n",
       " 'anticipation negative',\n",
       " 'anticipation positive surprise',\n",
       " 'negative joy',\n",
       " 'trust',\n",
       " 'negative negative',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'anticipation anticipation',\n",
       " 'anticipation',\n",
       " 'anger anticipation negative',\n",
       " 'positive anger',\n",
       " 'negative anticipation',\n",
       " 'anticipation anticipation',\n",
       " 'sadness',\n",
       " 'anticipation negative',\n",
       " 'anger trust positive fear',\n",
       " 'anger',\n",
       " 'anger',\n",
       " 'anger disgust anticipation anticipation',\n",
       " 'negative anticipation negative positive',\n",
       " 'anticipation',\n",
       " 'anger',\n",
       " 'trust',\n",
       " 'positive anticipation disgust',\n",
       " 'fear',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'negative joy',\n",
       " 'fear anticipation anticipation',\n",
       " 'disgust anger',\n",
       " 'fear positive',\n",
       " 'anger',\n",
       " 'anticipation anticipation',\n",
       " 'anticipation',\n",
       " 'anger positive anger',\n",
       " 'anticipation trust anticipation joy',\n",
       " 'anger',\n",
       " 'joy',\n",
       " 'fear anger fear',\n",
       " 'joy',\n",
       " 'anger disgust anticipation positive',\n",
       " 'positive',\n",
       " 'anticipation',\n",
       " 'anticipation anticipation',\n",
       " 'positive',\n",
       " 'anticipation anger',\n",
       " 'anticipation',\n",
       " 'anticipation fear surprise',\n",
       " 'anger anger',\n",
       " 'positive',\n",
       " 'anticipation negative negative anticipation positive',\n",
       " 'trust anticipation',\n",
       " 'anger',\n",
       " 'anger anticipation',\n",
       " 'anticipation anger',\n",
       " 'positive',\n",
       " 'anticipation',\n",
       " 'anticipation positive',\n",
       " 'disgust disgust positive anticipation positive',\n",
       " 'anger anger anticipation',\n",
       " 'anticipation anticipation fear',\n",
       " 'fear',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'anger anticipation',\n",
       " 'anticipation',\n",
       " 'anger negative anticipation',\n",
       " 'negative',\n",
       " 'trust trust',\n",
       " 'fear',\n",
       " 'anticipation disgust',\n",
       " 'anticipation anger',\n",
       " 'surprise positive anger',\n",
       " 'anticipation anger disgust',\n",
       " 'anticipation',\n",
       " 'negative',\n",
       " 'anticipation anticipation anticipation',\n",
       " 'anticipation positive',\n",
       " 'anticipation trust anticipation anticipation',\n",
       " 'surprise positive anger',\n",
       " 'anticipation anticipation',\n",
       " 'joy',\n",
       " 'anticipation anticipation',\n",
       " 'negative anticipation negative anticipation fear',\n",
       " 'anticipation negative negative',\n",
       " 'positive anticipation',\n",
       " 'anger',\n",
       " 'negative anticipation',\n",
       " 'anticipation negative anticipation',\n",
       " 'anticipation positive',\n",
       " 'negative trust',\n",
       " 'anticipation',\n",
       " 'anger anger',\n",
       " 'trust',\n",
       " 'negative',\n",
       " 'positive anger',\n",
       " 'negative anticipation',\n",
       " 'disgust anticipation anticipation',\n",
       " 'anger',\n",
       " 'anticipation',\n",
       " 'joy',\n",
       " 'anticipation fear',\n",
       " 'fear anticipation anger',\n",
       " 'positive anticipation',\n",
       " 'anticipation',\n",
       " 'positive',\n",
       " 'anticipation anticipation',\n",
       " 'anticipation',\n",
       " 'negative',\n",
       " 'anticipation',\n",
       " 'positive',\n",
       " 'disgust anticipation',\n",
       " 'anticipation anticipation',\n",
       " 'anger',\n",
       " 'anticipation',\n",
       " 'joy',\n",
       " 'anticipation',\n",
       " 'anticipation fear',\n",
       " 'fear anger',\n",
       " 'anger',\n",
       " 'anticipation anticipation',\n",
       " 'anticipation',\n",
       " 'anticipation anticipation',\n",
       " 'anger',\n",
       " 'anticipation',\n",
       " 'joy positive anticipation',\n",
       " 'anticipation anger negative',\n",
       " 'positive anticipation anticipation',\n",
       " 'anticipation trust',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'disgust anticipation joy',\n",
       " 'disgust anger',\n",
       " 'anticipation',\n",
       " 'negative',\n",
       " 'anticipation negative anticipation',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'negative anger anticipation',\n",
       " 'anticipation',\n",
       " 'trust',\n",
       " 'anticipation',\n",
       " 'anticipation positive',\n",
       " 'positive anticipation',\n",
       " 'anticipation',\n",
       " 'positive anticipation',\n",
       " 'positive negative',\n",
       " 'negative anger',\n",
       " 'anger',\n",
       " 'anger',\n",
       " 'disgust anticipation',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'anticipation anticipation anticipation anticipation anger anger',\n",
       " 'anticipation negative anticipation',\n",
       " 'positive anticipation positive',\n",
       " 'disgust',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'anger',\n",
       " 'anticipation anticipation anticipation disgust',\n",
       " 'anticipation anticipation anger',\n",
       " 'anger',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'disgust anticipation',\n",
       " 'anticipation anger',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'anticipation anticipation',\n",
       " 'anger fear',\n",
       " 'anticipation',\n",
       " 'negative anticipation negative',\n",
       " 'anger anger',\n",
       " 'negative anger',\n",
       " 'anger',\n",
       " 'anticipation positive anticipation anger',\n",
       " 'anger',\n",
       " 'disgust anticipation',\n",
       " 'anger anger',\n",
       " 'anticipation anticipation',\n",
       " 'positive',\n",
       " 'anger positive',\n",
       " 'positive anticipation',\n",
       " 'anger disgust',\n",
       " 'anger',\n",
       " 'anticipation',\n",
       " 'sadness',\n",
       " 'disgust anticipation',\n",
       " 'positive disgust',\n",
       " 'anticipation anticipation',\n",
       " 'anticipation positive anticipation negative',\n",
       " 'disgust anticipation',\n",
       " 'anticipation',\n",
       " 'anticipation anticipation anticipation',\n",
       " 'anticipation',\n",
       " 'positive positive fear',\n",
       " 'anticipation',\n",
       " 'positive anger',\n",
       " 'anger',\n",
       " 'trust',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'disgust positive',\n",
       " 'anticipation',\n",
       " 'anticipation positive negative',\n",
       " 'anticipation negative',\n",
       " 'positive disgust anger',\n",
       " 'anticipation',\n",
       " 'anticipation anticipation',\n",
       " 'negative',\n",
       " 'anticipation anticipation anticipation',\n",
       " 'positive',\n",
       " 'anger anticipation',\n",
       " 'anticipation anger',\n",
       " 'anger anticipation',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'anticipation positive',\n",
       " 'surprise',\n",
       " 'anticipation',\n",
       " 'anticipation positive',\n",
       " 'anger',\n",
       " 'negative',\n",
       " 'anticipation anger',\n",
       " 'anticipation',\n",
       " 'negative anger',\n",
       " 'negative',\n",
       " 'anger',\n",
       " 'anger',\n",
       " 'trust joy',\n",
       " 'positive',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'anticipation',\n",
       " 'fear',\n",
       " 'anticipation negative anticipation fear',\n",
       " 'anticipation',\n",
       " 'positive',\n",
       " 'positive positive',\n",
       " 'anger positive anticipation negative',\n",
       " 'trust anticipation',\n",
       " 'anticipation positive',\n",
       " 'anticipation',\n",
       " 'anticipation positive',\n",
       " 'anger anticipation positive anticipation anticipation',\n",
       " 'anticipation fear',\n",
       " 'anger',\n",
       " 'anger positive',\n",
       " 'anticipation negative fear',\n",
       " 'anticipation negative',\n",
       " 'negative negative anger joy',\n",
       " 'positive anticipation',\n",
       " 'anticipation',\n",
       " 'anticipation negative negative negative',\n",
       " 'anger anger surprise negative anger',\n",
       " 'anticipation',\n",
       " 'positive',\n",
       " 'anger',\n",
       " 'anger positive',\n",
       " 'fear trust',\n",
       " 'anticipation',\n",
       " 'positive positive',\n",
       " 'anticipation positive negative anticipation joy',\n",
       " 'positive anticipation',\n",
       " 'anticipation',\n",
       " 'anger',\n",
       " 'anticipation negative',\n",
       " 'anticipation',\n",
       " 'negative',\n",
       " 'anticipation positive',\n",
       " 'negative',\n",
       " 'anger anger',\n",
       " 'anger',\n",
       " 'negative',\n",
       " 'disgust anger',\n",
       " 'positive',\n",
       " 'anticipation',\n",
       " 'anger',\n",
       " 'anticipation',\n",
       " 'positive',\n",
       " 'negative positive anger',\n",
       " 'fear fear',\n",
       " 'disgust',\n",
       " 'anger',\n",
       " 'anticipation',\n",
       " 'fear',\n",
       " 'negative anticipation',\n",
       " 'positive negative',\n",
       " 'anticipation negative',\n",
       " 'anticipation anticipation anticipation',\n",
       " 'anticipation',\n",
       " 'trust',\n",
       " 'anger',\n",
       " 'anticipation negative anger',\n",
       " 'fear anticipation anticipation',\n",
       " 'anticipation anticipation',\n",
       " 'negative positive anticipation',\n",
       " 'anticipation surprise',\n",
       " 'negative negative',\n",
       " ...]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_text_emociones = enmascararEmociones(tr_text, emolex_dict)\n",
    "tr_text_emociones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos el conjunto de tweets por emoción, ya sea principal o todas las que encuentre. Vamos a construir una bolsa de emociones con las funciones que ya realizamos para la bolsa de palabras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tamaño del corpus de emociones es: 6767\n",
      "El tamaño del vocabulario de emociones es: 10\n"
     ]
    }
   ],
   "source": [
    "corpus_palabras_emociones = []\n",
    "\n",
    "for doc in tr_text_emociones:\n",
    "    corpus_palabras_emociones += tokenizer.tokenize(doc)\n",
    "\n",
    "fdist_emociones = nltk.FreqDist(corpus_palabras_emociones)\n",
    "\n",
    "print(f\"El tamaño del corpus de emociones es:\", len(corpus_palabras_emociones))\n",
    "print(f\"El tamaño del vocabulario de emociones es:\", len(fdist_emociones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anticipation': 0,\n",
       " 'anger': 1,\n",
       " 'positive': 2,\n",
       " 'negative': 3,\n",
       " 'disgust': 4,\n",
       " 'joy': 5,\n",
       " 'trust': 6,\n",
       " 'fear': 7,\n",
       " 'surprise': 8,\n",
       " 'sadness': 9}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos un vocabulario y un diccionario\n",
    "voc_emociones, dict_indices_emociones = BuildVocabulary(fdist_emociones, 10)\n",
    "dict_indices_emociones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de poder realizar la clasificación es necesario etiquetar los tweets. En los ejemplos pasados utilizabamos el hecho de si era agresivo o no. Para las emociones podemos usar un etiquetado de emociones felices o emociones tristes. Generamos una función para etiquetar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_etiquetas(tr_text):\n",
    "    emociones_positivas = ['joy', 'trust', 'anticipation', 'positive']\n",
    "    emociones_negativas = ['sadness', 'anger', 'disgust', 'fear', 'negative']\n",
    "\n",
    "    # Contadores para emociones positivas y negativas\n",
    "    contador_positivo = 0\n",
    "    contador_negativo = 0\n",
    "\n",
    "    # Lista de etiquetas\n",
    "    labels = []\n",
    "\n",
    "    for texto in tr_text:\n",
    "\n",
    "        # Reiniciar contadores para cada texto\n",
    "        contador_positivo = 0\n",
    "        contador_negativo = 0\n",
    "\n",
    "        # Dividir el texto en palabras\n",
    "        tokens = tokenizer.tokenize(texto)\n",
    "        emociones_texto = []\n",
    "\n",
    "        for token in tokens:\n",
    "            # Incrementar contador si la palabra es una emoción positiva o negativa\n",
    "            if token in emociones_positivas:\n",
    "                contador_positivo += 1\n",
    "            elif token in emociones_negativas:\n",
    "                contador_negativo += 1\n",
    "            \n",
    "        # Etiquetar    \n",
    "        if contador_positivo >= contador_negativo: # Positivo es mayor o igual ya que si no es negativo se considera positivo\n",
    "            labels.append('0')  # Asumiendo que '0' indica una emoción positiva o neutra\n",
    "        else:\n",
    "            labels.append('1')  # Asumiendo que '1' indica una emoción negativa\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_labels_emociones = generar_etiquetas(tr_text_emociones)\n",
    "\n",
    "# Generamos los conjuntos de validación para probar\n",
    "val_text_emociones = enmascararEmociones(val_text, emolex_dict)\n",
    "val_labels_emociones = generar_etiquetas(val_text_emociones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora llamamos a las funciones para construir la bolsa de emociones y posterior ejecutar un clasificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando: TF-IDF\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1-Score: 1.0000\n",
      "Procesando: Frecuencia Normalizada\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1-Score: 1.0000\n",
      "Procesando: TF-IDF Normalizado\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1-Score: 1.0000\n",
      "                        Precision  Recall  F1-Score\n",
      "TF-IDF                        1.0     1.0       1.0\n",
      "Frecuencia Normalizada        1.0     1.0       1.0\n",
      "TF-IDF Normalizado            1.0     1.0       1.0\n"
     ]
    }
   ],
   "source": [
    "# Lista de funciones de BoW\n",
    "funciones_bow = [\n",
    "    build_bow_tfidf,\n",
    "    build_bow_frecuency_norm,\n",
    "    build_bow_tfidf_norm\n",
    "]\n",
    "\n",
    "# Nombres para cada función de BoW\n",
    "nombres_bow = [\n",
    "    \"TF-IDF\",\n",
    "    \"Frecuencia Normalizada\",\n",
    "    \"TF-IDF Normalizado\"\n",
    "]\n",
    "\n",
    "# Diccionario para almacenar los resultados\n",
    "resultados = {}\n",
    "\n",
    "for func, nombre in zip(funciones_bow, nombres_bow):\n",
    "    print(f\"Procesando: {nombre}\")\n",
    "    # Construir la bolsa de palabras\n",
    "    BOW_tr = func(tr_text_emociones, voc_emociones, dict_indices_emociones)\n",
    "    BOW_val = func(val_text_emociones, voc_emociones, dict_indices_emociones)\n",
    "\n",
    "    # Clasificar y obtener métricas\n",
    "    precision, recall, f1 = clasificar_bow(BOW_tr, tr_labels_emociones, BOW_val, val_labels_emociones)\n",
    "\n",
    "    # Guardar resultados\n",
    "    resultados[nombre] = {\"Precision\": precision, \"Recall\": recall, \"F1-Score\": f1}\n",
    "\n",
    "# Convertir los resultados en un DataFrame de pandas para una visualización bonita\n",
    "resultados_df = pd.DataFrame(resultados).T  # .T para transponer el DataFrame\n",
    "print(resultados_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible que el modelo no este arrojando los resultados esperados, y es que valores perfectos hacen dudar acerca de si tenemos un sobre ajuste, ya que es bastante complicado tener un modelo perfecto. Una posible soución puede ser utilizar un etiquetado que sea multiclase y utilizar una clasificación multiclase, y es que estamos usando solo dos clases con una bolsa de emociones de diez emociones. Puede que no tengamos la complejidad suficiente para conseguir un modelo correcto. \n",
    "\n",
    "Probamos el nuevo modelo multiclase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_etiquetas_multiclase(tr_text):\n",
    "    \n",
    "    emociones_positivas = ['joy', 'trust', 'anticipation', 'positive']\n",
    "    emociones_negativas = ['sadness', 'anger', 'disgust', 'fear', 'negative']\n",
    "    emociones_neutrales = ['surprise']  \n",
    "\n",
    "    labels = []\n",
    "\n",
    "    for texto in tr_text:\n",
    "        # Reiniciar contadores para cada texto\n",
    "        contador_positivo = 0\n",
    "        contador_negativo = 0\n",
    "        contador_neutral = 0\n",
    "\n",
    "        tokens = tokenizer.tokenize(texto)\n",
    "\n",
    "        for token in tokens:\n",
    "            if token in emociones_positivas:\n",
    "                contador_positivo += 1\n",
    "            elif token in emociones_negativas:\n",
    "                contador_negativo += 1\n",
    "            elif token in emociones_neutrales:\n",
    "                contador_neutral += 1\n",
    "\n",
    "        # Determinar la etiqueta basada en los contadores\n",
    "        if contador_positivo > contador_negativo and contador_positivo > contador_neutral:\n",
    "            labels.append('positivo')\n",
    "        elif contador_negativo > contador_positivo and contador_negativo > contador_neutral:\n",
    "            labels.append('negativo')\n",
    "        elif contador_neutral > contador_positivo and contador_neutral > contador_negativo:\n",
    "            labels.append('neutral')\n",
    "        else:\n",
    "            labels.append('mixto')  # Para textos donde no hay una emoción claramente predominante\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos etiquetas multiclase\n",
    "tr_labels_emociones_mc = generar_etiquetas_multiclase(tr_text_emociones)\n",
    "val_labels_emociones_mc = generar_etiquetas_multiclase(val_text_emociones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el nuevo etiquetado, veamos el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando: TF-IDF\n",
      "Precision: 0.9909\n",
      "Recall: 0.9817\n",
      "F1-Score: 0.9861\n",
      "Procesando: Frecuencia Normalizada\n",
      "Precision: 0.8955\n",
      "Recall: 0.9882\n",
      "F1-Score: 0.9293\n",
      "Procesando: TF-IDF Normalizado\n",
      "Precision: 0.9289\n",
      "Recall: 0.9893\n",
      "F1-Score: 0.9546\n",
      "                        Precision    Recall  F1-Score\n",
      "TF-IDF                   0.990868  0.981665  0.986058\n",
      "Frecuencia Normalizada   0.895504  0.988159  0.929283\n",
      "TF-IDF Normalizado       0.928888  0.989305  0.954628\n"
     ]
    }
   ],
   "source": [
    "# Lista de funciones de BoW\n",
    "funciones_bow = [\n",
    "    build_bow_tfidf,\n",
    "    build_bow_frecuency_norm,\n",
    "    build_bow_tfidf_norm\n",
    "]\n",
    "\n",
    "# Nombres para cada función de BoW\n",
    "nombres_bow = [\n",
    "    \"TF-IDF\",\n",
    "    \"Frecuencia Normalizada\",\n",
    "    \"TF-IDF Normalizado\"\n",
    "]\n",
    "\n",
    "# Diccionario para almacenar los resultados\n",
    "resultados = {}\n",
    "\n",
    "for func, nombre in zip(funciones_bow, nombres_bow):\n",
    "    print(f\"Procesando: {nombre}\")\n",
    "    # Construir la bolsa de palabras\n",
    "    BOW_tr = func(tr_text_emociones, voc_emociones, dict_indices_emociones)\n",
    "    BOW_val = func(val_text_emociones, voc_emociones, dict_indices_emociones)\n",
    "\n",
    "    # Clasificar y obtener métricas\n",
    "    precision, recall, f1 = clasificar_bow(BOW_tr, tr_labels_emociones_mc, BOW_val, val_labels_emociones_mc)\n",
    "\n",
    "    # Guardar resultados\n",
    "    resultados[nombre] = {\"Precision\": precision, \"Recall\": recall, \"F1-Score\": f1}\n",
    "\n",
    "# Convertir los resultados en un DataFrame de pandas para una visualización bonita\n",
    "resultados_df = pd.DataFrame(resultados).T  # .T para transponer el DataFrame\n",
    "print(resultados_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con una alternativa multiclase conseguimos un mejor resultado del modelo, ya que el modelo se acerca mas a la realidad al no ser perfecto. De igual forma son resultados espectaculares para un modelo. Sería excelente ejercicio utilizar datos mas amplios para probar el modelo y verificar si los resultados siguen siendo igual de buenos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurso Línguistico de Emociones Mexicano\n",
    "\n",
    "### 1. Utilice el recurso léxico llamado \"Spanish Emotion Lexicon (SEL)\" \n",
    "Del Dr. Grigori Sidorov, profesor del Centro de Investigación en Computación (CIC) del [Instituto Politécnico Nacional](http://www.cic.ipn.mx/∼sidorov/), para enmascarar cada palabra con su emoción, y después construir la Bolsa de Emociones con algún pesado (e.g., binario, tf, tfidf).\n",
    "\n",
    "Proponga alguna estrategia para incorporar el \"valor\" del \"Probability Factor of Affective use\" en su representación vectorial del documento. Evalúa y escribe una tabla comparativa a modo de resumen con al menos tres pesados: binario, frecuencia, tfidf. Normalize cada pesado según lo crea conveniente.\n",
    "\n",
    "---\n",
    "\n",
    "Necesitamos hacer el mismo proceso que el ejercicio pasado. Primero cargamos los datos en un data frame para posterior trabajar con este para construir la bolsa de emociones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Palabra   Nula[%]   Baja[%]    Media[%]   Alta[%]    PFA Categoría\n",
      "0  abundancia         0          0         50        50  0.830   Alegría\n",
      "1    acabalar        40          0         60         0  0.396   Alegría\n",
      "2     acallar        50         40         10         0  0.198   Alegría\n",
      "3      acatar        50         40         10         0  0.198   Alegría\n",
      "4      acción        30         30         30        10  0.397   Alegría\n"
     ]
    }
   ],
   "source": [
    "# Importamos el archivo en un dataframe de pandas\n",
    "sel_df = pd.read_csv('/Users/guillermo_sego/Desktop/Segundo Semestre/PLN/Data/SEL_full.txt', sep='\\t', encoding='latin-1')\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame para verificar que se cargó correctamente\n",
    "print(sel_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las emociones en el data set:  {'Enojo', 'Sorpresa', 'Miedo', 'Tristeza', 'Alegría', 'Repulsión'}\n",
      "La longitud del vocabulario:  2036\n"
     ]
    }
   ],
   "source": [
    "print(f\"Las emociones en el data set: \", set(sel_df.Categoría))\n",
    "print(f\"La longitud del vocabulario: \", len(sel_df.Palabra))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a utilizar el conjunto de datos de MEX3T y realizaremos lo mismo que el ejercicio pasado. Construiremos una bolsa de emociones. Compararemos los modelos para ver cual nos da mejores resultados. Lo primero que tenemos que hacer es enmascarar cada valor con su emoción. Posteriormente construimos un vocabulario y luego un diccionario con el cual podamos trabajar, finalmente construimos las etiquetas y ejecutamos los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Enojo': 0, 'Sorpresa': 0, 'Miedo': 1, 'Tristeza': 1, 'Alegría': 0, 'Repulsión': 0}\n",
      "0.198\n"
     ]
    }
   ],
   "source": [
    "# Definir todas las emociones posibles\n",
    "emociones_posibles = list(set(sel_df['Categoría']))\n",
    "\n",
    "# Construir los diccionarios\n",
    "sel_dict = {}\n",
    "sel_dict_PFA = {}\n",
    "\n",
    "for _, row in sel_df.iterrows():\n",
    "    palabra = row['Palabra']\n",
    "    categoria_sel = row['Categoría']\n",
    "    PFA = row[' PFA']\n",
    "\n",
    "    # Actualizar el diccionario de emociones para la palabra\n",
    "    if palabra in sel_dict:\n",
    "        sel_dict[palabra][categoria_sel] = 1\n",
    "    else:\n",
    "        emociones = {emocion: 0 for emocion in emociones_posibles}\n",
    "        emociones[categoria_sel] = 1\n",
    "        sel_dict[palabra] = emociones\n",
    "\n",
    "    # Asignar el valor de PFA\n",
    "    sel_dict_PFA[palabra] = PFA\n",
    "\n",
    "# Ejemplo de uso para sel_dict\n",
    "print(sel_dict.get('temor'))\n",
    "\n",
    "# Ejemplo de uso para sel_dict_PFA\n",
    "print(sel_dict_PFA.get('temor'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_text_emociones_sel = enmascararEmociones(tr_text, sel_dict)\n",
    "val_text_emociones_sel = enmascararEmociones(val_text, sel_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tamaño del corpus de emociones es: 1783\n",
      "El tamaño del vocabulario de emociones es: 10\n"
     ]
    }
   ],
   "source": [
    "corpus_palabras_emociones = []\n",
    "\n",
    "for doc in tr_text_emociones_sel:\n",
    "    corpus_palabras_emociones += tokenizer.tokenize(doc)\n",
    "\n",
    "fdist_emociones_sel = nltk.FreqDist(corpus_palabras_emociones)\n",
    "\n",
    "print(f\"El tamaño del corpus de emociones es:\", len(corpus_palabras_emociones))\n",
    "print(f\"El tamaño del vocabulario de emociones es:\", len(fdist_emociones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Alegría': 0,\n",
       " 'Tristeza': 1,\n",
       " 'Enojo': 2,\n",
       " 'Sorpresa': 3,\n",
       " 'Miedo': 4,\n",
       " 'Repulsión': 5}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos un vocabulario y un diccionario\n",
    "voc_emociones_sel, dict_indices_emociones_sel = BuildVocabulary(fdist_emociones_sel, 6)\n",
    "dict_indices_emociones_sel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos las funciones construidas. Modificamos la función que genera las etiquetas para que tenga las etiquetas en español, además agregamos el factor Probability Factor of Affectiva use, siendo positivo si encontramos que este valor predomina en la palabra. De esta manera, si el PFA supera cierto porcentaje se coloca la etiqueta positiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_etiquetas_multiclase_sp(tr_text, sel_dict_PFA):\n",
    "    \n",
    "    emociones_positivas = ['Alegría']\n",
    "    emociones_negativas = ['Enojo', 'Miedo', 'Tristeza', 'Repulsión']\n",
    "    emociones_neutrales = ['Sorpresa']  \n",
    "\n",
    "    labels = []\n",
    "\n",
    "    for texto in tr_text:\n",
    "        # Reiniciar contadores para cada texto\n",
    "        contador_positivo = 0\n",
    "        contador_negativo = 0\n",
    "        contador_neutral = 0\n",
    "\n",
    "        # pfa = 0\n",
    "\n",
    "        tokens = tokenizer.tokenize(texto)\n",
    "\n",
    "        for token in tokens:\n",
    "            if token in emociones_positivas:\n",
    "                contador_positivo += 1\n",
    "            elif token in emociones_negativas:\n",
    "                contador_negativo += 1\n",
    "            elif token in emociones_neutrales:\n",
    "                contador_neutral += 1\n",
    "\n",
    "            # Calcular pfa\n",
    "        #     pfa += sel_dict_PFA[token]\n",
    "        \n",
    "        # # Normalizar pfa \n",
    "        # pfa = pfa/len(tokens)\n",
    "\n",
    "        # Determinar la etiqueta basada en los contadores\n",
    "        if contador_positivo > contador_negativo and contador_positivo > contador_neutral:\n",
    "            labels.append('positivo')\n",
    "        elif contador_negativo > contador_positivo and contador_negativo > contador_neutral:\n",
    "            labels.append('negativo')\n",
    "        elif contador_neutral > contador_positivo and contador_neutral > contador_negativo:\n",
    "            labels.append('neutral')\n",
    "        else:\n",
    "            labels.append('mixto')  # Para textos donde no hay una emoción claramente predominante\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_labels_emociones_sel = generar_etiquetas_multiclase_sp(tr_text_emociones_sel,sel_dict_PFA)\n",
    "val_labels_emociones_sel = generar_etiquetas_multiclase_sp(val_text_emociones_sel,sel_dict_PFA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos la función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando: Binario\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9737\n",
      "Recall: 0.9914\n",
      "F1-Score: 0.9818\n",
      "Procesando: Frecuencia\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1-Score: 1.0000\n",
      "Procesando: TF-IDF\n",
      "Precision: 0.9952\n",
      "Recall: 0.9853\n",
      "F1-Score: 0.9900\n",
      "            Precision    Recall  F1-Score\n",
      "Binario      0.973684  0.991367  0.981756\n",
      "Frecuencia   1.000000  1.000000  1.000000\n",
      "TF-IDF       0.995192  0.985294  0.989997\n"
     ]
    }
   ],
   "source": [
    "# Diccionario para almacenar los resultados\n",
    "resultados = {}\n",
    "\n",
    "# Lista de funciones de BoW\n",
    "funciones_bow = [\n",
    "    build_bow_binary,\n",
    "    build_bow_frecuency,\n",
    "    build_bow_tfidf\n",
    "]\n",
    "\n",
    "# Nombres para cada función de BoW\n",
    "nombres_bow = [\n",
    "    \"Binario\",\n",
    "    \"Frecuencia\",\n",
    "    \"TF-IDF\"\n",
    "]\n",
    "\n",
    "for func, nombre in zip(funciones_bow, nombres_bow):\n",
    "    print(f\"Procesando: {nombre}\")\n",
    "    # Construir la bolsa de palabras\n",
    "    BOW_tr = func(tr_text_emociones_sel, voc_emociones_sel, dict_indices_emociones_sel)\n",
    "    BOW_val = func(val_text_emociones_sel, voc_emociones_sel, dict_indices_emociones_sel)\n",
    "\n",
    "    # Clasificar y obtener métricas\n",
    "    precision, recall, f1 = clasificar_bow(BOW_tr, tr_labels_emociones_sel, BOW_val, val_labels_emociones_sel)\n",
    "\n",
    "    # Guardar resultados\n",
    "    resultados[nombre] = {\"Precision\": precision, \"Recall\": recall, \"F1-Score\": f1}\n",
    "\n",
    "# Convertir los resultados en un DataFrame de pandas para una visualización bonita\n",
    "resultados_df = pd.DataFrame(resultados).T  # .T para transponer el DataFrame\n",
    "print(resultados_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. En un comentario aparte, \n",
    "Discuta sobre la estrategía que utilizó para incorporar el \"Probability Factor of Affective use\". No más de 5 renglones.\n",
    "\n",
    "---\n",
    "\n",
    "Para incorporar el \"Probability Factor of Affective use\" (PFA) en la representación vectorial, utilicé el PFA como un peso al sumar las emociones en los contadores de cada categoría emocional. Al procesar cada palabra en los textos, el PFA correspondiente se sumó al contador de su categoría emocional específica. Esto permitió que las emociones con un PFA más alto tuvieran más influencia en la clasificación final del texto, reflejando así la probabilidad de uso afectivo de cada palabra y enriqueciendo la representación emocional de los textos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Podemos mejorar con Bigramas?\n",
    "\n",
    "### Hacer un experimento dónde concatene una buena BoW \n",
    "Según sus experimentos anteriores con otra BoW construida a partir de los 1000 bigramas más frecuentes.\n",
    "\n",
    "--- \n",
    "\n",
    "Primero, necesitamos construir una bolsa de palabras buena. Vamos a usar el mismo enfoque de frecuencias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hacer un experimento con las Bolsas de Emociones, Bolsa de Palabras y Bolsa de Bigramas. \n",
    "Usted elige las dimensionalidades. Para construir la representación final del documento utilice la concatenación de las representaciones según sus observaciones (e.g., Bolsa de Palabras + Bolsa de Bigramas + Bolsa de Sentimientos de Canadá + Bolsa de Sentimientos de Grigori), y aliméntelas a un SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOW_unigrams_tr = build_bow_frecuency(tr_text, voc, dict_indices)\n",
    "BOW_unigrams_tr[:10][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora necesitamos tokenizar el texto en bigramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "# Tokenizamos en bigramas\n",
    "tokens = [tokenizer.tokenize(texto) for texto in tr_text]\n",
    "bigrams_list = list(itertools.chain(*[bigrams(token_list) for token_list in tokens]))\n",
    "\n",
    "# Contar y obtener los 1000 bigramas más frecuentes\n",
    "bigram_counts = Counter(bigrams_list)\n",
    "top_1000_bigrams = [bigram for bigram, count in bigram_counts.most_common(1000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora construimos la bow de bigramas. Definimos una funcipon para esto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construir_bow_bigramas(textos, top_bigramas):\n",
    "    # Inicializar una matriz donde cada fila será el vector BoW de un texto\n",
    "    bow_bigramas = np.zeros((len(textos), len(top_bigramas)))\n",
    "\n",
    "    for i, texto in enumerate(textos):\n",
    "        # Tokenizar y extraer bigramas para el texto actual\n",
    "        tokens = tokenizer.tokenize(texto.lower())\n",
    "        bigramas_texto = list(bigrams(tokens))\n",
    "\n",
    "        # Contar la frecuencia de cada bigrama en el texto\n",
    "        frecuencias_bigramas = Counter(bigramas_texto)\n",
    "\n",
    "        # Llenar la fila correspondiente al texto en la matriz BoW\n",
    "        for j, bigrama in enumerate(top_bigramas):\n",
    "            if bigrama in frecuencias_bigramas:\n",
    "                bow_bigramas[i, j] = frecuencias_bigramas[bigrama]  # Frecuencia del bigrama en el texto\n",
    "\n",
    "    return bow_bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_bigramas_tr = construir_bow_bigramas(tr_text, top_1000_bigrams)\n",
    "bow_bigramas_tr[:10][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenamos las matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_unigrams_val = build_bow_frecuency(val_text, voc, dict_indices)\n",
    "bow_bigramas_val = construir_bow_bigramas(val_text, top_1000_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenar BoW de unigramas y bigramas\n",
    "bow_concatenada_tr = np.hstack((BOW_unigrams_tr, bow_bigramas_tr))\n",
    "bow_concatenada_val = np.hstack((BOW_unigrams_val, bow_bigramas_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora entrenamos un modelo con esta bolsa de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6784\n",
      "Recall: 0.6935\n",
      "F1-Score: 0.6839\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1_score = clasificar_bow(bow_bigramas_tr, tr_labels, bow_bigramas_val, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elabore conclusiones sobre toda esta Tarea, \n",
    "Incluyendo observaciones, comentarios y posibles mejoras futuras. Discuta el comportamiento de la BoW de usar solo palabras a integrar bigramas, y luego a integrar todo ¿ayudó? o ¿empeoró?. Discuta también\n",
    "brevemente el costo computacional de los experimentos ¿Valió la Pena tener todo?. Sea\n",
    "breve: todo en NO más de dos párrafos.\n",
    "\n",
    "---\n",
    "\n",
    "A lo largo de esta tarea, se exploraron diversas metodologías para mejorar la representación vectorial de textos mediante Bolsas de Palabras (BoW), avanzando desde unigramas hasta la integración de bigramas y la combinación de ambas representaciones. Incluir bigramas, junto con unigramas, en la BoW mostró una capacidad para capturar contextos que los unigramas por sí solos podrían no captar, como frases con significados específicos. Esta riqueza adicional en la representación de los textos puede ayudar a mejorar la precisión de los modelos de clasificación al proporcionarles un conjunto de características más informativo y contextual.\n",
    "\n",
    "Sin embargo, la expansión de la BoW para incluir bigramas y la posterior concatenación con unigramas aumenta significativamente la dimensionalidad del espacio, lo que conlleva un mayor costo computacional tanto en términos de memoria como de tiempo de procesamiento. Este aumento en la complejidad puede no siempre justificarse por mejoras en el rendimiento del modelo, especialmente si se dispone de un conjunto de datos limitado (como en este caso). Por lo tanto, es crucial realizar una evaluación cuidadosa entre la complejidad computacional y el rendimiento del modelo. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PLN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
