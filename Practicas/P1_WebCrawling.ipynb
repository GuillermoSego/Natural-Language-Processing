{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 1: Web Crawling\n",
    "Guillermo Segura Gómez\n",
    "\n",
    "Ingresar a la página del gobierno de México, concretamente la versión estenográfica de la mañanera del presidente. Hacer un scraper para obtener información de todas las conferencias. Se tiene primero que extraer la información de las url, luego de cada url extraer la data de lo que se dice. Obtener los datos mas limpios que se pueda. \n",
    "\n",
    "\n",
    "### Scraping vs Crawling\n",
    "\n",
    "* Web Crawling: Es el proceso de navegar por internet y recolectar información sobre las páginas web. Un crawler, también conocido como spider o bot, visita automáticamente las páginas web y sigue los enlaces (hipervínculos) que encuentra en estas páginas. El objetivo principal de un crawler es indexar la información de las páginas web para que puedan ser recuperadas más tarde por un motor de búsqueda. Googlebot es un ejemplo de un crawler muy conocido.\n",
    "\n",
    "* Web Scraping: Es el proceso de extraer datos específicos de páginas web. A diferencia del crawling, que puede ser más general en la recolección de información, el scraping está orientado a obtener detalles específicos como precios de productos, información de contacto, textos, etc. Para hacer scraping, usualmente se necesita analizar el HTML de la página y extraer los datos necesarios.\n",
    "\n",
    "Vamos a realizar un crawling para navegar la información y un scraping para obtener el texto. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ligas de cada conferencia\n",
    "\n",
    "Cargamos la liga general de la cual obtendremos todas las ligas. Utilizamos headers para que se nos reconozca como una computadora y ejecutar el crawling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libreria para hacer solicitudes http\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_general = 'https://presidente.gob.mx/secciones/version-estenografica/'\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.106 Safari/537.36'\n",
    "}\n",
    "response_general = requests.get(url_general, headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objeto que obtenemos es un tipo **response**. \n",
    "\n",
    "### ¿Que es un objeto tipo response?\n",
    "\n",
    "**Instancia de la Clase `Response`**: Cuando se hace una solicitud como `requests.get(url)`, la función devuelve un objeto de la clase `Response`. Este objeto contiene múltiples datos y métodos útiles para trabajar con la respuesta del servidor.\n",
    "\n",
    "### Contenido del Objeto `Response`\n",
    "\n",
    "El objeto `Response` incluye varios atributos y métodos útiles, como:\n",
    "\n",
    "- **`.text`**: El contenido de la respuesta, en forma de cadena de texto (string). Si la respuesta es HTML, por ejemplo, aquí obtendrás el código HTML completo de la página.\n",
    "\n",
    "- **`.content`**: El contenido de la respuesta en formato binario (bytes). Esto es útil para datos no textuales como imágenes.\n",
    "\n",
    "- **`.json()`**: Si la respuesta es JSON, puedes usar este método para analizarla automáticamente y convertirla en un diccionario de Python. Por ejemplo, si accedes a un endpoint de una API que devuelve JSON, harías `response.json()` para obtener los datos en un formato manejable.\n",
    "\n",
    "- **`.status_code`**: El código de estado HTTP de la respuesta (como 200 para éxito, 404 para no encontrado, etc.).\n",
    "\n",
    "- **`.headers`**: Un diccionario que contiene los encabezados HTTP de la respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se guarda el contenido de la página en un archivo.txt\n",
    "with open('Conferencias_General.txt', 'w', encoding=response_general.encoding) as file:\n",
    "    file.write(response_general.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para limpiar los datos se puede utilizar una **expresión regular** que consiste en un string con el que se van a comparar los elementos de la página web. \n",
    "Ej de expresión regular `(http[s]?://[^\"]+)\"`\n",
    "\n",
    "Otra opción es utilizar la librería BeautifulSoup, muy utilizada para limpiar datos de páginas web. \n",
    "**Beautiful Soup**: Beautiful Soup es una biblioteca de Python para extraer datos de documentos HTML y XML. Esta biblioteca crea un árbol con todos los elementos del documento y puede ser utilizado para extraer información. Por lo tanto, esta biblioteca es útil para realizar web scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_general = response_general.text\n",
    "soup = BeautifulSoup(data_general, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La librería genera un objeto tipo `BeautifulSoup`, similar a como se genera un objeto tipo `response`. Sobre el objeto `BeautifulSoup` al igual que con el `response` podemos hacer operaciones. \n",
    "\n",
    "Nos interesan solo las url que tienen la estructura de la versión estenográfica o contienen esta palabra clave dentro de la url. `https://presidente.gob.mx/22-01-24-version-estenografica.../`\n",
    "\n",
    "Para encontrar todos los url necesitamos iterar sobre cada elemento del objeto soup (que contiene las url). Luego comparamos para que contengan las palabras clave. Para hacer esto podemos hacer uso de una list comprenhension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontrar todos los enlaces\n",
    "urls_1 = [link.get('href') for link in soup.find_all('a') if link.get('href') \n",
    "          and 'version-estenografica-de-la' in link.get('href')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estructura Básica de una Comprensión de Lista\n",
    "\n",
    "Una comprensión de lista en Python tiene la siguiente forma básica:\n",
    "\n",
    "```python\n",
    "[expresión for elemento in iterable if condición]\n",
    "```\n",
    "\n",
    "- **`expresión`**: Es lo que cada elemento de la lista resultante será. Puede ser el mismo elemento del iterable o alguna transformación del mismo.\n",
    "- **`for elemento in iterable`**: Es un bucle `for` que recorre cada elemento en un iterable (como una lista, un rango, etc.).\n",
    "- **`if condición`**: Es una condición opcional. Si se incluye, solo los elementos del iterable que cumplan con esta condición serán considerados.\n",
    "\n",
    "### Ejemplo específico\n",
    "\n",
    "- **`for link in soup.find_all('a')`**: Itera sobre cada elemento `link` (etiqueta `<a>`) en el objeto `soup`.\n",
    "\n",
    "- **`link.get('href')`**: Obtiene el valor del atributo `href` de cada etiqueta `<a>`.\n",
    "\n",
    "- **`if link.get('href')`**: Verifica si la etiqueta `<a>` tiene un atributo `href` no nulo.\n",
    "\n",
    "- **`'version-estenografica-de-la' in link.get('href')`**: Verifica si el texto `'version-estenografica-de-la'` está presente en el valor de `href`.\n",
    "\n",
    "Ahora necesitamos hacer esto para todas las páginas. Para esto utilizamos un ciclo for. Además agregamos una barra de progreso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando páginas: 100%|██████████| 135/135 [05:13<00:00,  2.32s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Iteramos sobre todas las páginas\n",
    "NPaginas = 135\n",
    "\n",
    "url_general = 'https://presidente.gob.mx/secciones/version-estenografica/page/'\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.106 Safari/537.36'\n",
    "}\n",
    "\n",
    "urls_conferencia = [] # Inicializamos la lista\n",
    "\n",
    "for i in tqdm(range(NPaginas), desc=\"Procesando páginas\"):\n",
    "    # Obtener cada pagina\n",
    "    response_i = requests.get(url_general + str(i+1) + '/', headers=headers)\n",
    "    \n",
    "    # Filtrar paginas web\n",
    "    data_i = response_i.text\n",
    "    soup_i = BeautifulSoup(data_i, 'html.parser')\n",
    "\n",
    "    # Guardar urls en lista\n",
    "    urls_conferencia += [link.get('href') for link in soup_i.find_all('a') if link.get('href') \n",
    "          and 'version-estenografica-de-la' in link.get('href')]\n",
    "    \n",
    "    time.sleep(1)  # Pausa de 1 segundo para no sobrecargar el servidor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se han guardado 2440 URLs en urls_conferencia.txt\n"
     ]
    }
   ],
   "source": [
    "# Guardado de url\n",
    "nombre_archivo = 'urls_conferencia.txt'\n",
    "\n",
    "# Abrir el archivo en modo de escritura ('w') y guardar cada URL\n",
    "with open(nombre_archivo, 'w', encoding='utf-8') as archivo:\n",
    "    for url in urls_conferencia:\n",
    "        archivo.write(url + '\\n')\n",
    "\n",
    "print(f'Se han guardado {len(urls_conferencia)} URLs en {nombre_archivo}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Información de las páginas\n",
    "\n",
    "En esta sección necesitamos obtener información de las páginas. Vamos a filtar el html de cada página utilizando la librería beautifulSoup.\n",
    "\n",
    "Es necesario guardar toda las las páginas contenidas en el arreglo `urls_conferencia`. Previo a esto, se realiza la limpieza para una sola de las páginas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraper al primer elemento de la lista\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.106 Safari/537.36'\n",
    "}\n",
    "response_conferencia = requests.get(urls_conferencia[2400], headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Se edita el contenido html de\n",
    "data = response_conferencia.text\n",
    "soup = BeautifulSoup(data, 'html.parser')\n",
    "\n",
    "# Extraer el texto\n",
    "texto = soup.get_text()\n",
    "\n",
    "# Reemplazar dos o más saltos de línea consecutivos con un solo salto de línea\n",
    "texto_limpio = re.sub(r'\\n{2,}', '\\n', texto)\n",
    "\n",
    "# Se guarda el contenido de la página en un archivo.txt\n",
    "with open('Conferencia_1.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(texto_limpio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizando el archivo que se obtiene (y otros mas) nos damos cuenta de que la conferencia comienza siempre con el presidente al habla y termina después de tres guiones (---). Podemos filtrar aún mas los datos para obtener únicamente el contenido de la rueda de prensa. El comienzo lo filtramos con la etiqueta `Search`.\n",
    "\n",
    "Para el final de cada archivo es mas complicado, en la mayoria tenemos los tres guiones, pero en los archivos el inicio utilizan otra nomenclatura. Vamos a tratar de incluir la mayoría de nomenclaturas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir el texto en el punto donde aparece search (y otros elementos) y tomamos lo siguiente\n",
    "texto_limpio = texto_limpio.split('Search')[1]\n",
    "texto_limpio = texto_limpio.split('---')[0]\n",
    "texto_limpio = texto_limpio.split('+++++')[0]\n",
    "texto_limpio = texto_limpio.split('– – – 0 – – –')[0]\n",
    "\n",
    "with open('Conferencia_1.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(texto_limpio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya que editamos y sabemos como se van a guardar los archivos, hacemos el ciclo completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando páginas: 2142it [36:30,  1.02s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# La carpeta 'MorningData' existe\n",
    "if not os.path.exists('MorningData'):\n",
    "    os.makedirs('MorningData')\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.106 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Función para leer URLs desde un archivo\n",
    "def leer_urls(archivo):\n",
    "    with open(archivo, 'r', encoding='utf-8') as f:\n",
    "        return f.read().splitlines()\n",
    "\n",
    "# Función para guardar la lista actualizada de URLs\n",
    "def guardar_urls(archivo, urls):\n",
    "    with open(archivo, 'w', encoding='utf-8') as f:\n",
    "        for url in urls:\n",
    "            f.write(url + '\\n')\n",
    "\n",
    "try:\n",
    "\n",
    "    # Leer las URLs desde el archivo\n",
    "    urls_conferencia = leer_urls('urls_conferencia.txt')\n",
    "    urls_procesadas = []\n",
    "\n",
    "    for i, url in tqdm(enumerate(urls_conferencia), desc=\"Procesando páginas\"):\n",
    "        # Hacer la request\n",
    "        response_i = requests.get(url, headers=headers)\n",
    "        data_i = response_i.text\n",
    "\n",
    "        # Extraer y limpiar el texto\n",
    "        soup_i = BeautifulSoup(data_i, 'html.parser')\n",
    "        texto_i = soup_i.get_text()\n",
    "        texto_limpio_i = re.sub(r'\\n{2,}', '\\n', texto_i)\n",
    "\n",
    "        texto_limpio_i = texto_limpio_i.split('Search')[1]\n",
    "        texto_limpio_i = texto_limpio_i.split('---')[0]\n",
    "        texto_limpio_i = texto_limpio_i.split('+++++')[0]\n",
    "        texto_limpio_i = texto_limpio_i.split('– – – 0 – – –')[0]\n",
    "\n",
    "        # Guardar los elementos\n",
    "        title_i = texto_limpio_i[1:9]\n",
    "\n",
    "        # Guardar en un archivo de texto\n",
    "        with open(f'MorningData/' + title_i + '.txt', 'w', encoding='utf-8') as file:\n",
    "            file.write(texto_limpio_i)\n",
    "        \n",
    "        # Marcar la URL como procesada\n",
    "        urls_procesadas.append(url)\n",
    "\n",
    "finally:\n",
    "    # Eliminar URLs procesadas de la lista original y guardar la lista actualizada\n",
    "    urls_restantes = [url for url in urls_conferencia if url not in urls_procesadas]\n",
    "    guardar_urls('urls_conferencia.txt', urls_restantes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PLN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
