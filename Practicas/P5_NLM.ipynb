{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 5. Neural Language Models\n",
    "\n",
    "Guillermo Segura Gómez\n",
    "\n",
    "## Entrenamiento de un modelo de lenguaje neuronal\n",
    "\n",
    "Un modelo neuronal es una estructura computacional que se inspira en la forma en que las neuronas en el cerebro humano procesan información. En su forma más básica, un modelo neuronal consiste en unidades llamadas \"neuronas artificiales\" o \"nodos\" que reciben entradas, las procesan y producen salidas. Estos modelos son capaces de aprender patrones complejos a partir de los datos gracias a su capacidad para ajustar los \"pesos\" de las conexiones entre las neuronas durante el proceso de entrenamiento.\n",
    "\n",
    "### Perceptrón: El modelo más sencillo\n",
    "El perceptrón es uno de los modelos neuronales más simples y sirve como base para arquitecturas más complejas. Fue desarrollado por Frank Rosenblatt en 1957. Un perceptrón toma varias entradas binarias o reales $(x_1, x_2, ..., x_n)$, las multiplica por pesos $(w_1, w_2, ..., w_n)$, y suma estos productos. A esta suma se le puede añadir un término de sesgo $(b)$ o *bias*. Luego, esta suma ponderada se pasa por una función de activación, que en el caso más simple podría ser una función escalón que devuelve 1 si la suma ponderada es mayor que un cierto umbral y -1 (o 0) en caso contrario.\n",
    "\n",
    "La fórmula de un perceptrón para una entrada y una salida sería algo así:\n",
    "\n",
    "$$ \\text{Salida} = f\\left(\\sum_{i=1}^{n} w_i \\cdot x_i + b\\right) $$\n",
    "\n",
    "Donde $f$ es la función de activación.\n",
    "\n",
    "El perceptrón puede aprender a separar dos clases linealmente separables ajustando sus pesos durante el entrenamiento, usualmente mediante el algoritmo de descenso del gradiente.\n",
    "\n",
    "### Modelo Neuronal propuesto por Bengio\n",
    "Yoshua Bengio es uno de los pioneros en el campo del aprendizaje profundo, y ha contribuido al desarrollo de varios tipos de redes neuronales, incluyendo las Redes Neuronales Profundas (Deep Neural Networks, DNN), las Redes Neuronales Recurrentes (Recurrent Neural Networks, RNN) para procesamiento de secuencias, y más recientemente, las Redes Generativas Adversarias (Generative Adversarial Networks, GAN) y las Transformadores (Transformers) para tareas de NLP y generación de imágenes.\n",
    "\n",
    "Una arquitectura neuronal profundamente influyente propuesta por Bengio y otros es la **Red Neuronal Profunda (DNN)**, que consiste en múltiples capas de neuronas. Cada capa toma las salidas de la capa anterior como entradas y produce nuevas salidas, que luego se pasan a la siguiente capa. Este proceso continúa hasta que se alcanza la capa de salida. Las capas entre la entrada y la salida se llaman \"capas ocultas\", y cada una puede aprender diferentes representaciones de los datos de entrada. La presencia de múltiples capas ocultas permite que la red aprenda características cada vez más abstractas y complejas a medida que los datos avanzan a través de la red, lo que ha permitido avances significativos en campos como la visión por computadora y el procesamiento del lenguaje natural.\n",
    "\n",
    "El entrenamiento de estos modelos se realiza generalmente a través de un algoritmo conocido como **propagación hacia atrás** (backpropagation), combinado con algún tipo de optimización, como el descenso de gradiente estocástico, para ajustar los pesos de la red y minimizar una función de pérdida que mide el error de la red.\n",
    "\n",
    "Cada uno de estos modelos tiene su propia complejidad y aplicabilidad dependiendo de la tarea específica, y el campo del aprendizaje profundo sigue evolucionando con nuevas arquitecturas y técnicas propuestas regularmente.\n",
    "\n",
    "Vamos a tratar de replicar en esta práctica el modelo propuesto por Bengio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General libraries\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "from typing import Tuple\n",
    "from argparse import Namespace\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Librerías para preprocesamiento, que son las que ya hemos estado utilizando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import FreqDist\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos la Liberia `Pytorch`\n",
    "\n",
    "PyTorch es una biblioteca de código abierto de aprendizaje automático (Machine Learning) para Python, utilizada tanto en investigación como en producción. Fue desarrollada inicialmente por el laboratorio de investigación de inteligencia artificial de Facebook (FAIR), y desde entonces ha ganado una amplia popularidad en la comunidad de investigación y desarrollo en inteligencia artificial (IA).\n",
    "\n",
    "PyTorch se destaca por varias razones:\n",
    "\n",
    "1. **Flexibilidad y dinamismo**: Utiliza lo que se llama \"gráficos de cálculo dinámico\" (también conocidos como \"definidos por ejecución\"), lo que significa que el gráfico de cálculo se construye sobre la marcha en cada iteración. Esto proporciona una gran flexibilidad y facilita la experimentación y el prototipado, especialmente para modelos complejos y dinámicos.\n",
    "\n",
    "2. **Facilidad de uso**: Su interfaz es intuitiva, especialmente para quienes están familiarizados con Python y las bibliotecas de cálculo numérico como NumPy. PyTorch también proporciona una gran cantidad de utilidades y abstracciones de alto nivel que simplifican tareas comunes de aprendizaje automático.\n",
    "\n",
    "3. **Soporte para redes neuronales profundas (Deep Learning)**: PyTorch incluye un módulo `torch.nn` dedicado a las redes neuronales, que ofrece una amplia variedad de capas y funciones de activación predefinidas, facilitando la construcción y entrenamiento de modelos complejos.\n",
    "\n",
    "4. **Autodiferenciación**: A través de su módulo `torch.autograd`, PyTorch proporciona autodiferenciación, lo que significa que puede calcular automáticamente gradientes o derivadas de operaciones con respecto a las entradas, lo cual es esencial para el algoritmo de retropropagación utilizado en el entrenamiento de redes neuronales.\n",
    "\n",
    "5. **Soporte para GPU**: PyTorch ofrece un soporte excelente para la computación en GPU (Unidad de Procesamiento Gráfico), lo que permite acelerar significativamente las operaciones matemáticas complejas y el entrenamiento de modelos, especialmente útil para grandes conjuntos de datos y modelos profundos.\n",
    "\n",
    "6. **Comunidad y ecosistema**: PyTorch goza de una comunidad activa y en crecimiento, que contribuye constantemente con una amplia gama de herramientas y bibliotecas complementarias, tutoriales, y soporte. Esto hace que sea más fácil encontrar recursos y solucionar problemas.\n",
    "\n",
    "PyTorch se ha convertido en una de las herramientas más importantes para los investigadores y profesionales en el campo del aprendizaje automático y el deep learning, debido a su flexibilidad, facilidad de uso, y robustas capacidades para el desarrollo y entrenamiento de modelos de IA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scikit-learn\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El propósito general de las siguientes líneas es asegurar la reproducibilidad de los experimentos que vamos hacer en ciencia de datos y aprendizaje automático. Al fijar las semillas de los generadores de números aleatorios en las distintas bibliotecas utilizadas, se asegura que el código producirá los mismos resultados cada vez que se ejecute, lo cual es crucial para la depuración, la comparación de modelos y la publicación de resultados científicos reproducibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1111\n",
    "random.seed(seed) #python seed\n",
    "np.random.seed(seed) #numpy seed\n",
    "torch.manual_seed(seed) #torch seed\n",
    "torch.backends.cudnn.benchmark = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `seed = 1111`: Esta línea establece un número (en este caso, 1111) como valor de la variable `seed`. Esta variable se utilizará para inicializar los generadores de números aleatorios en diferentes librerías, con el fin de hacer que los resultados sean reproducibles.\n",
    "\n",
    "2. `random.seed(seed)`: Esta línea inicializa el generador de números aleatorios de Python con la semilla definida en la variable `seed`. Esto significa que las funciones del módulo `random` de Python que generan números aleatorios producirán la misma secuencia de números cuando se ejecuten con la misma semilla.\n",
    "\n",
    "3. `np.random.seed(seed)`: De manera similar a la anterior, esta línea inicializa el generador de números aleatorios de la biblioteca NumPy con la misma semilla. NumPy es ampliamente utilizado para cálculos numéricos, y su generador de números aleatorios se utiliza, por ejemplo, para inicializar pesos en redes neuronales o para dividir conjuntos de datos de manera aleatoria durante el entrenamiento.\n",
    "\n",
    "4. `torch.manual_seed(seed)`: Esta línea inicializa el generador de números aleatorios de PyTorch con la misma semilla. PyTorch es una biblioteca para el aprendizaje automático y el deep learning, y utilizar una semilla fija asegura la reproducibilidad de los experimentos, especialmente importante para la inicialización de los pesos de las redes neuronales, entre otros.\n",
    "\n",
    "5. `torch.backends.cudnn.benchmark = False`: Esta línea desactiva el modo \"benchmark\" de la biblioteca cuDNN utilizada por PyTorch para operaciones de deep learning en GPUs NVIDIA. El modo \"benchmark\" busca la configuración óptima para las operaciones de convolución en tu hardware específico, lo cual puede mejorar el rendimiento. Sin embargo, esto puede llevar a resultados no deterministas debido a la selección variable de algoritmos de convolución. Al establecer `benchmark` como `False`, se prioriza la reproducibilidad sobre el rendimiento potencialmente óptimo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializamos los datos en un data frame de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_text = \"/Users/guillermo_sego/Desktop/Segundo Semestre/PLN/Data/MexData/mex20_train.txt\"\n",
    "path_text_val = \"/Users/guillermo_sego/Desktop/Segundo Semestre/PLN/Data/MexData/mex20_val.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(path_text, sep = '\\r\\n', engine = 'python', header = None).loc[:, 0].values.tolist()\n",
    "X_val = pd.read_csv(path_text_val, sep = '\\r\\n', engine = 'python', header = None).loc[:, 0].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una clase que reciba el tamaño del modleo, el número de tokens máximo y un tokenizador.\n",
    "\n",
    "La clase `NgramData` definida en el siguiente código está diseñada para procesar datos textuales para tareas de modelado de lenguaje o similares, utilizando un enfoque basado en n-gramas. Un n-grama es una secuencia contigua de \\(N$ ítems (en este caso, palabras) de un texto dado. El propósito de esta clase incluye varias funciones importantes para el preprocesamiento de datos textuales y su preparación para algoritmos de aprendizaje automático, especialmente en el contexto del procesamiento del lenguaje natural (NLP). A continuación, desgloso las funcionalidades principales de la clase:\n",
    "\n",
    "1. **Inicialización (`__init__`)**: Al crear una instancia de `NgramData`, se configuran varios parámetros importantes, como el tamaño de los n-gramas (`N`), el tamaño máximo del vocabulario (`vocab_max`), el tokenizador personalizado (`tokenizer`), y un modelo de embeddings (`embeddings_model`). También se definen algunos tokens especiales como desconocido (`UNK`), inicio de secuencia (`SOS`), y fin de secuencia (`EOS`), y un conjunto de signos de puntuación a ignorar.\n",
    "\n",
    "2. **Tokenización (`default_tokenizer`)**: La función `default_tokenizer` proporciona una forma simple de dividir un documento en tokens (palabras) usando espacios en blanco. Puede ser reemplazado por un tokenizador personalizado si se proporciona uno durante la inicialización.\n",
    "\n",
    "3. **Limpieza y preparación del vocabulario (`remove_word`, `get_vocab`, `sortFreqDict`)**: Estas funciones ayudan a limpiar y preparar el vocabulario. `remove_word` decide si una palabra debe ser eliminada basándose en si es un signo de puntuación o un dígito. `get_vocab` construye el vocabulario a partir de un corpus dado, excluyendo las palabras no deseadas y limitando el tamaño del vocabulario. `sortFreqDict` ordena las palabras en el vocabulario por frecuencia.\n",
    "\n",
    "4. **Ajuste del modelo (`fit`)**: Esta función ajusta el modelo al corpus dado, construyendo el vocabulario, y luego crea mapeos de palabras a identificadores únicos (`w2id`) y viceversa (`id2w`). Si se proporciona un modelo de embeddings, también se construye una matriz de embeddings para las palabras en el vocabulario.\n",
    "\n",
    "5. **Transformación de los datos (`transform`)**: `transform` convierte un corpus de documentos en un formato adecuado para el modelado, específicamente en pares de entradas y salidas donde cada entrada es una secuencia de \\(N-1$ palabras (en forma de identificadores) y la salida es la siguiente palabra en la secuencia.\n",
    "\n",
    "6. **Generación de n-gramas (`get_ngram_doc`)**: Esta función convierte un documento en una lista de n-gramas, asegurándose de que las palabras desconocidas se reemplacen por el token `UNK` y añadiendo tokens especiales al principio y al final del documento para indicar el inicio y el fin de la secuencia.\n",
    "\n",
    "7. **Reemplazo de palabras desconocidas (`replace_unk`)**: Reemplaza las palabras que no están en el vocabulario con el token de palabra desconocida (`UNK`).\n",
    "\n",
    "El propósito general de `NgramData` es facilitar el preprocesamiento de textos para tareas de NLP, especialmente para modelar la secuencia de palabras en un texto utilizando el enfoque de n-gramas, lo que es útil en aplicaciones como la predicción de la siguiente palabra, análisis de sentimientos, clasificación de texto, y más."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "args.N = 4\n",
    "\n",
    "class NgramData():\n",
    "    # Inicialización de la clase NgramData\n",
    "    def __init__(self, N:int, vocab_max: int = 5000, tokenizer = None, embeddings_model = None):\n",
    "        # Asignación de un tokenizador personalizado o el predeterminado si no se proporciona\n",
    "        self.tokenizer = tokenizer if tokenizer else self.default_tokenizer\n",
    "        # Definición de signos de puntuación a ignorar\n",
    "        self.punct = set(['.', ',', ';', ':', '-', '_', '!', '¡', '?', '¿', '^', '<url>', '*', '@usuario'])\n",
    "        # Número de palabras en cada n-grama\n",
    "        self.N = N\n",
    "        # Tamaño máximo del vocabulario\n",
    "        self.vocab_max = vocab_max\n",
    "        # Token para palabras desconocidas\n",
    "        self.UNK = '<unk>'\n",
    "        # Token para indicar el inicio de una secuencia\n",
    "        self.SOS = '<s>'\n",
    "        # Token para indicar el final de una secuencia\n",
    "        self.EOS = '</s>'\n",
    "        # Modelo de embeddings (opcional)\n",
    "        self.embeddings_model = embeddings_model\n",
    "\n",
    "    # Función para obtener el tamaño del vocabulario\n",
    "    def get_vocab_size(self) -> int:\n",
    "        return len(self.vocab)\n",
    "\n",
    "    # Tokenizador predeterminado que divide el texto por espacios\n",
    "    def default_tokenizer(self, doc: str) -> list:\n",
    "        return doc.split(\" \")\n",
    "    \n",
    "    # Función para determinar si una palabra debe eliminarse (basada en puntuación o si es un número)\n",
    "    def remove_word(self, word: str) -> bool:\n",
    "        word = word.lower()\n",
    "        is_punct = word in self.punct\n",
    "        is_digit = word.isnumeric()\n",
    "        return is_punct or is_digit\n",
    "    \n",
    "    # Construye el vocabulario a partir de un corpus, excluyendo palabras según `remove_word` y limitando el tamaño\n",
    "    def get_vocab(self, corpus: list) -> set:\n",
    "        # Construcción de la distribución de frecuencia de palabras\n",
    "        freq_dist = FreqDist([w.lower() for sentence in corpus for w in self.tokenizer(sentence) if not self.remove_word(w)])\n",
    "        # Ordenar palabras por frecuencia y limitar el tamaño del vocabulario\n",
    "        sorted_words = self.sortFreqDict(freq_dist)[:self.vocab_max-3]\n",
    "        return set(sorted_words)\n",
    "    \n",
    "    # Ordena el diccionario de frecuencia de palabras\n",
    "    def sortFreqDict(self, freq_dist) -> list:\n",
    "        freq_dict = dict(freq_dist)\n",
    "        return sorted(freq_dict, key=freq_dict.get, reverse=True)\n",
    "    \n",
    "    # Ajusta el modelo al corpus, construyendo vocabulario, mapeos de palabras a ID y opcionalmente una matriz de embeddings\n",
    "    def fit(self, corpus: list) -> None:\n",
    "        self.vocab = self.get_vocab(corpus)\n",
    "        # Agregar tokens especiales al vocabulario\n",
    "        self.vocab.update({self.UNK, self.SOS, self.EOS})\n",
    "        \n",
    "        self.w2id = {}\n",
    "        self.id2w = {}\n",
    "        \n",
    "        # Opcional: inicialización de la matriz de embeddings\n",
    "        if self.embeddings_model is not None:\n",
    "            self.embedding_matrix = np.empty([len(self.vocab), self.embeddings_model.vector_size])\n",
    "            \n",
    "        id = 0\n",
    "        for doc in corpus:\n",
    "            for word in self.tokenizer(doc):\n",
    "                word_ = word.lower()\n",
    "                if word_ in self.vocab and word_ not in self.w2id:\n",
    "                    self.w2id[word_] = id\n",
    "                    self.id2w[id] = word_\n",
    "                    # Si se proporciona un modelo de embeddings, asignar vector o vector aleatorio si la palabra no está en el modelo\n",
    "                    if self.embeddings_model is not None:\n",
    "                        self.embedding_matrix[id] = self.embeddings_model[word_] if word_ in self.embeddings_model else np.random.rand(self.embeddings_model.vector_size)\n",
    "                    id += 1\n",
    "        \n",
    "        # Actualizar mapeos con tokens especiales\n",
    "        self.w2id.update({self.UNK: id, self.SOS: id+1, self.EOS: id+2})\n",
    "        self.id2w.update({id: self.UNK, id+1: self.SOS, id+2: self.EOS})\n",
    "    \n",
    "    # Transforma el corpus en secuencias de entrada (X_ngrams) y etiquetas objetivo (y) para el modelado\n",
    "    def transform(self, corpus: list) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        X_ngrams = []  # Lista para almacenar secuencias de entrada\n",
    "        y = []  # Lista para almacenar las etiquetas objetivo\n",
    "\n",
    "        # Iterar sobre cada documento en el corpus\n",
    "        for doc in corpus:\n",
    "            # Obtener n-gramas para el documento actual\n",
    "            doc_ngram = self.get_ngram_doc(doc)\n",
    "            # Iterar sobre cada ventana de palabras (n-grama) en el documento\n",
    "            for words_window in doc_ngram:\n",
    "                # Convertir palabras en IDs usando el mapeo palabra-ID\n",
    "                words_window_ids = [self.w2id[w] for w in words_window]\n",
    "                # Las primeras N-1 palabras son la entrada, la última palabra es la etiqueta objetivo\n",
    "                X_ngrams.append(list(words_window_ids[:-1]))\n",
    "                y.append(words_window_ids[-1])\n",
    "        # Convertir las listas en arrays de NumPy para su uso en modelos de aprendizaje automático\n",
    "        return np.array(X_ngrams), np.array(y)\n",
    "    \n",
    "    # Genera n-gramas a partir de un documento, preparándolo para la transformación\n",
    "    def get_ngram_doc(self, doc: str) -> list:\n",
    "        # Tokenizar el documento\n",
    "        doc_tokens = self.tokenizer(doc)\n",
    "        # Reemplazar tokens desconocidos por el token <unk>\n",
    "        doc_tokens = self.replace_unk(doc_tokens)\n",
    "        # Convertir todos los tokens a minúsculas\n",
    "        doc_tokens = [w.lower() for w in doc_tokens]\n",
    "        # Añadir tokens de inicio (<s>) y fin (</s>) al documento\n",
    "        doc_tokens = [self.SOS]*(self.N-1) + doc_tokens + [self.EOS]\n",
    "        # Generar y retornar n-gramas del documento procesado\n",
    "        return list(ngrams(doc_tokens, self.N))\n",
    "    \n",
    "    # Reemplaza tokens desconocidos en una lista de tokens por el token <unk>\n",
    "    def replace_unk(self, doc_tokens: list) -> list:\n",
    "        # Iterar sobre cada token en la lista de tokens\n",
    "        for i, token in enumerate(doc_tokens):\n",
    "            # Si el token no está en el vocabulario, reemplazarlo por <unk>\n",
    "            if token.lower() not in self.vocab:\n",
    "                doc_tokens[i] = self.UNK\n",
    "        # Retornar la lista de tokens con los desconocidos reemplazados\n",
    "        return doc_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = TweetTokenizer() # Inicializamos el tokenizador de tweets\n",
    "\n",
    "# Creamos un objeto con la clase que definimos\n",
    "ngram_data = NgramData(args.N, 5000, tk.tokenize)\n",
    "ngram_data.fit(X_train) # Construye el vocabulario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos que funcione el objeto que acabamos de crear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 5000\n"
     ]
    }
   ],
   "source": [
    "print(f'Vocab size: {ngram_data.get_vocab_size()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora utilizamos los métodos de la clase que creamos. El método `transform` transforma los datos textuales proporcionados en un formato que es adecuado para el entrenamiento o la evaluación en modelos de aprendizaje automático.\n",
    "- El método `transform` procesa este conjunto de datos y lo convierte en dos nuevas estructuras: `X_ngram_train` y `y_ngram_train`.\n",
    "\n",
    "- `X_ngram_train` contiene las secuencias de entrada transformadas, donde cada entrada es una secuencia de $N-1$ palabras (o tokens) representadas por sus identificadores numéricos. Estas secuencias se utilizan como entradas para tu modelo.\n",
    "- `y_ngram_train` contiene las etiquetas objetivo asociadas a cada secuencia de entrada en `X_ngram_train`. En el contexto de n-gramas, cada etiqueta es la palabra (o token) que sigue a la secuencia de $N-1$ palabras en el texto original, también representada por su identificador numérico. Estas etiquetas se utilizan como objetivos de predicción para tu modelo.\n",
    "\n",
    "Luego lo repetimos para el conjunto de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ngram_train, y_ngram_train = ngram_data.transform(X_train)\n",
    "X_ngram_val, y_ngram_val = ngram_data.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4998, 4998, 4998],\n",
       "       [4998, 4998, 4997],\n",
       "       [4998, 4997, 4997],\n",
       "       ...,\n",
       "       [4997,  947,   32],\n",
       "       [ 947,   32, 2520],\n",
       "       [  32, 2520, 4997]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ngram_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya tenemos las listas de entrenamiento para nuestro modelo. Ahora si queremos ver las palabras en lugar de sus tokens, construimos el siguiente vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " 'q',\n",
       " 'se',\n",
       " 'puede',\n",
       " 'esperar',\n",
       " 'del',\n",
       " 'maricon',\n",
       " 'de',\n",
       " 'closet',\n",
       " 'de',\n",
       " 'la',\n",
       " 'yañez',\n",
       " 'aun',\n",
       " 'recuerdo',\n",
       " 'esa',\n",
       " 'ves',\n",
       " 'q',\n",
       " 'lo',\n",
       " 'vi',\n",
       " 'en']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ngram_data.id2w[w] for w in y_ngram_train[:22]] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora necesitamos pasar estos datos en tensores de PyTorch y, organizarlos en un DataLoader de PyTorch para facilitar el entrenamiento y la evaluación de modelos de aprendizaje profundo. PyTorch utiliza tensores, que son una generalización de matrices y vectores, como su estructura de datos principal para realizar operaciones de aprendizaje automático, especialmente en redes neuronales.\n",
    "\n",
    "Para el entrenamiento, es útil organizar los datos en lotes (batches), y PyTorch ofrece una herramienta llamada **DataLoader** para esto. Un DataLoader puede cargar los datos en lotes de un tamaño especificado y puede mezclar los datos para reducir el riesgo de sobreajuste. Para usar un DataLoader, primero debes organizar tus tensores en un Dataset, que es otra abstracción de PyTorch que facilita trabajar con conjuntos de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch size in args\n",
    "args.batch_size = 64\n",
    "# Num workers\n",
    "args.num_workers = 2\n",
    "\n",
    "# Convertimos los datos a tensores de pytorch\n",
    "train_dataset = TensorDataset(torch.tensor(X_ngram_train, dtype = torch.int64),\n",
    "                              torch.tensor(y_ngram_train, dtype = torch.int64))\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size = args.batch_size,\n",
    "                          num_workers=args.num_workers,\n",
    "                          shuffle = True)\n",
    "\n",
    "val_dataset = TensorDataset(torch.tensor(X_ngram_val, dtype = torch.int64),\n",
    "                            torch.tensor(y_ngram_val, dtype = torch.int64))\n",
    "\n",
    "val_loader = DataLoader(train_dataset,\n",
    "                          batch_size = args.batch_size,\n",
    "                          num_workers=args.num_workers,\n",
    "                          shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Qué es un Batch?\n",
    "\n",
    "Un \"batch\" o lote es simplemente un subconjunto de tu conjunto de datos de entrenamiento que se utiliza para entrenar el modelo en una sola iteración del algoritmo de optimización, como el descenso de gradiente. En lugar de pasar todo el conjunto de datos a través de la red de una vez (lo que se conoce como entrenamiento por lotes o \"batch training\") o pasar una sola muestra a la vez (entrenamiento estocástico), pasas un número fijo de muestras. Este número es el tamaño del lote, también conocido como \"batch size\".\n",
    "\n",
    "### ¿Por qué usar Batches?\n",
    "\n",
    "1. **Eficiencia computacional**: Utilizar lotes permite que el proceso de entrenamiento sea más eficiente desde el punto de vista computacional. Las operaciones matriciales en lotes pueden aprovechar mejor las optimizaciones de hardware, como las GPUs, en comparación con el procesamiento de una sola muestra a la vez.\n",
    "\n",
    "2. **Estabilidad y calidad del entrenamiento**: El entrenamiento con lotes puede ayudar a estabilizar el aprendizaje. Al promediar el gradiente sobre varias muestras, se reduce la varianza en la estimación del gradiente, lo que puede llevar a una convergencia más suave durante el entrenamiento.\n",
    "\n",
    "3. **Uso de la memoria**: Cargar el conjunto de datos completo en la memoria a la vez puede no ser factible, especialmente para conjuntos de datos grandes. El uso de lotes permite que el modelo se entrene con subconjuntos del conjunto de datos, lo que es más manejable desde el punto de vista del uso de la memoria.\n",
    "\n",
    "### Trade-offs del Tamaño del Batch\n",
    "\n",
    "El tamaño del lote es un hiperparámetro que puedes ajustar, y su elección puede afectar la calidad del modelo y el tiempo de entrenamiento:\n",
    "\n",
    "- **Batch sizes grandes**: Pueden llevar a una estimación más precisa del gradiente, pero pueden requerir más memoria y pueden llevar a un entrenamiento más lento por iteración. Además, batch sizes grandes a veces pueden llevar a un entrenamiento que converge a mínimos menos óptimos debido a la suavización del paisaje del error.\n",
    "\n",
    "- **Batch sizes pequeños**: Pueden aumentar la varianza en la estimación del gradiente, lo que puede ayudar al modelo a escapar de mínimos locales, potencialmente encontrando mejores mínimos globales. Sin embargo, esto también puede hacer que la trayectoria de entrenamiento sea más ruidosa y posiblemente más lenta en converger. Desde el punto de vista computacional, los lotes más pequeños son menos eficientes.\n",
    "\n",
    "### Iteración, Época y Batch\n",
    "\n",
    "Para aclarar más:\n",
    "\n",
    "- **Iteración**: Cada paso del algoritmo de entrenamiento, en el que el modelo se actualiza, es una iteración. En cada iteración, se utiliza un lote de datos.\n",
    "\n",
    "- **Época**: Una época completa ocurre cuando cada muestra en el conjunto de datos de entrenamiento ha sido utilizada una vez para actualizar el modelo. Por lo tanto, el número de iteraciones necesarias para completar una época depende del tamaño del conjunto de datos y del tamaño del lote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([64, 3])\n",
      "y shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(f'X shape: {batch[0].shape}')\n",
    "print(f'y shape: {batch[1].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parámetros que definen aspectos estructurales y de regularización de un modelo de red neuronal, que influyen en cómo el modelo aprenderá de los datos y generalizará a nuevos ejemplos no vistos. Utilizamos los parámetros del modelo propuesto por Bengio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab size\n",
    "args.vocab_size = ngram_data.get_vocab_size()\n",
    "\n",
    "# Dimension of word embeddings\n",
    "args.d = 50\n",
    "\n",
    "# Dimension for hidden layer\n",
    "args.d_h = 100\n",
    "\n",
    "# Dropout\n",
    "args.dropout = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la clase `NeuralLM`, que representa un Modelo de Lenguaje Neuronal, utilizando PyTorch. Esta clase hereda de `nn.Module`, que es la clase base para todos los módulos de red neuronal en PyTorch, proporcionando funcionalidades útiles como el seguimiento de parámetros, la GPU/CPU transferencia, etc. Veamos los componentes y la funcionalidad de la clase `NeuralLM`:\n",
    "\n",
    "#### Inicialización (`__init__`)\n",
    "En el método `__init__`, se inicializan los componentes del modelo:\n",
    "\n",
    "1. **`self.window_size`**: Define el tamaño de la ventana de entrada para el modelo, basado en el parámetro `N` menos 1. Esto se debe a que, en un modelo de lenguaje basado en n-gramas, si `N` es el tamaño del n-grama, entonces `N-1` palabras se utilizan para predecir la `N`-ésima palabra.\n",
    "\n",
    "2. **`self.embedding_dim`**: Es la dimensión de los embeddings de palabras, definida por el parámetro `args.d`. Los embeddings de palabras transforman índices de palabras en vectores densos que capturan información semántica.\n",
    "\n",
    "3. **`self.emb`**: Una capa de embedding que convierte índices de palabras en embeddings. Utiliza el tamaño del vocabulario (`args.vocab_size`) y la dimensión de los embeddings (`args.d`) definidos anteriormente.\n",
    "\n",
    "4. **`self.fc1`**: La primera capa lineal (o completamente conectada) que transforma la entrada aplanada de la capa de embedding en una representación de dimensión intermedia (`args.d_h`). La entrada de esta capa es el tamaño de la ventana de entrada multiplicado por la dimensión del embedding.\n",
    "\n",
    "5. **`self.drop1`**: Una capa de Dropout que \"apaga\" aleatoriamente un porcentaje (`args.dropout`) de las activaciones en la capa anterior para prevenir el sobreajuste.\n",
    "\n",
    "6. **`self.fc2`**: La segunda capa lineal que transforma la salida de la capa oculta en un vector del tamaño del vocabulario. Esta capa produce las puntuaciones (logits) para cada palabra en el vocabulario, que luego pueden ser convertidas en probabilidades mediante una función softmax. No hay sesgo en esta capa (`bias=False`).\n",
    "\n",
    "#### Forward Pass (`forward`)\n",
    "En el método `forward`, se define cómo fluyen los datos a través del modelo:\n",
    "\n",
    "1. **Embedding**: Primero, los índices de palabras en `x` son transformados en embeddings de palabras mediante `self.emb(x)`.\n",
    "\n",
    "2. **Flatten**: Los embeddings son aplanados en un vector único para cada muestra en el lote (`x.view(-1, self.window_size*self.embedding_dim)`). Esto es necesario porque las capas lineales esperan entradas de forma plana.\n",
    "\n",
    "3. **Capa oculta y activación**: La salida aplanada es pasada a través de la primera capa lineal (`self.fc1(x)`) y luego a través de una función de activación ReLU (`F.relu()`), produciendo la representación de la capa oculta `h`.\n",
    "\n",
    "4. **Dropout**: La representación de la capa oculta pasa a través de la capa de Dropout (`self.drop1(h)`), lo que ayuda a prevenir el sobreajuste reduciendo la dependencia en cualquier neurona individual.\n",
    "\n",
    "5. **Salida**: Finalmente, la representación de la capa oculta después del Dropout se pasa a través de la segunda capa lineal (`self.fc2(h)`), generando las puntuaciones (logits) para cada palabra en el vocabulario.\n",
    "\n",
    "Este modelo puede ser entrenado para predecir la siguiente palabra en una secuencia, dado un contexto de `N-1` palabras, lo cual es una tarea común en el modelado del lenguaje. La función de pérdida (como la entropía cruzada) y un optimizador (como el descenso de gradiente estocástico o Adam) se usarían en conjunto con este modelo para entrenarlo en un conjunto de datos específico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLM(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        # Inicializa la clase padre, nn.Module\n",
    "        super(NeuralLM, self).__init__()\n",
    "        \n",
    "        # Tamaño de la ventana de entrada (contexto) para el modelo, basado en N-1\n",
    "        # donde N es el tamaño de los n-gramas considerados.\n",
    "        self.window_size = args.N-1\n",
    "        \n",
    "        # Dimensión de los embeddings de palabras, que transforman índices de palabras\n",
    "        # en vectores densos que capturan información semántica.\n",
    "        self.embedding_dim = args.d\n",
    "        \n",
    "        # Capa de embedding que convierte índices de palabras en vectores densos.\n",
    "        # Utiliza el tamaño del vocabulario y la dimensión de los embeddings especificados.\n",
    "        self.emb = nn.Embedding(args.vocab_size, self.embedding_dim)\n",
    "        \n",
    "        # Primera capa lineal que transforma la entrada aplanada de la capa de embedding\n",
    "        # a una representación de dimensión intermedia especificada por args.d_h.\n",
    "        self.fc1 = nn.Linear(self.embedding_dim * self.window_size, args.d_h)\n",
    "        \n",
    "        # Capa de Dropout que \"apaga\" aleatoriamente un porcentaje de las activaciones\n",
    "        # en la capa anterior para prevenir el sobreajuste.\n",
    "        self.drop1 = nn.Dropout(p=args.dropout)\n",
    "        \n",
    "        # Segunda capa lineal que transforma la salida de la capa oculta en un vector\n",
    "        # del tamaño del vocabulario. Esta capa produce los logits para cada palabra en el vocabulario.\n",
    "        self.fc2 = nn.Linear(args.d_h, args.vocab_size, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Transforma los índices de palabras en x en embeddings de palabras.\n",
    "        x = self.emb(x)\n",
    "        \n",
    "        # Aplana los embeddings en un vector único para cada muestra en el lote,\n",
    "        # preparándolos para la entrada a la capa lineal.\n",
    "        x = x.view(-1, self.window_size * self.embedding_dim)\n",
    "        \n",
    "        # Pasa la entrada aplanada a través de la primera capa lineal y luego\n",
    "        # a través de una función de activación ReLU.\n",
    "        h = F.relu(self.fc1(x))\n",
    "        \n",
    "        # Aplica Dropout a la representación de la capa oculta para mejorar la generalización.\n",
    "        h = self.drop1(h)\n",
    "        \n",
    "        # La salida final se obtiene pasando la representación de la capa oculta después de Dropout\n",
    "        # a través de la segunda capa lineal, generando los logits para cada palabra en el vocabulario.\n",
    "        return self.fc2(h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora necesitamos mas funciones para poder hacer la evaluación del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta función toma los logits (es decir, las salidas no normalizadas de un modelo) \n",
    "# y devuelve las predicciones de clase como índices.\n",
    "def get_preds(raw_logits):\n",
    "    # Calcula las probabilidades aplicando la función softmax a los logits.\n",
    "    # La operación detach() se usa para evitar que se calculen gradientes para estas operaciones,\n",
    "    # ya que solo se necesitan las probabilidades para hacer predicciones.\n",
    "    probs = F.softmax(raw_logits.detach(), dim=1)\n",
    "    \n",
    "    # Encuentra el índice de la mayor probabilidad en cada fila (es decir, para cada ejemplo en el lote),\n",
    "    # que corresponde a la clase predicha. Luego, convierte el tensor a un array de NumPy.\n",
    "    y_pred = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "\n",
    "# Evalúa el modelo en un conjunto de datos proporcionado y devuelve la precisión del modelo.\n",
    "def model_eval(data, model, gpu=False):\n",
    "    # Desactiva el cálculo de gradientes para acelerar las cosas y reducir el uso de memoria\n",
    "    # ya que no se necesita para la evaluación.\n",
    "    with torch.no_grad():\n",
    "        preds, tgts = [], []  # Listas para almacenar predicciones y etiquetas verdaderas\n",
    "        \n",
    "        # Itera sobre los lotes de datos en el DataLoader\n",
    "        for window_words, labels in data:\n",
    "            # Si se utiliza GPU, mueve los datos al dispositivo adecuado\n",
    "            if gpu:\n",
    "                window_words = window_words.cuda()\n",
    "                \n",
    "            # Obtiene los logits del modelo para el lote actual\n",
    "            outputs = model(window_words)\n",
    "            \n",
    "            # Obtiene las predicciones de clase para el lote actual utilizando la función get_preds\n",
    "            y_pred = get_preds(outputs)\n",
    "            \n",
    "            # Extrae las etiquetas verdaderas del lote actual y las convierte a un array de NumPy\n",
    "            tgt = labels.numpy()\n",
    "            \n",
    "            # Almacena las predicciones y las etiquetas verdaderas\n",
    "            tgts.append(tgt)\n",
    "            preds.append(y_pred)\n",
    "        \n",
    "        # Aplana las listas de listas para obtener una única lista de etiquetas y predicciones\n",
    "        tgts = [e for l in tgts for e in l]\n",
    "        preds = [e for l in preds for e in l]\n",
    "        \n",
    "        # Calcula y devuelve la precisión del modelo comparando las predicciones con las etiquetas verdaderas\n",
    "        return accuracy_score(tgts, preds)\n",
    "    \n",
    "\n",
    "# Guarda el estado actual del modelo y, si es el mejor modelo hasta el momento según \n",
    "# algún criterio, guarda una copia separada.\n",
    "def save_checkpoint(state, is_best, checkpoint_path, filename=\"checkpoint.pt\"):\n",
    "    # Construye la ruta completa del archivo donde se guardará el estado del modelo\n",
    "    filename = os.path.join(checkpoint_path, filename)\n",
    "    \n",
    "    # Guarda el estado del modelo en la ruta especificada\n",
    "    torch.save(state, filename)\n",
    "    \n",
    "    # Si el modelo actual es el \"mejor\" según algún criterio, guarda una copia separada\n",
    "    if is_best:\n",
    "        # Copia el archivo del checkpoint al archivo del \"mejor modelo\"\n",
    "        shutil.copyfile(filename, os.path.join(checkpoint_path, \"model_best.pt\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesitamos ahora definir todos los hiperparámetros de la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guillermo_sego/anaconda3/envs/PLN/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "# Model hyperparameters\n",
    "\n",
    "args. vocab_size = ngram_data.get_vocab_size()\n",
    "args.d = 100\n",
    "args.d_h = 200\n",
    "args.dropout = 0.1\n",
    "\n",
    "# Training hyperparameters\n",
    "args.lr = 2.3e-1\n",
    "args.num_epochs = 100\n",
    "args.patience = 20\n",
    "\n",
    "# Scheduler hyperparameters\n",
    "args.lr_patience = 10\n",
    "args.lr_factor = 0.5\n",
    "\n",
    "# Saving directoty\n",
    "args.savedir = 'model'\n",
    "os.makedirs(args.savedir, exist_ok=True)\n",
    "\n",
    "# Create model\n",
    "model = NeuralLM(args)\n",
    "\n",
    "# Send to GPU\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "if args.use_gpu:\n",
    "    model.cuda()\n",
    "    \n",
    "# Loss, optimizer an scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr= args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, 'min',\n",
    "                patience=args.lr_patience,\n",
    "                verbose=True,\n",
    "                factor=args.lr_factor\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora realizamos el entrenamiento y validación del modelo de aprendizaje profundo utilizando PyTorch, incluyendo características como el early stopping y el ajuste dinámico de hiperparámetros (scheduler). Aquí está lo que sucede durante el proceso:\n",
    "\n",
    "1. **Inicialización**: Se establece un tiempo de inicio para medir la duración del entrenamiento, se inicializa la mejor métrica a 0 para seguimiento, y se crean listas para almacenar la historia de las métricas durante el entrenamiento y la validación.\n",
    "\n",
    "2. **Bucle de Entrenamiento**: Para cada época dentro del número total de épocas especificadas:\n",
    "   \n",
    "   a. Se inicializa el modelo en modo de entrenamiento.\n",
    "   \n",
    "   b. Se itera sobre los lotes de datos del conjunto de entrenamiento (`train_loader`). Para cada lote, se mueven los datos al dispositivo adecuado (GPU si está disponible), se realiza un forward pass para obtener las predicciones del modelo, y se calcula la pérdida utilizando una función de pérdida (`criterion`) especificada.\n",
    "   \n",
    "   c. Se calcula una métrica de entrenamiento (precisión) comparando las predicciones con las etiquetas verdaderas, y se almacena para análisis posterior.\n",
    "   \n",
    "   d. Se ejecuta el backward pass para calcular los gradientes de la pérdida respecto a los parámetros del modelo, seguido de un paso de optimización para actualizar los parámetros del modelo utilizando el optimizador especificado. Se limpian los gradientes antes de cada backward pass para evitar la acumulación.\n",
    "\n",
    "3. **Evaluación y Ajuste**: Después de cada época de entrenamiento:\n",
    "   \n",
    "   a. Se evalúa el modelo en el conjunto de validación utilizando la función `model_eval`, que desactiva el cálculo de gradientes y devuelve la precisión en el conjunto de validación.\n",
    "   \n",
    "   b. Se actualiza el scheduler basado en la métrica de validación, lo cual puede ajustar la tasa de aprendizaje u otros hiperparámetros según el rendimiento en el conjunto de validación.\n",
    "\n",
    "4. **Seguimiento y Guardado de Mejores Modelos**: Si el rendimiento en el conjunto de validación mejora respecto al mejor registrado hasta el momento, se actualiza la mejor métrica y se guarda el estado del modelo (incluyendo los parámetros del modelo, el estado del optimizador, el scheduler y la mejor métrica) usando `save_checkpoint`. Si no hay mejora, se incrementa un contador que puede desencadenar el early stopping si se alcanza un límite de paciencia definido.\n",
    "\n",
    "5. **Early Stopping**: Si no se observa mejora en la métrica de validación durante un número especificado de épocas (`args.patience`), el entrenamiento se detiene anticipadamente para evitar el sobreajuste y reducir el tiempo de cómputo.\n",
    "\n",
    "6. **Registro de Métricas y Tiempos**: Se imprime la precisión de entrenamiento, la pérdida promedio de la época, la precisión de validación y el tiempo total de la época al final de cada época, proporcionando una visión en tiempo real del progreso del entrenamiento.\n",
    "\n",
    "Finalmente, al concluir todas las épocas o al activarse el early stopping, se imprime la duración total del entrenamiento, ofreciendo una medida del tiempo que tomó entrenar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.17303716155144017\n",
      "Epoch [1/100], Loss: 5.5234 - Val accuracy: 0.1909 - Epoch time: 1711067898.82\n",
      "Train acc: 0.18379570813079982\n",
      "Epoch [2/100], Loss: 5.0786 - Val accuracy: 0.1449 - Epoch time: 1711067915.53\n",
      "Train acc: 0.18953652884344996\n",
      "Epoch [3/100], Loss: 4.8677 - Val accuracy: 0.1607 - Epoch time: 1711067934.89\n",
      "Train acc: 0.1940113058892058\n",
      "Epoch [4/100], Loss: 4.7005 - Val accuracy: 0.1809 - Epoch time: 1711067952.71\n",
      "Train acc: 0.1977990800626682\n",
      "Epoch [5/100], Loss: 4.5542 - Val accuracy: 0.2263 - Epoch time: 1711067968.51\n",
      "Train acc: 0.1999077928534126\n",
      "Epoch [6/100], Loss: 4.4211 - Val accuracy: 0.2167 - Epoch time: 1711067985.10\n",
      "Train acc: 0.20232187512553731\n",
      "Epoch [7/100], Loss: 4.3000 - Val accuracy: 0.2156 - Epoch time: 1711068002.23\n",
      "Train acc: 0.2063271426706303\n",
      "Epoch [8/100], Loss: 4.1793 - Val accuracy: 0.2279 - Epoch time: 1711068018.39\n",
      "Train acc: 0.2080595574056964\n",
      "Epoch [9/100], Loss: 4.0763 - Val accuracy: 0.2524 - Epoch time: 1711068034.29\n",
      "Train acc: 0.2104949810187603\n",
      "Epoch [10/100], Loss: 3.9731 - Val accuracy: 0.2214 - Epoch time: 1711068051.46\n",
      "Train acc: 0.21582529476157955\n",
      "Epoch [11/100], Loss: 3.8763 - Val accuracy: 0.2607 - Epoch time: 1711068065.79\n",
      "Train acc: 0.22082732846583378\n",
      "Epoch [12/100], Loss: 3.7875 - Val accuracy: 0.2775 - Epoch time: 1711068080.12\n",
      "Train acc: 0.2298104637849998\n",
      "Epoch [13/100], Loss: 3.7041 - Val accuracy: 0.2591 - Epoch time: 1711068094.85\n",
      "Train acc: 0.27656494797734305\n",
      "Epoch [14/100], Loss: 3.3365 - Val accuracy: 0.3336 - Epoch time: 1711068109.30\n",
      "Train acc: 0.28551701281484754\n",
      "Epoch [15/100], Loss: 3.2680 - Val accuracy: 0.3391 - Epoch time: 1711068123.60\n",
      "Train acc: 0.2894287550717069\n",
      "Epoch [16/100], Loss: 3.2253 - Val accuracy: 0.3554 - Epoch time: 1711068139.36\n",
      "Train acc: 0.29490563361185873\n",
      "Epoch [17/100], Loss: 3.1847 - Val accuracy: 0.3583 - Epoch time: 1711068157.16\n",
      "Train acc: 0.3003162912264492\n",
      "Epoch [18/100], Loss: 3.1509 - Val accuracy: 0.3658 - Epoch time: 1711068177.50\n",
      "Train acc: 0.3052269965452135\n",
      "Epoch [19/100], Loss: 3.1181 - Val accuracy: 0.3727 - Epoch time: 1711068194.07\n",
      "Train acc: 0.3095570918531314\n",
      "Epoch [20/100], Loss: 3.0903 - Val accuracy: 0.3306 - Epoch time: 1711068213.86\n",
      "Train acc: 0.3126826567709798\n",
      "Epoch [21/100], Loss: 3.0616 - Val accuracy: 0.3664 - Epoch time: 1711068231.90\n",
      "Train acc: 0.31715617844373917\n",
      "Epoch [22/100], Loss: 3.0403 - Val accuracy: 0.3751 - Epoch time: 1711068249.39\n",
      "Train acc: 0.32042768047242204\n",
      "Epoch [23/100], Loss: 3.0136 - Val accuracy: 0.3831 - Epoch time: 1711068267.23\n",
      "Train acc: 0.32501073343911946\n",
      "Epoch [24/100], Loss: 2.9926 - Val accuracy: 0.3889 - Epoch time: 1711068283.15\n",
      "Train acc: 0.35688151538585144\n",
      "Epoch [25/100], Loss: 2.8073 - Val accuracy: 0.4178 - Epoch time: 1711068298.83\n",
      "Train acc: 0.3614463654441008\n",
      "Epoch [26/100], Loss: 2.7822 - Val accuracy: 0.4216 - Epoch time: 1711068314.41\n",
      "Train acc: 0.3625366568914956\n",
      "Epoch [27/100], Loss: 2.7687 - Val accuracy: 0.4275 - Epoch time: 1711068333.80\n",
      "Train acc: 0.363117266902342\n",
      "Epoch [28/100], Loss: 2.7620 - Val accuracy: 0.4296 - Epoch time: 1711068356.73\n",
      "Train acc: 0.3659691605069698\n",
      "Epoch [29/100], Loss: 2.7491 - Val accuracy: 0.4264 - Epoch time: 1711068373.25\n",
      "Train acc: 0.36592114248985663\n",
      "Epoch [30/100], Loss: 2.7375 - Val accuracy: 0.3732 - Epoch time: 1711068390.45\n",
      "Train acc: 0.3691791492588278\n",
      "Epoch [31/100], Loss: 2.7260 - Val accuracy: 0.4182 - Epoch time: 1711068407.15\n",
      "Train acc: 0.3710167014823444\n",
      "Epoch [32/100], Loss: 2.7149 - Val accuracy: 0.4371 - Epoch time: 1711068424.45\n",
      "Train acc: 0.37115793094444216\n",
      "Epoch [33/100], Loss: 2.7139 - Val accuracy: 0.4384 - Epoch time: 1711068440.99\n",
      "Train acc: 0.3715530595950669\n",
      "Epoch [34/100], Loss: 2.7037 - Val accuracy: 0.4410 - Epoch time: 1711068459.01\n",
      "Train acc: 0.37308053468846664\n",
      "Epoch [35/100], Loss: 2.6960 - Val accuracy: 0.4400 - Epoch time: 1711068478.11\n",
      "Train acc: 0.3922883692202627\n",
      "Epoch [36/100], Loss: 2.6059 - Val accuracy: 0.4439 - Epoch time: 1711068495.48\n",
      "Train acc: 0.39429194452255656\n",
      "Epoch [37/100], Loss: 2.5916 - Val accuracy: 0.4475 - Epoch time: 1711068514.35\n",
      "Train acc: 0.3937763000642751\n",
      "Epoch [38/100], Loss: 2.5903 - Val accuracy: 0.4510 - Epoch time: 1711068529.70\n",
      "Train acc: 0.395741586490178\n",
      "Epoch [39/100], Loss: 2.5830 - Val accuracy: 0.4483 - Epoch time: 1711068545.01\n",
      "Train acc: 0.394600766279677\n",
      "Epoch [40/100], Loss: 2.5766 - Val accuracy: 0.4456 - Epoch time: 1711068560.87\n",
      "Train acc: 0.39665800877756796\n",
      "Epoch [41/100], Loss: 2.5738 - Val accuracy: 0.4535 - Epoch time: 1711068578.16\n",
      "Train acc: 0.39570204224079064\n",
      "Epoch [42/100], Loss: 2.5728 - Val accuracy: 0.4467 - Epoch time: 1711068594.37\n",
      "Train acc: 0.39738267283975415\n",
      "Epoch [43/100], Loss: 2.5639 - Val accuracy: 0.4572 - Epoch time: 1711068609.73\n",
      "Train acc: 0.3975110347286386\n",
      "Epoch [44/100], Loss: 2.5632 - Val accuracy: 0.4399 - Epoch time: 1711068625.18\n",
      "Train acc: 0.3993893865745391\n",
      "Epoch [45/100], Loss: 2.5583 - Val accuracy: 0.4468 - Epoch time: 1711068642.00\n",
      "Train acc: 0.39815504107580446\n",
      "Epoch [46/100], Loss: 2.5588 - Val accuracy: 0.4638 - Epoch time: 1711068659.41\n",
      "Train acc: 0.4086132396657695\n",
      "Epoch [47/100], Loss: 2.5094 - Val accuracy: 0.4638 - Epoch time: 1711068676.38\n",
      "Train acc: 0.4101576622946209\n",
      "Epoch [48/100], Loss: 2.5018 - Val accuracy: 0.4648 - Epoch time: 1711068692.50\n",
      "Train acc: 0.4106064581408428\n",
      "Epoch [49/100], Loss: 2.5023 - Val accuracy: 0.4605 - Epoch time: 1711068707.99\n",
      "Train acc: 0.40855486482143577\n",
      "Epoch [50/100], Loss: 2.5012 - Val accuracy: 0.4603 - Epoch time: 1711068723.33\n",
      "Train acc: 0.4108898585947857\n",
      "Epoch [51/100], Loss: 2.4955 - Val accuracy: 0.4648 - Epoch time: 1711068740.40\n",
      "Train acc: 0.4092261755312739\n",
      "Epoch [52/100], Loss: 2.4973 - Val accuracy: 0.4671 - Epoch time: 1711068755.97\n",
      "Train acc: 0.4100123528702848\n",
      "Epoch [53/100], Loss: 2.4980 - Val accuracy: 0.4668 - Epoch time: 1711068771.27\n",
      "Train acc: 0.4106933927208452\n",
      "Epoch [54/100], Loss: 2.4929 - Val accuracy: 0.4625 - Epoch time: 1711068791.61\n",
      "Train acc: 0.41068303589362465\n",
      "Epoch [55/100], Loss: 2.4927 - Val accuracy: 0.4672 - Epoch time: 1711068812.19\n",
      "Train acc: 0.4111842435624472\n",
      "Epoch [56/100], Loss: 2.4887 - Val accuracy: 0.4661 - Epoch time: 1711068831.14\n",
      "Train acc: 0.4123887739525168\n",
      "Epoch [57/100], Loss: 2.4856 - Val accuracy: 0.4707 - Epoch time: 1711068852.41\n",
      "Train acc: 0.4176431376290523\n",
      "Epoch [58/100], Loss: 2.4632 - Val accuracy: 0.4648 - Epoch time: 1711068870.37\n",
      "Train acc: 0.4183910260916724\n",
      "Epoch [59/100], Loss: 2.4597 - Val accuracy: 0.4620 - Epoch time: 1711068887.11\n",
      "Train acc: 0.4170367924717792\n",
      "Epoch [60/100], Loss: 2.4579 - Val accuracy: 0.4700 - Epoch time: 1711068906.39\n",
      "Train acc: 0.4198692277949625\n",
      "Epoch [61/100], Loss: 2.4539 - Val accuracy: 0.4633 - Epoch time: 1711068925.63\n",
      "Train acc: 0.41877517022857835\n",
      "Epoch [62/100], Loss: 2.4588 - Val accuracy: 0.4716 - Epoch time: 1711068942.33\n",
      "Train acc: 0.41782799130277587\n",
      "Epoch [63/100], Loss: 2.4578 - Val accuracy: 0.4683 - Epoch time: 1711068962.94\n",
      "Train acc: 0.418525978688788\n",
      "Epoch [64/100], Loss: 2.4524 - Val accuracy: 0.4719 - Epoch time: 1711068983.05\n",
      "Train acc: 0.4188379388783996\n",
      "Epoch [65/100], Loss: 2.4529 - Val accuracy: 0.4614 - Epoch time: 1711069001.75\n",
      "Train acc: 0.4189754022215081\n",
      "Epoch [66/100], Loss: 2.4526 - Val accuracy: 0.4730 - Epoch time: 1711069024.75\n",
      "Train acc: 0.4186207593500181\n",
      "Epoch [67/100], Loss: 2.4540 - Val accuracy: 0.4699 - Epoch time: 1711069044.63\n",
      "Train acc: 0.4188087514562327\n",
      "Epoch [68/100], Loss: 2.4500 - Val accuracy: 0.4697 - Epoch time: 1711069062.34\n",
      "Train acc: 0.4205587414132487\n",
      "Epoch [69/100], Loss: 2.4407 - Val accuracy: 0.4711 - Epoch time: 1711069080.21\n",
      "Train acc: 0.421422751878038\n",
      "Epoch [70/100], Loss: 2.4351 - Val accuracy: 0.4702 - Epoch time: 1711069097.30\n",
      "Train acc: 0.423157677359097\n",
      "Epoch [71/100], Loss: 2.4373 - Val accuracy: 0.4743 - Epoch time: 1711069114.38\n",
      "Train acc: 0.4231589327320934\n",
      "Epoch [72/100], Loss: 2.4337 - Val accuracy: 0.4735 - Epoch time: 1711069131.42\n",
      "Train acc: 0.42264266058731376\n",
      "Epoch [73/100], Loss: 2.4364 - Val accuracy: 0.4719 - Epoch time: 1711069150.62\n",
      "Train acc: 0.4231517143373639\n",
      "Epoch [74/100], Loss: 2.4355 - Val accuracy: 0.4727 - Epoch time: 1711069166.73\n",
      "Train acc: 0.42178775157674847\n",
      "Epoch [75/100], Loss: 2.4347 - Val accuracy: 0.4693 - Epoch time: 1711069184.35\n",
      "Train acc: 0.4223489033061503\n",
      "Epoch [76/100], Loss: 2.4343 - Val accuracy: 0.4725 - Epoch time: 1711069201.55\n",
      "Train acc: 0.422247531936689\n",
      "Epoch [77/100], Loss: 2.4315 - Val accuracy: 0.4655 - Epoch time: 1711069217.29\n",
      "Train acc: 0.4236999984935524\n",
      "Epoch [78/100], Loss: 2.4293 - Val accuracy: 0.4722 - Epoch time: 1711069232.05\n",
      "Train acc: 0.4222139507090347\n",
      "Epoch [79/100], Loss: 2.4328 - Val accuracy: 0.4731 - Epoch time: 1711069246.46\n",
      "Train acc: 0.4237998006467682\n",
      "Epoch [80/100], Loss: 2.4258 - Val accuracy: 0.4717 - Epoch time: 1711069263.76\n",
      "Train acc: 0.4246917431607279\n",
      "Epoch [81/100], Loss: 2.4252 - Val accuracy: 0.4702 - Epoch time: 1711069282.40\n",
      "Train acc: 0.42470335536094483\n",
      "Epoch [82/100], Loss: 2.4233 - Val accuracy: 0.4715 - Epoch time: 1711069297.71\n",
      "Train acc: 0.42451504941148116\n",
      "Epoch [83/100], Loss: 2.4268 - Val accuracy: 0.4712 - Epoch time: 1711069314.91\n",
      "Train acc: 0.42395672227132125\n",
      "Epoch [84/100], Loss: 2.4262 - Val accuracy: 0.4722 - Epoch time: 1711069331.15\n",
      "Train acc: 0.42399438346121404\n",
      "Epoch [85/100], Loss: 2.4250 - Val accuracy: 0.4731 - Epoch time: 1711069349.48\n",
      "Train acc: 0.4249381101112763\n",
      "Epoch [86/100], Loss: 2.4247 - Val accuracy: 0.4730 - Epoch time: 1711069368.91\n",
      "Train acc: 0.4240527583055478\n",
      "Epoch [87/100], Loss: 2.4230 - Val accuracy: 0.4722 - Epoch time: 1711069386.90\n",
      "Train acc: 0.4251204530390069\n",
      "Epoch [88/100], Loss: 2.4260 - Val accuracy: 0.4729 - Epoch time: 1711069404.80\n",
      "Train acc: 0.4245385876551641\n",
      "Epoch [89/100], Loss: 2.4232 - Val accuracy: 0.4721 - Epoch time: 1711069422.45\n",
      "Train acc: 0.423624676113767\n",
      "Epoch [90/100], Loss: 2.4246 - Val accuracy: 0.4742 - Epoch time: 1711069442.37\n",
      "Train acc: 0.42525132567388424\n",
      "Epoch [91/100], Loss: 2.4213 - Val accuracy: 0.4731 - Epoch time: 1711069462.11\n",
      "Train acc: 0.42618406781022783\n",
      "Epoch [92/100], Loss: 2.4172 - Val accuracy: 0.4719 - Epoch time: 1711069479.14\n",
      "Train acc: 0.4256477096975053\n",
      "Epoch [93/100], Loss: 2.4222 - Val accuracy: 0.4727 - Epoch time: 1711069497.34\n",
      "Train acc: 0.42574311804523357\n",
      "Epoch [94/100], Loss: 2.4195 - Val accuracy: 0.4735 - Epoch time: 1711069512.98\n",
      "Train acc: 0.42633596794279516\n",
      "Epoch [95/100], Loss: 2.4189 - Val accuracy: 0.4724 - Epoch time: 1711069528.33\n",
      "Train acc: 0.42578517304061386\n",
      "Epoch [96/100], Loss: 2.4208 - Val accuracy: 0.4733 - Epoch time: 1711069543.03\n",
      "Train acc: 0.42393789167637486\n",
      "Epoch [97/100], Loss: 2.4256 - Val accuracy: 0.4739 - Epoch time: 1711069559.09\n",
      "Train acc: 0.42423698429277307\n",
      "Epoch [98/100], Loss: 2.4227 - Val accuracy: 0.4729 - Epoch time: 1711069576.28\n",
      "Train acc: 0.4261445235608404\n",
      "Epoch [99/100], Loss: 2.4176 - Val accuracy: 0.4735 - Epoch time: 1711069592.70\n",
      "Train acc: 0.4242175260113285\n",
      "Epoch [100/100], Loss: 2.4209 - Val accuracy: 0.4720 - Epoch time: 1711069611.86\n",
      "--- 1731.800127029419 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "best_metric = 0\n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "for epoch in range(args.num_epochs):\n",
    "    epoch_stats_time = time.time()\n",
    "    loss_epoch = []\n",
    "    training_metric = []\n",
    "    model.train()\n",
    "    \n",
    "    for window_words, labels in train_loader:\n",
    "        if args.use_gpu:\n",
    "            window_words = window_words.cuda()\n",
    "            labels = labels.cuda()\n",
    "            \n",
    "        #Forward pass\n",
    "        outputs = model(window_words)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_epoch.append(loss.item())\n",
    "        \n",
    "        #Get training metrics\n",
    "        y_pred = get_preds(outputs)\n",
    "        tgt = labels.cpu().numpy()\n",
    "        training_metric.append(accuracy_score(tgt, y_pred))\n",
    "        \n",
    "        #Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    #Get metric in training dataset\n",
    "    mean_epoch_metric = np.mean(training_metric)\n",
    "    train_metric_history.append(mean_epoch_metric)\n",
    "    \n",
    "    #Get metric in validation dataset\n",
    "    model.eval()\n",
    "    tuning_metric = model_eval(val_loader, model, gpu=args.use_gpu)\n",
    "    metric_history.append(mean_epoch_metric)\n",
    "    \n",
    "    #update scheduler\n",
    "    scheduler.step(tuning_metric)\n",
    "    \n",
    "    #Check for metric improvement\n",
    "    is_improvement = tuning_metric > best_metric\n",
    "    if is_improvement:\n",
    "        best_metric = tuning_metric\n",
    "        n_no_improve = 0\n",
    "    else:\n",
    "        n_no_improve = 1\n",
    "        \n",
    "    save_checkpoint(\n",
    "        {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"scheduler\": scheduler.state_dict(),\n",
    "            \"best_metric\": best_metric,\n",
    "        },\n",
    "        is_improvement,\n",
    "        args.savedir,\n",
    "    )\n",
    "    \n",
    "    #Early stopping\n",
    "    if n_no_improve >= args.patience:\n",
    "        print(\"No improvement. Breaking out of loop.\")\n",
    "        \n",
    "    print(\"Train acc: {}\".format(mean_epoch_metric))\n",
    "    print(\"Epoch [{}/{}], Loss: {:.4f} - Val accuracy: {:.4f} - Epoch time: {:.2f}\"\n",
    "         .format(epoch+1, args.num_epochs, np.mean(loss_epoch), tuning_metric, (time.time())))\n",
    "    \n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación de un modelo de lenguaje neuronal\n",
    "\n",
    "Evaluar un modelo de lenguaje neuronal es un paso crucial después del entrenamiento para entender su rendimiento y cómo se generaliza a datos no vistos. La evaluación implica medir cuán bien el modelo predice o genera texto, utilizando métricas específicas y conjuntos de datos de prueba o validación. Aquí hay algunos aspectos clave para comenzar:\n",
    "\n",
    "### 1. **Conjunto de Datos de Evaluación**:\n",
    "Usualmente, se separa un conjunto de datos que no se utiliza durante el entrenamiento para evaluar el modelo. Este conjunto puede ser de validación (usado para ajustar hiperparámetros) o de prueba (usado para medir el rendimiento final del modelo). Es importante que el modelo no haya \"visto\" estos datos durante el entrenamiento para obtener una evaluación justa de su capacidad de generalización.\n",
    "\n",
    "### 2. **Métricas de Evaluación**:\n",
    "Dependiendo de la tarea específica del modelo de lenguaje, las métricas de evaluación pueden variar:\n",
    "   - **Perplejidad**: Comúnmente usada en modelos de lenguaje, mide cuán \"sorprendido\" está el modelo por los datos de prueba, con valores más bajos indicando un mejor rendimiento.\n",
    "   - **Precisión, Recall, y F1**: Usadas en tareas de clasificación, como análisis de sentimientos o clasificación de temas.\n",
    "   - **BLEU, ROUGE, METEOR**: Usadas para evaluación en tareas de generación de texto, como traducción automática o resumen automático, donde se comparan las salidas del modelo con referencias humanas.\n",
    "\n",
    "### 3. **Evaluación Cualitativa**:\n",
    "Además de las métricas cuantitativas, es útil realizar una evaluación cualitativa del modelo, examinando las predicciones del modelo y cómo maneja varios casos de uso, errores comunes, y situaciones específicas del dominio. Esto puede incluir la revisión manual de las predicciones del modelo para evaluar su coherencia, relevancia y creatividad.\n",
    "\n",
    "### 4. **Análisis de Errores**:\n",
    "Identificar y analizar los errores cometidos por el modelo puede proporcionar insights valiosos sobre sus limitaciones y áreas para mejora. Esto puede implicar explorar casos donde el modelo se desempeña mal y tratar de entender las razones detrás de estos errores.\n",
    "\n",
    "### 5. **Comparación con Modelos de Referencia**:\n",
    "Es útil comparar el rendimiento de tu modelo con el de modelos de referencia o benchmarks en la misma tarea. Esto puede darte una idea de cómo se compara tu enfoque con el estado del arte o con enfoques más simples.\n",
    "\n",
    "### 6. **Consideraciones Éticas y de Sesgo**:\n",
    "En la evaluación de modelos de lenguaje, especialmente aquellos entrenados en grandes corpus de texto de internet, es importante considerar y evaluar posibles sesgos y problemas éticos en las predicciones del modelo. Esto puede incluir sesgos de género, raza, o cultural que el modelo podría haber aprendido de los datos de entrenamiento.\n",
    "\n",
    "### 7. **Evaluación en Ambientes de Producción**:\n",
    "Finalmente, si el modelo va a ser desplegado en un sistema en producción, es crucial realizar evaluaciones en un entorno que simule el uso real, ya que el comportamiento del modelo puede variar significativamente en condiciones del mundo real en comparación con un entorno de prueba controlado.\n",
    "\n",
    "La evaluación de modelos de lenguaje es un proceso iterativo y multifacético que va más allá de simples métricas, abarcando evaluaciones cualitativas, comparativas, y éticas para obtener una comprensión completa del rendimiento y las aplicaciones del modelo.\n",
    "\n",
    "---\n",
    "\n",
    "Vamos a comparar la representación de las palabras, calculando las distancias y viendo que tan cerca están. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_closest_words(embeddings, ngram_data, word, n):\n",
    "    word_id = torch.LongTensor([ngram_data.w2id[word]])\n",
    "    word_embed = embeddings(word_id)\n",
    "    dists = torch.norm(embeddings.weight - word_embed, dim = 1).detach()\n",
    "    lst = sorted(enumerate(dists.numpy()), key = lambda x: x[1])\n",
    "    for idx, difference in lst[1:n+1]:\n",
    "        print(ngram_data.id2w[idx], difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el mejor modelo que se haya guardado y proseguimos para hacer una visualización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "<s> 10.224871\n",
      "<unk> 10.376606\n",
      "compras 10.435783\n",
      "feliz 10.801502\n",
      "👸 10.8067255\n",
      "últimos 11.08122\n",
      "#eliminatoriasconmebol 11.145872\n",
      "chillón 11.239394\n",
      "único 11.266181\n",
      "teléfono 11.281281\n"
     ]
    }
   ],
   "source": [
    "# Model with learned embeddings from scratch\n",
    "best_model = NeuralLM(args)\n",
    "best_model.load_state_dict(torch.load(\"model/model_best.pt\")[\"state_dict\"])\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"Learned embeddings\")\n",
    "print(\"-\"*30)\n",
    "print_closest_words(best_model.emb, ngram_data, \"jaja\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(text, tokenizer):\n",
    "    # Tokeniza el texto y convierte cada palabra a minúsculas. Si la palabra está en el vocabulario (w2id), la usa;\n",
    "    # de lo contrario, la reemplaza por el token \"<unk>\" para palabras desconocidas.\n",
    "    all_tokens = [w.lower() if w.lower() in ngram_data.w2id else \"<unk>\" for w in tokenizer.tokenize(text)]\n",
    "    \n",
    "    # Convierte los tokens a sus índices numéricos correspondientes según el mapeo w2id en ngram_data.\n",
    "    token_ids = [ngram_data.w2id[word.lower()] for word in all_tokens]\n",
    "    \n",
    "    return all_tokens, token_ids\n",
    "\n",
    "\n",
    "def sample_next_word(logits, temperature=1.0):\n",
    "    # Convierte los logits a un array de numpy y ajusta la \"temperatura\" de la predicción.\n",
    "    logits = np.asarray(logits).astype(\"float64\")\n",
    "    preds = logits / temperature\n",
    "    \n",
    "    # Convierte los logits ajustados a probabilidades usando softmax.\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    \n",
    "    # Muestrea un índice de palabra de la distribución de probabilidades.\n",
    "    probas = np.random.multinomial(1, preds)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def predict_next_token(model, token_ids):\n",
    "    # Convierte la lista de índices de tokens a un tensor de PyTorch y agrega una dimensión de lote.\n",
    "    word_ids_tensor = torch.LongTensor(token_ids).unsqueeze(0)\n",
    "    \n",
    "    # Obtiene los logits de la predicción del modelo para la secuencia de tokens y los convierte a numpy.\n",
    "    y_raw_pred = model(word_ids_tensor).squeeze(0).detach().numpy()\n",
    "    \n",
    "    # Muestra el índice de la siguiente palabra de la distribución de logits.\n",
    "    y_pred = sample_next_word(y_raw_pred, 1.0)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def generate_sentence(model, initial_text, tokenizer):\n",
    "    # Obtiene tokens y sus índices del texto inicial.\n",
    "    all_tokens, window_word_ids = parse_text(initial_text, tokenizer)\n",
    "    \n",
    "    # Genera hasta 100 palabras adicionales.\n",
    "    for i in range(100):\n",
    "        # Predice el índice de la siguiente palabra utilizando el modelo.\n",
    "        y_pred = predict_next_token(model, window_word_ids)\n",
    "        next_word = ngram_data.id2w[y_pred]  # Convierte el índice de palabra predicho a texto.\n",
    "        all_tokens.append(next_word)  # Añade la palabra predicha a la lista de tokens.\n",
    "        \n",
    "        # Si se genera el token de fin de secuencia, detiene la generación.\n",
    "        if next_word == \"</s>\":\n",
    "            break\n",
    "        else:\n",
    "            # Actualiza la ventana de palabras para la siguiente predicción.\n",
    "            window_word_ids.pop(0)  # Elimina la primera palabra.\n",
    "            window_word_ids.append(y_pred)  # Añade la nueva palabra al final.\n",
    "    \n",
    "    # Une los tokens generados en una cadena de texto y la devuelve.\n",
    "    return \" \".join(all_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realiza la generación de texto utilizando el modelo de lenguaje neuronal previamente entrenado (`best_model`), comenzando con un texto inicial dado (`initial_tokens`). Veamos paso a paso el proceso:\n",
    "\n",
    "1. **Definir el Texto Inicial**: La variable `initial_tokens` contiene el texto inicial desde el cual el modelo comenzará a generar texto. En este caso, parece que se utilizan tokens especiales de inicio `<s>` para indicar el comienzo de una nueva secuencia. La presencia de múltiples tokens `<s>` podría ser una convención para señalar el inicio de una secuencia, especialmente en modelos que utilizan n-gramas o contextos de longitud fija.\n",
    "\n",
    "2. **Impresión de Encabezados**: Se imprimen líneas de guiones y el texto \"Learned embeddings\" como encabezado para indicar que a continuación se mostrarán los resultados de la generación de texto.\n",
    "\n",
    "3. **Generación de Texto**: La función `generate_sentence` se utiliza para generar una secuencia de texto a partir del `initial_tokens` proporcionado. Esta función toma el modelo entrenado (`best_model`), el texto inicial, y un tokenizador (`tk`) para procesar el texto. La función realiza las siguientes tareas:\n",
    "   - Tokeniza el texto inicial y obtiene los índices correspondientes de los tokens en el vocabulario del modelo (`parse_text`).\n",
    "   - Utiliza el modelo para predecir la próxima palabra en la secuencia, basándose en los índices de tokens actuales (`predict_next_token`).\n",
    "   - Añade la palabra predicha a la secuencia y actualiza el contexto de palabras para la próxima predicción.\n",
    "   - Repite el proceso de predicción y actualización para generar hasta 100 palabras o hasta que se genere un token de fin de secuencia (`</s>`).\n",
    "   - Devuelve el texto generado como una cadena de texto, uniendo los tokens generados.\n",
    "\n",
    "4. **Impresión del Texto Generado**: Finalmente, el texto generado por la función `generate_sentence` se imprime en la consola. Este texto es una continuación del `initial_tokens` proporcionado, generado por el modelo `best_model` basándose en lo que ha aprendido durante el entrenamiento sobre la estructura y el contenido típicos del lenguaje en el dominio de los datos con los que fue entrenado.\n",
    "\n",
    "En resumen, estas funciones y el código proporcionado demuestran cómo se puede utilizar un modelo de lenguaje neuronal entrenado para generar texto de manera autónoma, comenzando con un prompt o secuencia inicial dada, lo cual es útil en aplicaciones como la generación de texto creativo, la auto-completación de texto, y más."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "<s> <s> <s> se armó el joto mamón que <unk> en esta amargado y lo <unk> que son viejas a mi madre que consiente listo es tonta y soltar </s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = \"<s><s><s>\"\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"Learned embeddings\")\n",
    "print(\"-\"*30)\n",
    "print(generate_sentence(best_model, initial_tokens, tk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "<s> <s> estoy hasta la puta madre pero a los jotos madre el cuerpo cuando la misma está pero loca </s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = \"<s><s>estoy\"\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"Learned embeddings\")\n",
    "print(\"-\"*30)\n",
    "print(generate_sentence(best_model, initial_tokens, tk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "<s> saludos a la novia <unk> tiene un genio pedo está desde el invierno pasado para ver el capitulo de <unk> haciendo súper \" the queda <unk> <unk> a la verga chamaca </s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = \"<s> saludos a\"\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"Learned embeddings\")\n",
    "print(\"-\"*30)\n",
    "print(generate_sentence(best_model, initial_tokens, tk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "<s> saludos a <unk> como me hijo de mil putas <unk> tu forma de ser jamás <unk> siempre les <unk> <unk> así de <unk> amika no se les puede decir nada sin que <unk> </s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = \"<s> saludos a\"\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"Learned embeddings\")\n",
    "print(\"-\"*30)\n",
    "print(generate_sentence(best_model, initial_tokens, tk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "yo opino que como no c puede mal <unk> 😤 </s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = \"yo opino que\"\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"Learned embeddings\")\n",
    "print(\"-\"*30)\n",
    "print(generate_sentence(best_model, initial_tokens, tk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `log_likelihood` calcula la log-verosimilitud de una secuencia de texto dada bajo un modelo de lenguaje entrenado. La log-verosimilitud es una medida de cuán probable es que el modelo haya generado una secuencia de texto dada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(model, text, ngram_model):\n",
    "    # Transforma el texto dado en n-gramas (X) y las etiquetas objetivo (y) utilizando el modelo n-gram.\n",
    "    X, y = ngram_data.transform([text])\n",
    "    \n",
    "    # Ignora los primeros dos n-gramas. Esto podría ser específico para cómo se estructura el texto o los n-gramas.\n",
    "    X, y = X[2:], y[2:]\n",
    "    \n",
    "    # Convierte X en un tensor de PyTorch y agrega una dimensión de lote, preparándolo para el modelo.\n",
    "    X = torch.LongTensor(X).unsqueeze(0)\n",
    "    \n",
    "    # Obtiene los logits del modelo para el texto transformado y luego los desconecta del grafo de cómputo.\n",
    "    logits = model(X).detach()\n",
    "    \n",
    "    # Aplica softmax a los logits para obtener una distribución de probabilidades sobre el vocabulario.\n",
    "    probs = F.softmax(logits, dim=1).numpy()\n",
    "    \n",
    "    # Calcula la log-verosimilitud sumando los logaritmos de las probabilidades de las palabras reales (y)\n",
    "    # según lo predicho por el modelo.\n",
    "    return np.sum([np.log(probs[i][w]) for i, w in enumerate(y)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos el modelo. La log-verosimilitud es una medida de cuán probable es que una secuencia de texto dada sea generada por un modelo de lenguaje. Valores más altos de log-verosimilitud indican que el modelo asigna una mayor probabilidad a la secuencia de texto observada, lo que sugiere que el modelo piensa que es más \"verosímil\" o probable.\n",
    "\n",
    "Cuando calculas la log-verosimilitud, sumas los logaritmos de las probabilidades asignadas a las palabras reales. Dado que estas probabilidades están entre 0 y 1, sus logaritmos son negativos. Por lo tanto, una log-verosimilitud más alta (menos negativa) significa que las probabilidades asociadas a las palabras observadas son más altas, y el modelo considera que la secuencia es más probable. Una log-verosimilitud muy negativa indica que el modelo asigna muy bajas probabilidades a las palabras observadas, lo que sugiere que el modelo considera que esa secuencia de texto es poco probable.\n",
    "\n",
    "Así que, en resumen, buscas maximizar la log-verosimilitud. Valores más cercanos a cero o menos negativos son mejores, ya que indican que el modelo asigna mayores probabilidades a las secuencias de texto observadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -24.262943\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model, \"Estamos en la clase de procesamiento de lenguaje\", ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -40.692436\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"log likelihood: \", log_likelihood(best_model, \"la natural estmos clase en de de lenguaje procesamiento\", ngram_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estructuras sintácticas correctas\n",
    "\n",
    "El siguiente código explora las diferentes permutaciones de una secuencia de palabras dada y las evalúa usando el modelo de lenguaje (`best_model`) para determinar la log-verosimilitud de cada permutación. A continuación, detallo lo que hace cada parte del código y luego te doy una introducción a las estructuras sintácticas correctas.\n",
    "\n",
    "### Exploración de Permutaciones\n",
    "\n",
    "1. **Creación de la Lista de Palabras**: La frase \"sino gano me voy a la chingada\" se divide en palabras individuales para formar `word_list`.\n",
    "\n",
    "2. **Generación de Permutaciones**: Utilizando `permutations` de `itertools`, el código genera todas las posibles permutaciones de la lista de palabras. Cada permutación se une en una cadena de texto separada por espacios y se almacena en `perms`.\n",
    "\n",
    "3. **Evaluación de las Permutaciones**: Para cada permutación, se calcula la log-verosimilitud usando la función `log_likelihood`, pasando el `best_model`, la permutación de texto, y `ngram_data`. Se crean tuplas de log-verosimilitud y texto de permutación.\n",
    "\n",
    "4. **Ordenación y Presentación de Resultados**: Las tuplas se ordenan por log-verosimilitud de manera descendente. Luego, el código imprime las 5 permutaciones con mayor log-verosimilitud y las 5 permutaciones con menor log-verosimilitud para mostrar cuáles secuencias de palabras son consideradas más y menos probables por el modelo.\n",
    "\n",
    "### Estructuras Sintácticas Correctas\n",
    "\n",
    "Las estructuras sintácticas en un idioma se refieren a cómo se organizan las palabras y frases para formar oraciones significativas y gramaticalmente correctas. La sintaxis incluye reglas sobre el orden de las palabras, la concordancia entre sujetos y verbos, el uso correcto de los tiempos verbales, y la estructuración de frases complejas, entre otros aspectos.\n",
    "\n",
    "En lenguajes como el español o el inglés, la estructura sintáctica típica sigue un patrón de \"Sujeto-Verbo-Objeto\" (SVO), aunque puede variar dependiendo del propósito y estilo de la oración. Por ejemplo, en la frase \"Yo como manzanas\", \"Yo\" es el sujeto, \"como\" el verbo, y \"manzanas\" el objeto.\n",
    "\n",
    "La sintaxis también involucra el uso de conectores, preposiciones, artículos y otros elementos que ayudan a unir las palabras en frases coherentes y estructuradas. Una comprensión profunda de la sintaxis es crucial para la construcción de oraciones que no solo sean gramaticalmente correctas, sino que también transmitan claramente la intención del hablante o escritor.\n",
    "\n",
    "En el contexto del fragmento de código proporcionado, el modelo de lenguaje intenta capturar algunas de estas reglas sintácticas implícitamente a través del aprendizaje de las probabilidades de secuencias de palabras basado en el corpus de entrenamiento. Sin embargo, al generar todas las permutaciones posibles de una lista de palabras, muchas de las secuencias resultantes serán sintácticamente incorrectas o carecerán de sentido, ya que no todas las combinaciones de palabras forman oraciones coherentes o gramaticalmente correctas en un idioma natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "-26.891914 sino gano a la chingada me voy\n",
      "-27.684141 gano sino me voy a la chingada\n",
      "-28.632072 gano voy a me la chingada sino\n",
      "-29.288256 gano sino me a voy la chingada\n",
      "-29.987896 gano voy me a la chingada sino\n",
      "--------------------------------------------------\n",
      "-97.33807 la a voy chingada gano me sino\n",
      "-97.442696 a me sino gano voy chingada la\n",
      "-98.99779 me a chingada sino voy gano la\n",
      "-99.170105 la a sino gano voy chingada me\n",
      "-100.42936 la a chingada gano me sino voy\n"
     ]
    }
   ],
   "source": [
    "from itertools import permutations\n",
    "from random import shuffle\n",
    "\n",
    "word_list = \"sino gano me voy a la chingada\".split(\" \")\n",
    "perms = [\" \".join(perm) for perm in permutations(word_list)]\n",
    "#print(len(perms))\n",
    "print(\"-\"*50)\n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse = True)[:5]:\n",
    "    print(p, t)\n",
    "print(\"-\"*50)  \n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse = True)[-5:]:\n",
    "    print(p, t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PLN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
