{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practica 1: Web Crawling\n",
    "\n",
    "Ingresar a la página del gobierno de México, concretamente la versión estenográfica de la mañanera del presidente. Hacer un scraper para obtener información de todas las conferencias. Se tiene primero que extraer la información de las url, luego de cada url extraer la data de lo que se dice. Obtener los datos mas limpios que se pueda. \n",
    "\n",
    "\n",
    "### Scraping vs Crawling\n",
    "\n",
    "* Web Crawling: Es el proceso de navegar por internet y recolectar información sobre las páginas web. Un crawler, también conocido como spider o bot, visita automáticamente las páginas web y sigue los enlaces (hipervínculos) que encuentra en estas páginas. El objetivo principal de un crawler es indexar la información de las páginas web para que puedan ser recuperadas más tarde por un motor de búsqueda. Googlebot es un ejemplo de un crawler muy conocido.\n",
    "\n",
    "* Web Scraping: Es el proceso de extraer datos específicos de páginas web. A diferencia del crawling, que puede ser más general en la recolección de información, el scraping está orientado a obtener detalles específicos como precios de productos, información de contacto, textos, etc. Para hacer scraping, usualmente se necesita analizar el HTML de la página y extraer los datos necesarios.\n",
    "\n",
    "Vamos a realizar un crawling para navegar la información y un scraping para obtener el texto. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ligas de cada conferencia\n",
    "\n",
    "Cargamos la liga general de la cual obtendremos todas las ligas. Utilizamos headers para que se nos reconozca como una computadora y ejecutar el crawling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libreria para hacer solicitudes http\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_general = 'https://presidente.gob.mx/secciones/version-estenografica/'\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.106 Safari/537.36'\n",
    "}\n",
    "response_general = requests.get(url_general, headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objeto que obtenemos es un tipo **response**. \n",
    "\n",
    "### ¿Que es un objeto tipo response?\n",
    "\n",
    "**Instancia de la Clase `Response`**: Cuando se hace una solicitud como `requests.get(url)`, la función devuelve un objeto de la clase `Response`. Este objeto contiene múltiples datos y métodos útiles para trabajar con la respuesta del servidor.\n",
    "\n",
    "### Contenido del Objeto `Response`\n",
    "\n",
    "El objeto `Response` incluye varios atributos y métodos útiles, como:\n",
    "\n",
    "- **`.text`**: El contenido de la respuesta, en forma de cadena de texto (string). Si la respuesta es HTML, por ejemplo, aquí obtendrás el código HTML completo de la página.\n",
    "\n",
    "- **`.content`**: El contenido de la respuesta en formato binario (bytes). Esto es útil para datos no textuales como imágenes.\n",
    "\n",
    "- **`.json()`**: Si la respuesta es JSON, puedes usar este método para analizarla automáticamente y convertirla en un diccionario de Python. Por ejemplo, si accedes a un endpoint de una API que devuelve JSON, harías `response.json()` para obtener los datos en un formato manejable.\n",
    "\n",
    "- **`.status_code`**: El código de estado HTTP de la respuesta (como 200 para éxito, 404 para no encontrado, etc.).\n",
    "\n",
    "- **`.headers`**: Un diccionario que contiene los encabezados HTTP de la respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se guarda el contenido de la página en un archivo.txt\n",
    "with open('Conferencias_General.txt', 'w', encoding=response_general.encoding) as file:\n",
    "    file.write(response_general.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para limpiar los datos se puede utilizar una **expresión regular** que consiste en un string con el que se van a comparar los elementos de la página web. \n",
    "Ej de expresión regular `(http[s]?://[^\"]+)\"`\n",
    "\n",
    "Otra opción es utilizar la librería BeautifulSoup, muy utilizada para limpiar datos de páginas web. \n",
    "**Beautiful Soup**: Beautiful Soup es una biblioteca de Python para extraer datos de documentos HTML y XML. Esta biblioteca crea un árbol con todos los elementos del documento y puede ser utilizado para extraer información. Por lo tanto, esta biblioteca es útil para realizar web scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_general = response_general.text\n",
    "soup = BeautifulSoup(data_general, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La librería genera un objeto tipo `BeautifulSoup`, similar a como se genera un objeto tipo `response`. Sobre el objeto `BeautifulSoup` al igual que con el `response` podemos hacer operaciones. \n",
    "\n",
    "Nos interesan solo las url que tienen la estructura de la versión estenográfica o contienen esta palabra clave dentro de la url. `https://presidente.gob.mx/22-01-24-version-estenografica.../`\n",
    "\n",
    "Para encontrar todos los url necesitamos iterar sobre cada elemento del objeto soup (que contiene las url). Luego comparamos para que contengan las palabras clave. Para hacer esto podemos hacer uso de una list comprenhension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontrar todos los enlaces\n",
    "urls_1 = [link.get('href') for link in soup.find_all('a') if link.get('href') \n",
    "          and 'version-estenografica-de-la' in link.get('href')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estructura Básica de una Comprensión de Lista\n",
    "\n",
    "Una comprensión de lista en Python tiene la siguiente forma básica:\n",
    "\n",
    "```python\n",
    "[expresión for elemento in iterable if condición]\n",
    "```\n",
    "\n",
    "- **`expresión`**: Es lo que cada elemento de la lista resultante será. Puede ser el mismo elemento del iterable o alguna transformación del mismo.\n",
    "- **`for elemento in iterable`**: Es un bucle `for` que recorre cada elemento en un iterable (como una lista, un rango, etc.).\n",
    "- **`if condición`**: Es una condición opcional. Si se incluye, solo los elementos del iterable que cumplan con esta condición serán considerados.\n",
    "\n",
    "### Ejemplo específico\n",
    "\n",
    "- **`for link in soup.find_all('a')`**: Itera sobre cada elemento `link` (etiqueta `<a>`) en el objeto `soup`.\n",
    "\n",
    "- **`link.get('href')`**: Obtiene el valor del atributo `href` de cada etiqueta `<a>`.\n",
    "\n",
    "- **`if link.get('href')`**: Verifica si la etiqueta `<a>` tiene un atributo `href` no nulo.\n",
    "\n",
    "- **`'version-estenografica-de-la' in link.get('href')`**: Verifica si el texto `'version-estenografica-de-la'` está presente en el valor de `href`.\n",
    "\n",
    "Ahora necesitamos hacer esto para todas las páginas. Para esto utilizamos un ciclo for. Además agregamos una barra de progreso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando páginas:   0%|          | 0/135 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando páginas: 100%|██████████| 135/135 [04:56<00:00,  2.20s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Iteramos sobre todas las páginas\n",
    "NPaginas = 135\n",
    "\n",
    "url_general = 'https://presidente.gob.mx/secciones/version-estenografica/page/'\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.106 Safari/537.36'\n",
    "}\n",
    "\n",
    "urls_conferencia = [] # Inicializamos la lista\n",
    "\n",
    "for i in tqdm(range(NPaginas), desc=\"Procesando páginas\"):\n",
    "    # Obtener cada pagina\n",
    "    response_i = requests.get(url_general + str(i+1) + '/', headers=headers)\n",
    "    \n",
    "    # Filtrar paginas web\n",
    "    data_i = response_i.text\n",
    "    soup_i = BeautifulSoup(data_i, 'html.parser')\n",
    "\n",
    "    # Guardar urls en lista\n",
    "    urls_conferencia += [link.get('href') for link in soup_i.find_all('a') if link.get('href') \n",
    "          and 'version-estenografica-de-la' in link.get('href')]\n",
    "    \n",
    "    time.sleep(1)  # Pausa de 1 segundo para no sobrecargar el servidor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2432"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls_conferencia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Información de las páginas\n",
    "\n",
    "En esta sección necesitamos obtener información de las páginas. Vamos a filtar el html de cada página utilizando la librería beautifulSoup.\n",
    "\n",
    "Primero, es necesario guardar toda las las páginas contenidas en el arreglo `urls_conferencia`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in urls_conferencia:\n",
    "    # Crawling\n",
    "    response_i = requests.get(url, headers=headers)\n",
    "\n",
    "    # Limpieza por beautifulSoup\n",
    "    data_i = response_i.text\n",
    "    soup_i = BeautifulSoup(data_i, 'html.parser')\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PLN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
