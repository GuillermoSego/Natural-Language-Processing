{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LlamBERT: Large-scale low-cost data annotation in NLP\n",
        "\n",
        "## Proyecto final.  Procesamiento del Lenguaje Natural\n",
        "\n",
        "Guillermo Segura Gómez\n",
        "\n",
        "### Introducción\n",
        "\n",
        "En la actualidad cuando nos referimos a la tarea de etiquetar un corpus de lenguaje natural los Grandes Modelos de Lenguaje (LLMs) como GPT-4 y Llama 2 se presentan a si mismos como una muy buena solución. De hecho un mínimo de *prompt-tuning* es suficiente para ser altamente competentes en muchas tareas de NLP. Sin embargo correr millones de prompts demanda una gran cantidad de recursos computacionales. Una muy buena alternativa resulta en la combinación de un LLM con modelos mas pequeños pero muy eficientes.\n",
        "\n",
        "El artículo plantea una metodología híbrida denominada **LlamBERT** en la que se utiliza un LLM, en este caso Llama 2, para etiquetar un subconjunto de datos extraídos aleatoriamente del corpus. Con estas etiquetas se realiza un fine tuning a un modelo tipo transformer encoder de pequeña escala para poder etiquetar el corpus completo. Esto plantea una reducción bastante significativa del *tiempo de inferencia* además de la reducción del costo computacional, mientras se mantiene una alta precisión.\n",
        "\n",
        "### Enfoque metodológico\n",
        "* Pasos del Enfoque LlamBERT:\n",
        "  1. Anotación Inicial: Utiliza Llama-2 para etiquetar un subconjunto aleatorio de datos no etiquetados con una configuración de 0-shot.\n",
        "  2. Clasificación: Clasifica las respuestas de Llama-2 en las categorías deseadas.\n",
        "  3. Filtrado: Descarta los datos que no se clasifican en ninguna categoría especificada.\n",
        "  4. Fine-Tuning: Utiliza las etiquetas resultantes para afinar un clasificador BERT.\n",
        "  5. Etiquetado Completo: Aplica el clasificador BERT afinado para anotar el corpus no etiquetado original.\n",
        "\n",
        "### Datasets Utilizados\n",
        "* IMDb Dataset: Dataset de reseñas de películas utilizado para evaluar la clasificación de sentimientos. Incluye 25,000 reseñas etiquetadas para entrenamiento, 25,000 para prueba y 50,000 reseñas no etiquetadas.\n",
        "* UMLS Dataset: Vocabulario biomédico utilizado para evaluar la clasificación de conceptos anatómicos. Incluye 3 millones de conceptos, de los cuales se seleccionaron 150,000 conceptos anatómicos.\n",
        "\n",
        "### Resultados Experimentales\n",
        "\n",
        "Primero se realizó una prueba utilizando el dataset IMDb con los LLM realizando la tarea de etiquetado completo.\n",
        "\n",
        "#### Tabla 1: Comparación de Rendimiento en IMDb\n",
        "\n",
        "\n",
        "**Objetivo:**\n",
        "Comparar el rendimiento de los modelos Llama-2 y GPT-4 en la tarea de clasificación de sentimientos en el conjunto de datos IMDb utilizando diferentes configuraciones de few-shot (0-shot, 1-shot y 2-shot).\n",
        "\n",
        "**Modelos Comparados:**\n",
        "1. **Llama-2-7b-chat:** Un modelo más pequeño de Llama-2.\n",
        "2. **Llama-2-70b-chat:** Un modelo más grande y potente de Llama-2.\n",
        "3. **GPT-4-0613:** Modelo de OpenAI, evaluado solo en configuración 0-shot debido a las limitaciones de acceso a la API.\n",
        "\n",
        "**Configuraciones de Few-Shot:**\n",
        "- **0-shot:** El modelo no recibe ejemplos específicos de la tarea antes de realizar la predicción.\n",
        "- **1-shot:** El modelo recibe un ejemplo específico de la tarea antes de realizar la predicción.\n",
        "- **2-shot:** El modelo recibe dos ejemplos específicos de la tarea antes de realizar la predicción.\n",
        "\n",
        "\n",
        "**Interpretación de los Resultados:**\n",
        "- **Mejora con Few-Shot:** Para Llama-2-7b-chat, la precisión mejora significativamente al pasar de 0-shot a 2-shot, lo que indica que el modelo se beneficia mucho de los ejemplos adicionales.\n",
        "- **Estabilidad en Llama-2-70b-chat:** Para el modelo más grande Llama-2-70b-chat, la precisión es alta incluso en la configuración 0-shot, y no mejora mucho con ejemplos adicionales, lo que sugiere que ya es muy capaz sin necesidad de ejemplos específicos.\n",
        "- **Rendimiento de GPT-4:** El modelo GPT-4 tiene una alta precisión en 0-shot, lo que destaca su capacidad en la tarea sin necesidad de entrenamiento adicional.\n",
        "\n",
        "**Tiempos de Inferencia:**\n",
        "- **Llama-2-7b-chat:** Más rápido que Llama-2-70b-chat debido a su menor tamaño.\n",
        "- **Llama-2-70b-chat:** Tiempos de inferencia significativamente más largos debido a su tamaño y complejidad.\n",
        "- **GPT-4:** No se proporcionaron tiempos de inferencia específicos, pero generalmente se sabe que modelos grandes como GPT-4 tienen tiempos de inferencia largos.\n",
        "\n",
        "#### Tabla 2: Fine-Tuning de Modelos BERT\n",
        "\n",
        "**Objetivo:**\n",
        "Evaluar el rendimiento de diferentes modelos BERT preentrenados afinados con datos estándar y con datos etiquetados por Llama-2-70b-chat.\n",
        "\n",
        "**Modelos Comparados:**\n",
        "1. **distilbert-base**\n",
        "2. **bert-base**\n",
        "3. **bert-large**\n",
        "4. **roberta-base**\n",
        "5. **roberta-large**\n",
        "\n",
        "**Configuraciones de Datos de Entrenamiento:**\n",
        "- **Baseline:** Afinado con datos de entrenamiento estándar (gold-standard).\n",
        "- **LlamBERT:** Afinado con datos etiquetados por Llama-2-70b-chat en configuración 0-shot.\n",
        "- **LlamBERT con Datos Adicionales:** Afinado con datos etiquetados por Llama-2-70b-chat más 50,000 datos adicionales.\n",
        "- **Estrategia Combinada:** Primero afinado con datos etiquetados por Llama-2 y luego afinado nuevamente con datos estándar.\n",
        "\n",
        "\n",
        "**Interpretación de los Resultados:**\n",
        "- **Precisión de LlamBERT:** Los modelos afinados con etiquetas generadas por Llama-2 (LlamBERT) tienen una precisión similar a los afinados con datos estándar, destacando la efectividad del enfoque LlamBERT.\n",
        "- **Mejora con Datos Adicionales:** Añadir 50,000 datos adicionales etiquetados por Llama-2 mejoró ligeramente la precisión en todos los modelos.\n",
        "- **Mejor Desempeño:** RoBERTa-large alcanzó la mejor precisión (96.68%) en la estrategia combinada, mostrando la ventaja de usar tanto etiquetas generadas por Llama como datos estándar.\n",
        "\n",
        "#### Resumen del Proceso Experimental\n",
        "\n",
        "1. **Etiquetado Inicial con Llama-2:** Llama-2-70b-chat fue utilizado para etiquetar un subconjunto de datos no etiquetados en una configuración de 0-shot.\n",
        "2. **Fine-Tuning de BERT:**\n",
        "   - Modelos BERT preentrenados fueron afinados con estas etiquetas generadas por Llama-2.\n",
        "   - Se compararon los resultados de estos modelos con los afinados usando datos estándar.\n",
        "3. **Evaluación y Comparación:**\n",
        "   - Se evaluó la precisión de los modelos en el conjunto de datos de prueba de IMDb.\n",
        "   - Se analizaron los tiempos de inferencia y la eficiencia de cada modelo.\n",
        "4. **Análisis del Error:**\n",
        "   - Se evaluó el impacto del etiquetado incorrecto y se realizó un análisis manual del error en las predicciones del modelo.\n",
        "\n",
        "\n",
        "El artículo demuestra que el enfoque LlamBERT es una solución práctica y efectiva para la anotación de datos a gran escala en NLP, combinando la potencia de LLMs como Llama-2 para generar etiquetas con la eficiencia de modelos más pequeños como BERT para el fine-tuning. Esto permite reducir costos computacionales y mantener una alta precisión en tareas específicas.\n",
        "\n",
        "### Análisis del Error\n",
        "\n",
        "- **Cantidad de Datos y Precisión:**\n",
        "  - **Proceso:** Afinaron `roberta-large` usando diferentes tamaños de subconjuntos de datos de entrenamiento estándar y datos etiquetados por Llama-2-70b-chat.\n",
        "  - **Observación:** La mejora en el rendimiento se estabiliza rápidamente en el caso de LlamBERT, indicando que aumentar la cantidad de datos más allá de cierto punto no mejora significativamente la precisión.\n",
        "  - **Conclusión:** Etiquetar 10,000 entradas es suficiente para obtener un buen equilibrio entre precisión y eficiencia.\n",
        "- **Impacto del Etiquetado Incorrecto:**\n",
        "  - **Experimento:** Compararon el impacto del error de etiquetado de Llama-2 (4.61%) con el etiquetado incorrecto aleatorio.\n",
        "  - **Resultados:** `roberta-large` muestra resistencia al error aleatorio, pero los errores de Llama-2 tienen un impacto mayor en la precisión.\n",
        "- **Análisis Manual del Error:**\n",
        "  - **Proceso:** Revisaron manualmente 100 reseñas mal clasificadas, obteniendo anotaciones humanas independientes.\n",
        "  - **Resultados:** Las salidas del modelo se alineaban más con el sentimiento humano que con las etiquetas estándar, sugiriendo que las etiquetas humanas podrían ser más precisas en algunos casos.\n",
        "\n",
        "\n",
        "El artículo muestra cómo el enfoque LlamBERT puede ser una solución efectiva y eficiente para la anotación de grandes volúmenes de datos en NLP. Al combinar LLMs como Llama-2 para la anotación inicial con modelos más pequeños como BERT para la afinación, es posible reducir los costos computacionales sin sacrificar significativamente la precisión.\n",
        "\n",
        "## Implemtación\n",
        "\n",
        "Para esta parte se va implementar el artículo, primero etiquetando el corpus utilizando los modelos de Llama-2 y GPT-4 y luego utilizando esos datos para realizar un fine-tuning a un modelo BERT para lograr etiquetar el corpus completo.\n"
      ],
      "metadata": {
        "id": "ZsDmKE9AieYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Librerias\n",
        "!pip install neptune python-dotenv\n",
        "!pip install --upgrade transformers[torch] accelerate\n",
        "!pip install --upgrade transformers neptune-client neptune"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xJ9U10LMod5",
        "outputId": "1abf4d48-ddba-4cc9-f066-190fec668729"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting neptune\n",
            "  Downloading neptune-1.10.4-py3-none-any.whl (502 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.6/502.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting GitPython>=2.0.8 (from neptune)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.10/dist-packages (from neptune) (9.4.0)\n",
            "Requirement already satisfied: PyJWT in /usr/lib/python3/dist-packages (from neptune) (2.3.0)\n",
            "Collecting boto3>=1.28.0 (from neptune)\n",
            "  Downloading boto3-1.34.113-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bravado<12.0.0,>=11.0.0 (from neptune)\n",
            "  Downloading bravado-11.0.3-py2.py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (8.1.7)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from neptune) (0.18.3)\n",
            "Requirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (3.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from neptune) (24.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from neptune) (2.0.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from neptune) (5.9.5)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (2.31.0)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (1.3.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (1.16.0)\n",
            "Collecting swagger-spec-validator>=2.7.4 (from neptune)\n",
            "  Downloading swagger_spec_validator-3.0.3-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (4.11.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from neptune) (2.0.7)\n",
            "Requirement already satisfied: websocket-client!=1.0.0,>=0.35.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (1.8.0)\n",
            "Collecting botocore<1.35.0,>=1.34.113 (from boto3>=1.28.0->neptune)\n",
            "  Downloading botocore-1.34.113-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.28.0->neptune)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3>=1.28.0->neptune)\n",
            "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bravado-core>=5.16.1 (from bravado<12.0.0,>=11.0.0->neptune)\n",
            "  Downloading bravado-core-6.1.1.tar.gz (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (1.0.8)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (2.8.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (6.0.1)\n",
            "Collecting simplejson (from bravado<12.0.0,>=11.0.0->neptune)\n",
            "  Downloading simplejson-3.19.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting monotonic (from bravado<12.0.0,>=11.0.0->neptune)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython>=2.0.8->neptune)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune) (2024.2.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from swagger-spec-validator>=2.7.4->neptune) (4.19.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->neptune) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->neptune) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->neptune) (1.25.2)\n",
            "Collecting jsonref (from bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune)\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (0.18.1)\n",
            "Collecting fqdn (from jsonschema->swagger-spec-validator>=2.7.4->neptune)\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting isoduration (from jsonschema->swagger-spec-validator>=2.7.4->neptune)\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Collecting jsonpointer>1.13 (from jsonschema->swagger-spec-validator>=2.7.4->neptune)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting rfc3339-validator (from jsonschema->swagger-spec-validator>=2.7.4->neptune)\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Collecting rfc3986-validator>0.1.0 (from jsonschema->swagger-spec-validator>=2.7.4->neptune)\n",
            "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting uri-template (from jsonschema->swagger-spec-validator>=2.7.4->neptune)\n",
            "  Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: webcolors>=1.11 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (1.13)\n",
            "Collecting arrow>=0.15.0 (from isoduration->jsonschema->swagger-spec-validator>=2.7.4->neptune)\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema->swagger-spec-validator>=2.7.4->neptune)\n",
            "  Downloading types_python_dateutil-2.9.0.20240316-py3-none-any.whl (9.7 kB)\n",
            "Building wheels for collected packages: bravado-core\n",
            "  Building wheel for bravado-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bravado-core: filename=bravado_core-6.1.1-py2.py3-none-any.whl size=67672 sha256=b57e914c44f6a4acb1868059fd326bfa0f0b29c01166de4eb5176406d1bde204\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/35/4a/44ec4c358db21a5d63ed4e40f0f0012a438106f220bce4ccba\n",
            "Successfully built bravado-core\n",
            "Installing collected packages: monotonic, uri-template, types-python-dateutil, smmap, simplejson, rfc3986-validator, rfc3339-validator, python-dotenv, jsonref, jsonpointer, jmespath, fqdn, gitdb, botocore, arrow, s3transfer, isoduration, GitPython, swagger-spec-validator, boto3, bravado-core, bravado, neptune\n",
            "Successfully installed GitPython-3.1.43 arrow-1.3.0 boto3-1.34.113 botocore-1.34.113 bravado-11.0.3 bravado-core-6.1.1 fqdn-1.5.1 gitdb-4.0.11 isoduration-20.11.0 jmespath-1.0.1 jsonpointer-2.4 jsonref-1.1.0 monotonic-1.6 neptune-1.10.4 python-dotenv-1.0.1 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 s3transfer-0.10.1 simplejson-3.19.2 smmap-5.0.1 swagger-spec-validator-3.0.3 types-python-dateutil-2.9.0.20240316 uri-template-1.3.0\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.41.0)\n",
            "Collecting transformers[torch]\n",
            "  Downloading transformers-4.41.1-py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.23.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.3.0+cu121)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->transformers[torch])\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->transformers[torch])\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, transformers, accelerate\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.41.0\n",
            "    Uninstalling transformers-4.41.0:\n",
            "      Successfully uninstalled transformers-4.41.0\n",
            "Successfully installed accelerate-0.30.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 transformers-4.41.1\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.1)\n",
            "Collecting neptune-client\n",
            "  Downloading neptune_client-1.10.4-py3-none-any.whl (502 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.6/502.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: neptune in /usr/local/lib/python3.10/dist-packages (1.10.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: GitPython>=2.0.8 in /usr/local/lib/python3.10/dist-packages (from neptune-client) (3.1.43)\n",
            "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.10/dist-packages (from neptune-client) (9.4.0)\n",
            "Requirement already satisfied: PyJWT in /usr/lib/python3/dist-packages (from neptune-client) (2.3.0)\n",
            "Requirement already satisfied: boto3>=1.28.0 in /usr/local/lib/python3.10/dist-packages (from neptune-client) (1.34.113)\n",
            "Requirement already satisfied: bravado<12.0.0,>=11.0.0 in /usr/local/lib/python3.10/dist-packages (from neptune-client) (11.0.3)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from neptune-client) (8.1.7)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from neptune-client) (0.18.3)\n",
            "Requirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from neptune-client) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from neptune-client) (2.0.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from neptune-client) (5.9.5)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from neptune-client) (1.3.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from neptune-client) (1.16.0)\n",
            "Requirement already satisfied: swagger-spec-validator>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from neptune-client) (3.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from neptune-client) (4.11.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from neptune-client) (2.0.7)\n",
            "Requirement already satisfied: websocket-client!=1.0.0,>=0.35.0 in /usr/local/lib/python3.10/dist-packages (from neptune-client) (1.8.0)\n",
            "Requirement already satisfied: botocore<1.35.0,>=1.34.113 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.28.0->neptune-client) (1.34.113)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.28.0->neptune-client) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.28.0->neptune-client) (0.10.1)\n",
            "Requirement already satisfied: bravado-core>=5.16.1 in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (6.1.1)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (1.0.8)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (2.8.2)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (3.19.2)\n",
            "Requirement already satisfied: monotonic in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (1.6)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython>=2.0.8->neptune-client) (4.0.11)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from swagger-spec-validator>=2.7.4->neptune-client) (4.19.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->neptune-client) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->neptune-client) (2024.1)\n",
            "Requirement already satisfied: jsonref in /usr/local/lib/python3.10/dist-packages (from bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (1.1.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune-client) (5.0.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (0.18.1)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.5.1)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (2.4)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>0.1.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (0.1.1)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=1.11 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.13)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from isoduration->jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.3.0)\n",
            "Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.10/dist-packages (from arrow>=0.15.0->isoduration->jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (2.9.0.20240316)\n",
            "Installing collected packages: neptune-client\n",
            "Successfully installed neptune-client-1.10.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/aielte-research/LlamBERT.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "km6FEISi1cPX",
        "outputId": "d649a644-3d6d-412f-d544-ef7657227162"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LlamBERT'...\n",
            "remote: Enumerating objects: 1309, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 1309 (delta 3), reused 1 (delta 0), pack-reused 1297\u001b[K\n",
            "Receiving objects: 100% (1309/1309), 33.51 MiB | 11.33 MiB/s, done.\n",
            "Resolving deltas: 100% (800/800), done.\n",
            "Updating files: 100% (168/168), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/LlamBERT/BERT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TGvG6pOLRnp",
        "outputId": "029e30f2-5bb6-421a-edd1-286627d7997e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LlamBERT/BERT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python bert_finetune.py -c conf/UMLS/region_10k_quicktest.yaml"
      ],
      "metadata": {
        "id": "MVrD6yaQLriA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1a46195-77fd-442a-b74f-38671c7c8493"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-05-28 15:46:03.779419: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-28 15:46:03.779475: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-28 15:46:03.882829: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-28 15:46:03.905639: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-05-28 15:46:05.140362: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Using device=cuda\n",
            "tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 316kB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "config.json: 100% 571/571 [00:00<00:00, 3.81MB/s]\n",
            "vocab.txt: 100% 232k/232k [00:00<00:00, 27.4MB/s]\n",
            "tokenizer.json: 100% 466k/466k [00:00<00:00, 2.03MB/s]\n",
            "model.safetensors: 100% 1.34G/1.34G [00:12<00:00, 109MB/s] \n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING: Keeping only 100 sentences for test and train for tesing!\n",
            "Number of train samples loaded: 100\n",
            "Number of test samples loaded: 100\n",
            "Using evaluation strategy steps\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "{'loss': 0.5991, 'grad_norm': 24.290802001953125, 'learning_rate': 1e-06, 'epoch': 0.14}\n",
            "  3% 1/35 [00:03<01:47,  3.16s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.93it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:01,  2.89it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.47it/s]\u001b[A\n",
            " 71% 5/7 [00:01<00:00,  2.32it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  2.20it/s]\u001b[A\n",
            "                                  \n",
            "\u001b[A{'eval_test_loss': 0.8795925378799438, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.1249, 'eval_test_samples_per_second': 32.001, 'eval_test_steps_per_second': 2.24, 'epoch': 0.14}\n",
            "  3% 1/35 [00:06<01:47,  3.16s/it]\n",
            "100% 7/7 [00:02<00:00,  2.79it/s]\u001b[A\n",
            "{'loss': 0.6154, 'grad_norm': 8.681886672973633, 'learning_rate': 9.705882352941176e-07, 'epoch': 0.29}\n",
            "  6% 2/35 [00:07<02:06,  3.82s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.88it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.83it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.43it/s]\u001b[A\n",
            " 71% 5/7 [00:01<00:00,  2.29it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  2.17it/s]\u001b[A\n",
            "                                  \n",
            "\u001b[A{'eval_test_loss': 0.9545534253120422, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.1538, 'eval_test_samples_per_second': 31.708, 'eval_test_steps_per_second': 2.22, 'epoch': 0.29}\n",
            "  6% 2/35 [00:10<02:06,  3.82s/it]\n",
            "100% 7/7 [00:02<00:00,  2.76it/s]\u001b[A\n",
            "{'loss': 0.4945, 'grad_norm': 21.568973541259766, 'learning_rate': 9.411764705882352e-07, 'epoch': 0.43}\n",
            "  9% 3/35 [00:11<02:11,  4.10s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.92it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.81it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.43it/s]\u001b[A\n",
            " 71% 5/7 [00:01<00:00,  2.28it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  2.17it/s]\u001b[A\n",
            "                                  \n",
            "\u001b[A{'eval_test_loss': 1.037205696105957, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.1551, 'eval_test_samples_per_second': 31.695, 'eval_test_steps_per_second': 2.219, 'epoch': 0.43}\n",
            "  9% 3/35 [00:15<02:11,  4.10s/it]\n",
            "100% 7/7 [00:02<00:00,  2.78it/s]\u001b[A\n",
            "{'loss': 0.7009, 'grad_norm': 5.757525444030762, 'learning_rate': 9.117647058823529e-07, 'epoch': 0.57}\n",
            " 11% 4/35 [00:16<02:11,  4.24s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.89it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.79it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.42it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:00,  2.25it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  2.15it/s]\u001b[A\n",
            "                                  \n",
            "\u001b[A{'eval_test_loss': 1.1026721000671387, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.1977, 'eval_test_samples_per_second': 31.272, 'eval_test_steps_per_second': 2.189, 'epoch': 0.57}\n",
            " 11% 4/35 [00:19<02:11,  4.24s/it]\n",
            "100% 7/7 [00:02<00:00,  2.74it/s]\u001b[A\n",
            "{'loss': 0.5189, 'grad_norm': 11.812671661376953, 'learning_rate': 8.823529411764705e-07, 'epoch': 0.71}\n",
            " 14% 5/35 [00:20<02:09,  4.33s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.89it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.80it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.41it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:00,  2.25it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  2.14it/s]\u001b[A\n",
            "                                  \n",
            "\u001b[A{'eval_test_loss': 1.1687999963760376, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.2106, 'eval_test_samples_per_second': 31.146, 'eval_test_steps_per_second': 2.18, 'epoch': 0.71}\n",
            " 14% 5/35 [00:24<02:09,  4.33s/it]\n",
            "100% 7/7 [00:02<00:00,  2.72it/s]\u001b[A\n",
            "{'loss': 0.5092, 'grad_norm': 11.444682121276855, 'learning_rate': 8.529411764705882e-07, 'epoch': 0.86}\n",
            " 17% 6/35 [00:25<02:07,  4.39s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.96it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:01,  2.84it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.45it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:00,  2.27it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  2.16it/s]\u001b[A\n",
            "                                  \n",
            "\u001b[A{'eval_test_loss': 1.2258622646331787, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.1768, 'eval_test_samples_per_second': 31.478, 'eval_test_steps_per_second': 2.203, 'epoch': 0.86}\n",
            " 17% 6/35 [00:28<02:07,  4.39s/it]\n",
            "100% 7/7 [00:02<00:00,  2.75it/s]\u001b[A\n",
            "{'loss': 0.6599, 'grad_norm': 11.149889945983887, 'learning_rate': 8.235294117647058e-07, 'epoch': 1.0}\n",
            " 20% 7/35 [00:29<01:56,  4.17s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.70it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.75it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.37it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:00,  2.24it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  2.13it/s]\u001b[A\n",
            "                                  \n",
            "\u001b[A{'eval_test_loss': 1.2612080574035645, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.2178, 'eval_test_samples_per_second': 31.078, 'eval_test_steps_per_second': 2.175, 'epoch': 1.0}\n",
            " 20% 7/35 [00:32<01:56,  4.17s/it]\n",
            "100% 7/7 [00:02<00:00,  2.71it/s]\u001b[A\n",
            "{'loss': 0.5836, 'grad_norm': 7.692431926727295, 'learning_rate': 7.941176470588235e-07, 'epoch': 1.14}\n",
            " 23% 8/35 [00:33<01:55,  4.27s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.90it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.78it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.40it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:00,  2.24it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  2.14it/s]\u001b[A\n",
            "                                  \n",
            "\u001b[A{'eval_test_loss': 1.2745392322540283, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.2175, 'eval_test_samples_per_second': 31.08, 'eval_test_steps_per_second': 2.176, 'epoch': 1.14}\n",
            " 23% 8/35 [00:36<01:55,  4.27s/it]\n",
            "100% 7/7 [00:02<00:00,  2.72it/s]\u001b[A\n",
            "{'loss': 0.433, 'grad_norm': 11.54071044921875, 'learning_rate': 7.647058823529411e-07, 'epoch': 1.29}\n",
            " 26% 9/35 [00:38<01:53,  4.35s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.86it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.77it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.39it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:00,  2.23it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  2.12it/s]\u001b[A\n",
            "                                  \n",
            "\u001b[A{'eval_test_loss': 1.2696595191955566, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.2355, 'eval_test_samples_per_second': 30.907, 'eval_test_steps_per_second': 2.164, 'epoch': 1.29}\n",
            " 26% 9/35 [00:41<01:53,  4.35s/it]\n",
            "100% 7/7 [00:02<00:00,  2.70it/s]\u001b[A\n",
            "{'loss': 0.4252, 'grad_norm': 10.024511337280273, 'learning_rate': 7.352941176470589e-07, 'epoch': 1.43}\n",
            " 29% 10/35 [00:42<01:50,  4.41s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.83it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.74it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.37it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:00,  2.21it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  2.10it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.2638218402862549, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.2726, 'eval_test_samples_per_second': 30.557, 'eval_test_steps_per_second': 2.139, 'epoch': 1.43}\n",
            " 29% 10/35 [00:45<01:50,  4.41s/it]\n",
            "100% 7/7 [00:02<00:00,  2.68it/s]\u001b[A\n",
            "{'loss': 0.5979, 'grad_norm': 6.239227771759033, 'learning_rate': 7.058823529411765e-07, 'epoch': 1.57}\n",
            " 31% 11/35 [00:47<01:47,  4.47s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.82it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.74it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.35it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:00,  2.19it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  2.09it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.2616995573043823, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.2864, 'eval_test_samples_per_second': 30.428, 'eval_test_steps_per_second': 2.13, 'epoch': 1.57}\n",
            " 31% 11/35 [00:50<01:47,  4.47s/it]\n",
            "100% 7/7 [00:02<00:00,  2.66it/s]\u001b[A\n",
            "{'loss': 0.5727, 'grad_norm': 8.374578475952148, 'learning_rate': 6.764705882352941e-07, 'epoch': 1.71}\n",
            " 34% 12/35 [00:51<01:43,  4.51s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.80it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.68it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.32it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:00,  2.16it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  2.06it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.2532752752304077, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.3353, 'eval_test_samples_per_second': 29.983, 'eval_test_steps_per_second': 2.099, 'epoch': 1.71}\n",
            " 34% 12/35 [00:55<01:43,  4.51s/it]\n",
            "100% 7/7 [00:02<00:00,  2.62it/s]\u001b[A\n",
            "{'loss': 0.5342, 'grad_norm': 7.929442405700684, 'learning_rate': 6.470588235294117e-07, 'epoch': 1.86}\n",
            " 37% 13/35 [00:56<01:40,  4.56s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.74it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.67it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.30it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:00,  2.13it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  2.03it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.2440803050994873, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.3777, 'eval_test_samples_per_second': 29.606, 'eval_test_steps_per_second': 2.072, 'epoch': 1.86}\n",
            " 37% 13/35 [00:59<01:40,  4.56s/it]\n",
            "100% 7/7 [00:02<00:00,  2.58it/s]\u001b[A\n",
            "{'loss': 0.3944, 'grad_norm': 17.290300369262695, 'learning_rate': 6.176470588235294e-07, 'epoch': 2.0}\n",
            " 40% 14/35 [01:00<01:31,  4.37s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.62it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.65it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.29it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:00,  2.14it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  2.04it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.248620867729187, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.3517, 'eval_test_samples_per_second': 29.836, 'eval_test_steps_per_second': 2.089, 'epoch': 2.0}\n",
            " 40% 14/35 [01:03<01:31,  4.37s/it]\n",
            "100% 7/7 [00:02<00:00,  2.60it/s]\u001b[A\n",
            "{'loss': 0.4567, 'grad_norm': 10.265897750854492, 'learning_rate': 5.88235294117647e-07, 'epoch': 2.14}\n",
            " 43% 15/35 [01:05<01:29,  4.47s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.73it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.64it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.28it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:00,  2.12it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  2.02it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.257952094078064, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.3978, 'eval_test_samples_per_second': 29.43, 'eval_test_steps_per_second': 2.06, 'epoch': 2.14}\n",
            " 43% 15/35 [01:08<01:29,  4.47s/it]\n",
            "100% 7/7 [00:02<00:00,  2.56it/s]\u001b[A\n",
            "{'loss': 0.4891, 'grad_norm': 8.333294868469238, 'learning_rate': 5.588235294117647e-07, 'epoch': 2.29}\n",
            " 46% 16/35 [01:10<01:26,  4.56s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.65it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.59it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.24it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:00,  2.08it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  1.98it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.2700492143630981, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.46, 'eval_test_samples_per_second': 28.901, 'eval_test_steps_per_second': 2.023, 'epoch': 2.29}\n",
            " 46% 16/35 [01:13<01:26,  4.56s/it]\n",
            "100% 7/7 [00:02<00:00,  2.52it/s]\u001b[A\n",
            "{'loss': 0.4716, 'grad_norm': 9.694276809692383, 'learning_rate': 5.294117647058823e-07, 'epoch': 2.43}\n",
            " 49% 17/35 [01:14<01:23,  4.65s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.60it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.55it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.20it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:00,  2.04it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  1.96it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.2828141450881958, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.5193, 'eval_test_samples_per_second': 28.415, 'eval_test_steps_per_second': 1.989, 'epoch': 2.43}\n",
            " 49% 17/35 [01:18<01:23,  4.65s/it]\n",
            "100% 7/7 [00:02<00:00,  2.47it/s]\u001b[A\n",
            "{'loss': 0.5442, 'grad_norm': 12.409679412841797, 'learning_rate': 5e-07, 'epoch': 2.57}\n",
            " 51% 18/35 [01:19<01:20,  4.75s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.41it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.50it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.16it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:00,  2.00it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  1.93it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.2970463037490845, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.5875, 'eval_test_samples_per_second': 27.875, 'eval_test_steps_per_second': 1.951, 'epoch': 2.57}\n",
            " 51% 18/35 [01:23<01:20,  4.75s/it]\n",
            "100% 7/7 [00:03<00:00,  2.44it/s]\u001b[A\n",
            "{'loss': 0.4988, 'grad_norm': 9.55986499786377, 'learning_rate': 4.705882352941176e-07, 'epoch': 2.71}\n",
            " 54% 19/35 [01:24<01:17,  4.81s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.47it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.47it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.13it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:01,  1.97it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  1.89it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.313286304473877, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.6227, 'eval_test_samples_per_second': 27.604, 'eval_test_steps_per_second': 1.932, 'epoch': 2.71}\n",
            " 54% 19/35 [01:28<01:17,  4.81s/it]\n",
            "100% 7/7 [00:03<00:00,  2.39it/s]\u001b[A\n",
            "{'loss': 0.5807, 'grad_norm': 7.695165634155273, 'learning_rate': 4.4117647058823526e-07, 'epoch': 2.86}\n",
            " 57% 20/35 [01:29<01:13,  4.88s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.47it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.48it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.14it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:01,  1.98it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  1.90it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.3299505710601807, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.625, 'eval_test_samples_per_second': 27.586, 'eval_test_steps_per_second': 1.931, 'epoch': 2.86}\n",
            " 57% 20/35 [01:33<01:13,  4.88s/it]\n",
            "100% 7/7 [00:03<00:00,  2.40it/s]\u001b[A\n",
            "{'loss': 0.5608, 'grad_norm': 12.685516357421875, 'learning_rate': 4.117647058823529e-07, 'epoch': 3.0}\n",
            " 60% 21/35 [01:34<01:05,  4.67s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.50it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.44it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.13it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:01,  1.95it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  1.88it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.344740867614746, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.6668, 'eval_test_samples_per_second': 27.272, 'eval_test_steps_per_second': 1.909, 'epoch': 3.0}\n",
            " 60% 21/35 [01:37<01:05,  4.67s/it]\n",
            "100% 7/7 [00:03<00:00,  2.36it/s]\u001b[A\n",
            "{'loss': 0.397, 'grad_norm': 10.02953052520752, 'learning_rate': 3.8235294117647053e-07, 'epoch': 3.14}\n",
            " 63% 22/35 [01:39<01:02,  4.80s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.41it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.39it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.08it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:01,  1.92it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  1.84it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.3597391843795776, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.7456, 'eval_test_samples_per_second': 26.698, 'eval_test_steps_per_second': 1.869, 'epoch': 3.14}\n",
            " 63% 22/35 [01:42<01:02,  4.80s/it]\n",
            "100% 7/7 [00:03<00:00,  2.32it/s]\u001b[A\n",
            "{'loss': 0.4489, 'grad_norm': 14.171587944030762, 'learning_rate': 3.529411764705882e-07, 'epoch': 3.29}\n",
            " 66% 23/35 [01:44<00:59,  4.92s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.37it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.35it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.04it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:01,  1.89it/s]\u001b[A\n",
            " 86% 6/7 [00:03<00:00,  1.81it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.3703407049179077, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.8147, 'eval_test_samples_per_second': 26.214, 'eval_test_steps_per_second': 1.835, 'epoch': 3.29}\n",
            " 66% 23/35 [01:48<00:59,  4.92s/it]\n",
            "100% 7/7 [00:03<00:00,  2.28it/s]\u001b[A\n",
            "{'loss': 0.4747, 'grad_norm': 10.304849624633789, 'learning_rate': 3.2352941176470586e-07, 'epoch': 3.43}\n",
            " 69% 24/35 [01:49<00:55,  5.03s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.29it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.32it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.01it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:01,  1.86it/s]\u001b[A\n",
            " 86% 6/7 [00:03<00:00,  1.77it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.3808865547180176, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.8754, 'eval_test_samples_per_second': 25.804, 'eval_test_steps_per_second': 1.806, 'epoch': 3.43}\n",
            " 69% 24/35 [01:53<00:55,  5.03s/it]\n",
            "100% 7/7 [00:03<00:00,  2.24it/s]\u001b[A\n",
            "{'loss': 0.5792, 'grad_norm': 76.34644317626953, 'learning_rate': 2.941176470588235e-07, 'epoch': 3.57}\n",
            " 71% 25/35 [01:54<00:51,  5.13s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.21it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.28it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  1.98it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:01,  1.83it/s]\u001b[A\n",
            " 86% 6/7 [00:03<00:00,  1.75it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.389469861984253, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.9375, 'eval_test_samples_per_second': 25.397, 'eval_test_steps_per_second': 1.778, 'epoch': 3.57}\n",
            " 71% 25/35 [01:58<00:51,  5.13s/it]\n",
            "100% 7/7 [00:03<00:00,  2.21it/s]\u001b[A\n",
            "{'loss': 0.6397, 'grad_norm': 6.88744592666626, 'learning_rate': 2.6470588235294114e-07, 'epoch': 3.71}\n",
            " 74% 26/35 [02:00<00:47,  5.22s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.22it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.26it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  1.96it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:01,  1.82it/s]\u001b[A\n",
            " 86% 6/7 [00:03<00:00,  1.75it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.3959403038024902, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.9439, 'eval_test_samples_per_second': 25.356, 'eval_test_steps_per_second': 1.775, 'epoch': 3.71}\n",
            " 74% 26/35 [02:04<00:47,  5.22s/it]\n",
            "100% 7/7 [00:03<00:00,  2.21it/s]\u001b[A\n",
            "{'loss': 0.4864, 'grad_norm': 11.631363868713379, 'learning_rate': 2.352941176470588e-07, 'epoch': 3.86}\n",
            " 77% 27/35 [02:05<00:42,  5.29s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.26it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.30it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.01it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:01,  1.87it/s]\u001b[A\n",
            " 86% 6/7 [00:03<00:00,  1.79it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.4027973413467407, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.8605, 'eval_test_samples_per_second': 25.904, 'eval_test_steps_per_second': 1.813, 'epoch': 3.86}\n",
            " 77% 27/35 [02:09<00:42,  5.29s/it]\n",
            "100% 7/7 [00:03<00:00,  2.26it/s]\u001b[A\n",
            "{'loss': 0.292, 'grad_norm': 30.703157424926758, 'learning_rate': 2.0588235294117645e-07, 'epoch': 4.0}\n",
            " 80% 28/35 [02:10<00:35,  5.04s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.41it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.34it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.06it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:01,  1.89it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  1.81it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.409125804901123, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.814, 'eval_test_samples_per_second': 26.219, 'eval_test_steps_per_second': 1.835, 'epoch': 4.0}\n",
            " 80% 28/35 [02:14<00:35,  5.04s/it]\n",
            "100% 7/7 [00:03<00:00,  2.29it/s]\u001b[A\n",
            "{'loss': 0.4851, 'grad_norm': 7.6175384521484375, 'learning_rate': 1.764705882352941e-07, 'epoch': 4.14}\n",
            " 83% 29/35 [02:15<00:30,  5.10s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.38it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.38it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.07it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:01,  1.92it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  1.84it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.4143388271331787, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.7542, 'eval_test_samples_per_second': 26.637, 'eval_test_steps_per_second': 1.865, 'epoch': 4.14}\n",
            " 83% 29/35 [02:19<00:30,  5.10s/it]\n",
            "100% 7/7 [00:03<00:00,  2.32it/s]\u001b[A\n",
            "{'loss': 0.4097, 'grad_norm': 7.19621467590332, 'learning_rate': 1.4705882352941175e-07, 'epoch': 4.29}\n",
            " 86% 30/35 [02:20<00:25,  5.12s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.43it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.40it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.10it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:01,  1.94it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  1.86it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.4187841415405273, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.7194, 'eval_test_samples_per_second': 26.886, 'eval_test_steps_per_second': 1.882, 'epoch': 4.29}\n",
            " 86% 30/35 [02:24<00:25,  5.12s/it]\n",
            "100% 7/7 [00:03<00:00,  2.34it/s]\u001b[A\n",
            "{'loss': 0.5165, 'grad_norm': 7.774465560913086, 'learning_rate': 1.176470588235294e-07, 'epoch': 4.43}\n",
            " 89% 31/35 [02:25<00:20,  5.13s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.45it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.43it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.11it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:01,  1.95it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  1.87it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.4227982759475708, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.685, 'eval_test_samples_per_second': 27.137, 'eval_test_steps_per_second': 1.9, 'epoch': 4.43}\n",
            " 89% 31/35 [02:29<00:20,  5.13s/it]\n",
            "100% 7/7 [00:03<00:00,  2.37it/s]\u001b[A\n",
            "{'loss': 0.3911, 'grad_norm': 9.400823593139648, 'learning_rate': 8.823529411764706e-08, 'epoch': 4.57}\n",
            " 91% 32/35 [02:31<00:15,  5.12s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.44it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.44it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.12it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:01,  1.96it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  1.89it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.4256603717803955, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.6525, 'eval_test_samples_per_second': 27.379, 'eval_test_steps_per_second': 1.917, 'epoch': 4.57}\n",
            " 91% 32/35 [02:34<00:15,  5.12s/it]\n",
            "100% 7/7 [00:03<00:00,  2.39it/s]\u001b[A\n",
            "{'loss': 0.5635, 'grad_norm': 15.397494316101074, 'learning_rate': 5.88235294117647e-08, 'epoch': 4.71}\n",
            " 94% 33/35 [02:36<00:10,  5.11s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.46it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.45it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.13it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:01,  1.96it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  1.88it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.4278277158737183, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.6785, 'eval_test_samples_per_second': 27.185, 'eval_test_steps_per_second': 1.903, 'epoch': 4.71}\n",
            " 94% 33/35 [02:39<00:10,  5.11s/it]\n",
            "100% 7/7 [00:03<00:00,  2.35it/s]\u001b[A\n",
            "{'loss': 0.41, 'grad_norm': 6.737751483917236, 'learning_rate': 2.941176470588235e-08, 'epoch': 4.86}\n",
            " 97% 34/35 [02:41<00:05,  5.18s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.30it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.40it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.06it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:01,  1.91it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  1.86it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.429193377494812, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.7826, 'eval_test_samples_per_second': 26.437, 'eval_test_steps_per_second': 1.851, 'epoch': 4.86}\n",
            " 97% 34/35 [02:45<00:05,  5.18s/it]\n",
            "100% 7/7 [00:03<00:00,  2.36it/s]\u001b[A\n",
            "{'loss': 0.7821, 'grad_norm': 14.897391319274902, 'learning_rate': 0.0, 'epoch': 5.0}\n",
            "100% 35/35 [02:45<00:00,  4.93s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.46it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.44it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.12it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:01,  1.95it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  1.88it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.4295881986618042, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.6807, 'eval_test_samples_per_second': 27.169, 'eval_test_steps_per_second': 1.902, 'epoch': 5.0}\n",
            "100% 35/35 [02:49<00:00,  4.93s/it]\n",
            "100% 7/7 [00:03<00:00,  2.37it/s]\u001b[A\n",
            "{'train_runtime': 169.3982, 'train_samples_per_second': 2.952, 'train_steps_per_second': 0.207, 'train_loss': 0.5176188894680568, 'epoch': 5.0}\n",
            "100% 35/35 [02:49<00:00,  4.84s/it]\n",
            "100% 7/7 [00:03<00:00,  2.26it/s]\n",
            "final_test/accuracy: 0.0\n",
            "final_test/precision: 0.0\n",
            "final_test/recall: 0.0\n",
            "final_test/f1: 0.0\n",
            "final_test/tp: 0\n",
            "final_test/tn: 0\n",
            "final_test/fp: 0\n",
            "final_test/fn: 100\n",
            "Saving model to /content/saved_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "5Wh-06kTe6lQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar librerías\n",
        "import json\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Especifica la ruta donde se guardó tu modelo y los datos de prueba\n",
        "model_path = \"/content/saved_model\"\n",
        "test_data_path = \"data/UMLS_regions_10k/short_prompt/test.json\"\n",
        "\n",
        "# Cargar el tokenizer y el modelo\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "# Mover el modelo a la GPU si está disponible\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Cargar los datos de prueba\n",
        "with open(test_data_path, \"r\") as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "# Preparar los textos y las etiquetas\n",
        "texts = [item['txt'] for item in test_data]\n",
        "labels = [item['label'] for item in test_data]\n",
        "\n",
        "# Tokenizar y hacer predicciones en lotes más pequeños\n",
        "batch_size = 8  # Ajusta este valor según sea necesario\n",
        "\n",
        "predicted_labels = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for i in tqdm(range(0, len(texts), batch_size)):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "        inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
        "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "        predicted_labels.extend(predictions.cpu().numpy().tolist())\n",
        "\n",
        "print(predicted_labels)\n",
        "\n",
        "# Evaluar el rendimiento\n",
        "accuracy = accuracy_score(labels, predicted_labels)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(labels, predicted_labels, average=\"binary\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hC8MWW6_W2q7",
        "outputId": "0d7f53d7-59d8-4e67-a49f-bfba65a079ee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:25<00:00,  4.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Accuracy: 0.789\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F1 Score: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Comprimir la carpeta saved_model\n",
        "shutil.make_archive('/content/saved_model', 'zip', 'saved_model')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vvfU2jJ_dgG0",
        "outputId": "f16bf058-6f36-4b17-fb21-1d6a33b46364"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/saved_model.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Descargar el archivo comprimido\n",
        "files.download('/content/saved_model.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "MKqPY52weF4V",
        "outputId": "08c1aba8-ecf4-4955-bf79-fe720a061e12"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5552fae0-7efe-4002-93ab-794916ae5477\", \"saved_model.zip\", 22)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOIFL2TGp5ip",
        "outputId": "4a8b38b9-139b-4037-8298-49e58bb95995"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/saved_model /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "bPp_ncBWB6I9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demo modelo\n",
        "\n",
        "Predicciones básicas"
      ],
      "metadata": {
        "id": "BG56ebW6DMRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Cargar el modelo y el tokenizer\n",
        "model_path = \"/content/saved_model\"  # Ajusta esta ruta según tu ubicación\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "# Configurar el dispositivo\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Definir una función para hacer predicciones\n",
        "def predict(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "    return predictions.cpu().numpy()[0]\n",
        "\n",
        "# Ejemplo de uso\n",
        "text = \"This is a groundbreaking discovery in the field of medical science.\"\n",
        "prediction = predict(text)\n",
        "print(f\"Prediction: {'Positive' if prediction == 1 else 'Negative'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3C9VT026DP2y",
        "outputId": "ad8e61e9-bed2-4974-ea41-75443ce86ff8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import json\n",
        "\n",
        "# Cargar los datos de prueba\n",
        "test_data_path = \"data/UMLS_regions_10k/short_prompt/test.json\"\n",
        "with open(test_data_path, \"r\") as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "# Preparar los textos y las etiquetas\n",
        "texts = [item['txt'] for item in test_data]\n",
        "labels = [item['label'] for item in test_data]\n",
        "\n",
        "# Tokenizar y hacer predicciones en lotes\n",
        "batch_size = 8\n",
        "predicted_labels = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "        predicted_labels.extend(predictions.cpu().numpy().tolist())\n",
        "\n",
        "# Evaluar el rendimiento\n",
        "accuracy = accuracy_score(labels, predicted_labels)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(labels, predicted_labels, average=\"binary\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Eu8ono_DQdt",
        "outputId": "6d1ce839-707e-4867-9510-2826cd17fd9d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.789\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F1 Score: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para mostrar predicciones con ejemplos específicos\n",
        "def display_predictions(texts, labels, predictions, num_examples=5):\n",
        "    for i in range(num_examples):\n",
        "        print(f\"Text: {texts[i]}\")\n",
        "        print(f\"Actual Label: {'Negative' if labels[i] == 1 else 'Negative'}\")\n",
        "        print(f\"Predicted Label: {'Positive' if predictions[i] == 1 else 'Negative'}\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "# Mostrar ejemplos específicos\n",
        "display_predictions(texts, labels, predicted_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvQnS6m6EUQv",
        "outputId": "68ef54b9-1bed-4eeb-a577-9c72831ac85b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Decide if the term: medial internal nasal branch of anterior ethmoidal nerve; medial internal nasal branches of anterior ethmoidal nerve; rami nasales interni mediales nervus ethmoidalis anterioris; set of medial internal nasal branches of anterior ethmoidal nerve is related to the human nervous system. Exclude the only vascular structures, even if connected to the nervous system. If multiple examples or terms with multiple words are given, treat them all as a whole and make your decision based on that.\n",
            "Actual Label: Negative\n",
            "Predicted Label: Negative\n",
            "==================================================\n",
            "Text: Decide if the term: brodmann area 11 of right subcallosal gyrus; brodmann area 11 of right paraterminal gyrus is related to the human nervous system. Exclude the only vascular structures, even if connected to the nervous system. If multiple examples or terms with multiple words are given, treat them all as a whole and make your decision based on that.\n",
            "Actual Label: Negative\n",
            "Predicted Label: Negative\n",
            "==================================================\n",
            "Text: Decide if the term: tgfb ligand-receptor complex; tgfbeta ligand-receptor complex; tgf-beta ligand-receptor complex; tgfb ligand-receptor complex location; tgfbeta ligand-receptor complex location; tgf-beta ligand-receptor complex location; transforming growth factor beta ligand-receptor complex; transforming growth factor beta ligand-receptor complex location is related to the human nervous system. Exclude the only vascular structures, even if connected to the nervous system. If multiple examples or terms with multiple words are given, treat them all as a whole and make your decision based on that.\n",
            "Actual Label: Negative\n",
            "Predicted Label: Negative\n",
            "==================================================\n",
            "Text: Decide if the term: internal granular layer of brodmann area 18 is related to the human nervous system. Exclude the only vascular structures, even if connected to the nervous system. If multiple examples or terms with multiple words are given, treat them all as a whole and make your decision based on that.\n",
            "Actual Label: Negative\n",
            "Predicted Label: Negative\n",
            "==================================================\n",
            "Text: Decide if the term: white matter of right side of brainstem is related to the human nervous system. Exclude the only vascular structures, even if connected to the nervous system. If multiple examples or terms with multiple words are given, treat them all as a whole and make your decision based on that.\n",
            "Actual Label: Negative\n",
            "Predicted Label: Negative\n",
            "==================================================\n"
          ]
        }
      ]
    }
  ]
}