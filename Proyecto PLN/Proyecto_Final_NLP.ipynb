{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LlamBERT: Large-scale low-cost data annotation in NLP\n",
        "\n",
        "## Proyecto final.  Procesamiento del Lenguaje Natural\n",
        "\n",
        "Guillermo Segura G贸mez\n",
        "\n",
        "### Introducci贸n\n",
        "\n",
        "En la actualidad cuando nos referimos a la tarea de etiquetar un corpus de lenguaje natural los Grandes Modelos de Lenguaje (LLMs) como GPT-4 y Llama 2 se presentan a si mismos como una muy buena soluci贸n. De hecho un m铆nimo de *prompt-tuning* es suficiente para ser altamente competentes en muchas tareas de NLP. Sin embargo correr millones de prompts demanda una gran cantidad de recursos computacionales. Una muy buena alternativa resulta en la combinaci贸n de un LLM con modelos mas peque帽os pero muy eficientes.\n",
        "\n",
        "El art铆culo plantea una metodolog铆a h铆brida denominada **LlamBERT** en la que se utiliza un LLM, en este caso Llama 2, para etiquetar un subconjunto de datos extra铆dos aleatoriamente del corpus. Con estas etiquetas se realiza un fine tuning a un modelo tipo transformer encoder de peque帽a escala para poder etiquetar el corpus completo. Esto plantea una reducci贸n bastante significativa del *tiempo de inferencia* adem谩s de la reducci贸n del costo computacional, mientras se mantiene una alta precisi贸n.\n",
        "\n",
        "### Enfoque metodol贸gico\n",
        "* Pasos del Enfoque LlamBERT:\n",
        "  1. Anotaci贸n Inicial: Utiliza Llama-2 para etiquetar un subconjunto aleatorio de datos no etiquetados con una configuraci贸n de 0-shot.\n",
        "  2. Clasificaci贸n: Clasifica las respuestas de Llama-2 en las categor铆as deseadas.\n",
        "  3. Filtrado: Descarta los datos que no se clasifican en ninguna categor铆a especificada.\n",
        "  4. Fine-Tuning: Utiliza las etiquetas resultantes para afinar un clasificador BERT.\n",
        "  5. Etiquetado Completo: Aplica el clasificador BERT afinado para anotar el corpus no etiquetado original.\n",
        "\n",
        "### Datasets Utilizados\n",
        "* IMDb Dataset: Dataset de rese帽as de pel铆culas utilizado para evaluar la clasificaci贸n de sentimientos. Incluye 25,000 rese帽as etiquetadas para entrenamiento, 25,000 para prueba y 50,000 rese帽as no etiquetadas.\n",
        "* UMLS Dataset: Vocabulario biom茅dico utilizado para evaluar la clasificaci贸n de conceptos anat贸micos. Incluye 3 millones de conceptos, de los cuales se seleccionaron 150,000 conceptos anat贸micos.\n",
        "\n",
        "### Resultados Experimentales\n",
        "\n",
        "Primero se realiz贸 una prueba utilizando el dataset IMDb con los LLM realizando la tarea de etiquetado completo.\n",
        "\n",
        "#### Tabla 1: Comparaci贸n de Rendimiento en IMDb\n",
        "\n",
        "\n",
        "**Objetivo:**\n",
        "Comparar el rendimiento de los modelos Llama-2 y GPT-4 en la tarea de clasificaci贸n de sentimientos en el conjunto de datos IMDb utilizando diferentes configuraciones de few-shot (0-shot, 1-shot y 2-shot).\n",
        "\n",
        "**Modelos Comparados:**\n",
        "1. **Llama-2-7b-chat:** Un modelo m谩s peque帽o de Llama-2.\n",
        "2. **Llama-2-70b-chat:** Un modelo m谩s grande y potente de Llama-2.\n",
        "3. **GPT-4-0613:** Modelo de OpenAI, evaluado solo en configuraci贸n 0-shot debido a las limitaciones de acceso a la API.\n",
        "\n",
        "**Configuraciones de Few-Shot:**\n",
        "- **0-shot:** El modelo no recibe ejemplos espec铆ficos de la tarea antes de realizar la predicci贸n.\n",
        "- **1-shot:** El modelo recibe un ejemplo espec铆fico de la tarea antes de realizar la predicci贸n.\n",
        "- **2-shot:** El modelo recibe dos ejemplos espec铆ficos de la tarea antes de realizar la predicci贸n.\n",
        "\n",
        "\n",
        "**Interpretaci贸n de los Resultados:**\n",
        "- **Mejora con Few-Shot:** Para Llama-2-7b-chat, la precisi贸n mejora significativamente al pasar de 0-shot a 2-shot, lo que indica que el modelo se beneficia mucho de los ejemplos adicionales.\n",
        "- **Estabilidad en Llama-2-70b-chat:** Para el modelo m谩s grande Llama-2-70b-chat, la precisi贸n es alta incluso en la configuraci贸n 0-shot, y no mejora mucho con ejemplos adicionales, lo que sugiere que ya es muy capaz sin necesidad de ejemplos espec铆ficos.\n",
        "- **Rendimiento de GPT-4:** El modelo GPT-4 tiene una alta precisi贸n en 0-shot, lo que destaca su capacidad en la tarea sin necesidad de entrenamiento adicional.\n",
        "\n",
        "**Tiempos de Inferencia:**\n",
        "- **Llama-2-7b-chat:** M谩s r谩pido que Llama-2-70b-chat debido a su menor tama帽o.\n",
        "- **Llama-2-70b-chat:** Tiempos de inferencia significativamente m谩s largos debido a su tama帽o y complejidad.\n",
        "- **GPT-4:** No se proporcionaron tiempos de inferencia espec铆ficos, pero generalmente se sabe que modelos grandes como GPT-4 tienen tiempos de inferencia largos.\n",
        "\n",
        "#### Tabla 2: Fine-Tuning de Modelos BERT\n",
        "\n",
        "**Objetivo:**\n",
        "Evaluar el rendimiento de diferentes modelos BERT preentrenados afinados con datos est谩ndar y con datos etiquetados por Llama-2-70b-chat.\n",
        "\n",
        "**Modelos Comparados:**\n",
        "1. **distilbert-base**\n",
        "2. **bert-base**\n",
        "3. **bert-large**\n",
        "4. **roberta-base**\n",
        "5. **roberta-large**\n",
        "\n",
        "**Configuraciones de Datos de Entrenamiento:**\n",
        "- **Baseline:** Afinado con datos de entrenamiento est谩ndar (gold-standard).\n",
        "- **LlamBERT:** Afinado con datos etiquetados por Llama-2-70b-chat en configuraci贸n 0-shot.\n",
        "- **LlamBERT con Datos Adicionales:** Afinado con datos etiquetados por Llama-2-70b-chat m谩s 50,000 datos adicionales.\n",
        "- **Estrategia Combinada:** Primero afinado con datos etiquetados por Llama-2 y luego afinado nuevamente con datos est谩ndar.\n",
        "\n",
        "\n",
        "**Interpretaci贸n de los Resultados:**\n",
        "- **Precisi贸n de LlamBERT:** Los modelos afinados con etiquetas generadas por Llama-2 (LlamBERT) tienen una precisi贸n similar a los afinados con datos est谩ndar, destacando la efectividad del enfoque LlamBERT.\n",
        "- **Mejora con Datos Adicionales:** A帽adir 50,000 datos adicionales etiquetados por Llama-2 mejor贸 ligeramente la precisi贸n en todos los modelos.\n",
        "- **Mejor Desempe帽o:** RoBERTa-large alcanz贸 la mejor precisi贸n (96.68%) en la estrategia combinada, mostrando la ventaja de usar tanto etiquetas generadas por Llama como datos est谩ndar.\n",
        "\n",
        "#### Resumen del Proceso Experimental\n",
        "\n",
        "1. **Etiquetado Inicial con Llama-2:** Llama-2-70b-chat fue utilizado para etiquetar un subconjunto de datos no etiquetados en una configuraci贸n de 0-shot.\n",
        "2. **Fine-Tuning de BERT:**\n",
        "   - Modelos BERT preentrenados fueron afinados con estas etiquetas generadas por Llama-2.\n",
        "   - Se compararon los resultados de estos modelos con los afinados usando datos est谩ndar.\n",
        "3. **Evaluaci贸n y Comparaci贸n:**\n",
        "   - Se evalu贸 la precisi贸n de los modelos en el conjunto de datos de prueba de IMDb.\n",
        "   - Se analizaron los tiempos de inferencia y la eficiencia de cada modelo.\n",
        "4. **An谩lisis del Error:**\n",
        "   - Se evalu贸 el impacto del etiquetado incorrecto y se realiz贸 un an谩lisis manual del error en las predicciones del modelo.\n",
        "\n",
        "\n",
        "El art铆culo demuestra que el enfoque LlamBERT es una soluci贸n pr谩ctica y efectiva para la anotaci贸n de datos a gran escala en NLP, combinando la potencia de LLMs como Llama-2 para generar etiquetas con la eficiencia de modelos m谩s peque帽os como BERT para el fine-tuning. Esto permite reducir costos computacionales y mantener una alta precisi贸n en tareas espec铆ficas.\n",
        "\n",
        "### An谩lisis del Error\n",
        "\n",
        "- **Cantidad de Datos y Precisi贸n:**\n",
        "  - **Proceso:** Afinaron `roberta-large` usando diferentes tama帽os de subconjuntos de datos de entrenamiento est谩ndar y datos etiquetados por Llama-2-70b-chat.\n",
        "  - **Observaci贸n:** La mejora en el rendimiento se estabiliza r谩pidamente en el caso de LlamBERT, indicando que aumentar la cantidad de datos m谩s all谩 de cierto punto no mejora significativamente la precisi贸n.\n",
        "  - **Conclusi贸n:** Etiquetar 10,000 entradas es suficiente para obtener un buen equilibrio entre precisi贸n y eficiencia.\n",
        "- **Impacto del Etiquetado Incorrecto:**\n",
        "  - **Experimento:** Compararon el impacto del error de etiquetado de Llama-2 (4.61%) con el etiquetado incorrecto aleatorio.\n",
        "  - **Resultados:** `roberta-large` muestra resistencia al error aleatorio, pero los errores de Llama-2 tienen un impacto mayor en la precisi贸n.\n",
        "- **An谩lisis Manual del Error:**\n",
        "  - **Proceso:** Revisaron manualmente 100 rese帽as mal clasificadas, obteniendo anotaciones humanas independientes.\n",
        "  - **Resultados:** Las salidas del modelo se alineaban m谩s con el sentimiento humano que con las etiquetas est谩ndar, sugiriendo que las etiquetas humanas podr铆an ser m谩s precisas en algunos casos.\n",
        "\n",
        "\n",
        "El art铆culo muestra c贸mo el enfoque LlamBERT puede ser una soluci贸n efectiva y eficiente para la anotaci贸n de grandes vol煤menes de datos en NLP. Al combinar LLMs como Llama-2 para la anotaci贸n inicial con modelos m谩s peque帽os como BERT para la afinaci贸n, es posible reducir los costos computacionales sin sacrificar significativamente la precisi贸n.\n",
        "\n",
        "## Implemtaci贸n\n",
        "\n",
        "Para esta parte se va implementar el art铆culo, primero etiquetando el corpus utilizando los modelos de Llama-2 y GPT-4 y luego utilizando esos datos para realizar un fine-tuning a un modelo BERT para lograr etiquetar el corpus completo.\n"
      ],
      "metadata": {
        "id": "ZsDmKE9AieYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Librerias\n",
        "!pip install neptune python-dotenv\n",
        "!pip install --upgrade transformers[torch] accelerate\n",
        "!pip install --upgrade transformers neptune-client neptune"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xJ9U10LMod5",
        "outputId": "1abf4d48-ddba-4cc9-f066-190fec668729"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting neptune\n",
            "  Downloading neptune-1.10.4-py3-none-any.whl (502 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m502.6/502.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting GitPython>=2.0.8 (from neptune)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.10/dist-packages (from neptune) (9.4.0)\n",
            "Requirement already satisfied: PyJWT in /usr/lib/python3/dist-packages (from neptune) (2.3.0)\n",
            "Collecting boto3>=1.28.0 (from neptune)\n",
            "  Downloading boto3-1.34.113-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bravado<12.0.0,>=11.0.0 (from neptune)\n",
            "  Downloading bravado-11.0.3-py2.py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (8.1.7)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from neptune) (0.18.3)\n",
            "Requirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (3.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from neptune) (24.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from neptune) (2.0.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from neptune) (5.9.5)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (2.31.0)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (1.3.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (1.16.0)\n",
            "Collecting swagger-spec-validator>=2.7.4 (from neptune)\n",
            "  Downloading swagger_spec_validator-3.0.3-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (4.11.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from neptune) (2.0.7)\n",
            "Requirement already satisfied: websocket-client!=1.0.0,>=0.35.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (1.8.0)\n",
            "Collecting botocore<1.35.0,>=1.34.113 (from boto3>=1.28.0->neptune)\n",
            "  Downloading botocore-1.34.113-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.28.0->neptune)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3>=1.28.0->neptune)\n",
            "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bravado-core>=5.16.1 (from bravado<12.0.0,>=11.0.0->neptune)\n",
            "  Downloading bravado-core-6.1.1.tar.gz (63 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (1.0.8)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (2.8.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (6.0.1)\n",
            "Collecting simplejson (from bravado<12.0.0,>=11.0.0->neptune)\n",
            "  Downloading simplejson-3.19.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting monotonic (from bravado<12.0.0,>=11.0.0->neptune)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython>=2.0.8->neptune)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune) (2024.2.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from swagger-spec-validator>=2.7.4->neptune) (4.19.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->neptune) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->neptune) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->neptune) (1.25.2)\n",
            "Collecting jsonref (from bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune)\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (0.18.1)\n",
            "Collecting fqdn (from jsonschema->swagger-spec-validator>=2.7.4->neptune)\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting isoduration (from jsonschema->swagger-spec-validator>=2.7.4->neptune)\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Collecting jsonpointer>1.13 (from jsonschema->swagger-spec-validator>=2.7.4->neptune)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting rfc3339-validator (from jsonschema->swagger-spec-validator>=2.7.4->neptune)\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Collecting rfc3986-validator>0.1.0 (from jsonschema->swagger-spec-validator>=2.7.4->neptune)\n",
            "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting uri-template (from jsonschema->swagger-spec-validator>=2.7.4->neptune)\n",
            "  Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: webcolors>=1.11 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (1.13)\n",
            "Collecting arrow>=0.15.0 (from isoduration->jsonschema->swagger-spec-validator>=2.7.4->neptune)\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema->swagger-spec-validator>=2.7.4->neptune)\n",
            "  Downloading types_python_dateutil-2.9.0.20240316-py3-none-any.whl (9.7 kB)\n",
            "Building wheels for collected packages: bravado-core\n",
            "  Building wheel for bravado-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bravado-core: filename=bravado_core-6.1.1-py2.py3-none-any.whl size=67672 sha256=b57e914c44f6a4acb1868059fd326bfa0f0b29c01166de4eb5176406d1bde204\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/35/4a/44ec4c358db21a5d63ed4e40f0f0012a438106f220bce4ccba\n",
            "Successfully built bravado-core\n",
            "Installing collected packages: monotonic, uri-template, types-python-dateutil, smmap, simplejson, rfc3986-validator, rfc3339-validator, python-dotenv, jsonref, jsonpointer, jmespath, fqdn, gitdb, botocore, arrow, s3transfer, isoduration, GitPython, swagger-spec-validator, boto3, bravado-core, bravado, neptune\n",
            "Successfully installed GitPython-3.1.43 arrow-1.3.0 boto3-1.34.113 botocore-1.34.113 bravado-11.0.3 bravado-core-6.1.1 fqdn-1.5.1 gitdb-4.0.11 isoduration-20.11.0 jmespath-1.0.1 jsonpointer-2.4 jsonref-1.1.0 monotonic-1.6 neptune-1.10.4 python-dotenv-1.0.1 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 s3transfer-0.10.1 simplejson-3.19.2 smmap-5.0.1 swagger-spec-validator-3.0.3 types-python-dateutil-2.9.0.20240316 uri-template-1.3.0\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.41.0)\n",
            "Collecting transformers[torch]\n",
            "  Downloading transformers-4.41.1-py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.23.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.3.0+cu121)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->transformers[torch])\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->transformers[torch])\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, transformers, accelerate\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.41.0\n",
            "    Uninstalling transformers-4.41.0:\n",
            "      Successfully uninstalled transformers-4.41.0\n",
            "Successfully installed accelerate-0.30.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 transformers-4.41.1\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.1)\n",
            "Collecting neptune-client\n",
            "  Downloading neptune_client-1.10.4-py3-none-any.whl (502 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m502.6/502.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: neptune in /usr/local/lib/python3.10/dist-packages (1.10.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: GitPython>=2.0.8 in /usr/local/lib/python3.10/dist-packages (from neptune-client) (3.1.43)\n",
            "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.10/dist-packages (from neptune-client) (9.4.0)\n",
            "Requirement already satisfied: PyJWT in /usr/lib/python3/dist-packages (from neptune-client) (2.3.0)\n",
            "Requirement already satisfied: boto3>=1.28.0 in /usr/local/lib/python3.10/dist-packages (from neptune-client) (1.34.113)\n",
            "Requirement already satisfied: bravado<12.0.0,>=11.0.0 in /usr/local/lib/python3.10/dist-packages (from neptune-client) (11.0.3)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from neptune-client) (8.1.7)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from neptune-client) (0.18.3)\n",
            "Requirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from neptune-client) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from neptune-client) (2.0.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from neptune-client) (5.9.5)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from neptune-client) (1.3.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from neptune-client) (1.16.0)\n",
            "Requirement already satisfied: swagger-spec-validator>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from neptune-client) (3.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from neptune-client) (4.11.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from neptune-client) (2.0.7)\n",
            "Requirement already satisfied: websocket-client!=1.0.0,>=0.35.0 in /usr/local/lib/python3.10/dist-packages (from neptune-client) (1.8.0)\n",
            "Requirement already satisfied: botocore<1.35.0,>=1.34.113 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.28.0->neptune-client) (1.34.113)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.28.0->neptune-client) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.28.0->neptune-client) (0.10.1)\n",
            "Requirement already satisfied: bravado-core>=5.16.1 in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (6.1.1)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (1.0.8)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (2.8.2)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (3.19.2)\n",
            "Requirement already satisfied: monotonic in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (1.6)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython>=2.0.8->neptune-client) (4.0.11)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from swagger-spec-validator>=2.7.4->neptune-client) (4.19.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->neptune-client) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->neptune-client) (2024.1)\n",
            "Requirement already satisfied: jsonref in /usr/local/lib/python3.10/dist-packages (from bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (1.1.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune-client) (5.0.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (0.18.1)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.5.1)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (2.4)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>0.1.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (0.1.1)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=1.11 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.13)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from isoduration->jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.3.0)\n",
            "Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.10/dist-packages (from arrow>=0.15.0->isoduration->jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (2.9.0.20240316)\n",
            "Installing collected packages: neptune-client\n",
            "Successfully installed neptune-client-1.10.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/aielte-research/LlamBERT.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "km6FEISi1cPX",
        "outputId": "d649a644-3d6d-412f-d544-ef7657227162"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LlamBERT'...\n",
            "remote: Enumerating objects: 1309, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 1309 (delta 3), reused 1 (delta 0), pack-reused 1297\u001b[K\n",
            "Receiving objects: 100% (1309/1309), 33.51 MiB | 11.33 MiB/s, done.\n",
            "Resolving deltas: 100% (800/800), done.\n",
            "Updating files: 100% (168/168), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/LlamBERT/BERT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TGvG6pOLRnp",
        "outputId": "029e30f2-5bb6-421a-edd1-286627d7997e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LlamBERT/BERT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python bert_finetune.py -c conf/UMLS/region_10k_quicktest.yaml"
      ],
      "metadata": {
        "id": "MVrD6yaQLriA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1a46195-77fd-442a-b74f-38671c7c8493"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-05-28 15:46:03.779419: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-28 15:46:03.779475: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-28 15:46:03.882829: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-28 15:46:03.905639: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-05-28 15:46:05.140362: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Using device=cuda\n",
            "tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 316kB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "config.json: 100% 571/571 [00:00<00:00, 3.81MB/s]\n",
            "vocab.txt: 100% 232k/232k [00:00<00:00, 27.4MB/s]\n",
            "tokenizer.json: 100% 466k/466k [00:00<00:00, 2.03MB/s]\n",
            "model.safetensors: 100% 1.34G/1.34G [00:12<00:00, 109MB/s] \n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING: Keeping only 100 sentences for test and train for tesing!\n",
            "Number of train samples loaded: 100\n",
            "Number of test samples loaded: 100\n",
            "Using evaluation strategy steps\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "{'loss': 0.5991, 'grad_norm': 24.290802001953125, 'learning_rate': 1e-06, 'epoch': 0.14}\n",
            "  3% 1/35 [00:03<01:47,  3.16s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.93it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:01,  2.89it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.47it/s]\u001b[A\n",
            " 71% 5/7 [00:01<00:00,  2.32it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  2.20it/s]\u001b[A\n",
            "                                  \n",
            "\u001b[A{'eval_test_loss': 0.8795925378799438, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.1249, 'eval_test_samples_per_second': 32.001, 'eval_test_steps_per_second': 2.24, 'epoch': 0.14}\n",
            "  3% 1/35 [00:06<01:47,  3.16s/it]\n",
            "100% 7/7 [00:02<00:00,  2.79it/s]\u001b[A\n",
            "{'loss': 0.6154, 'grad_norm': 8.681886672973633, 'learning_rate': 9.705882352941176e-07, 'epoch': 0.29}\n",
            "  6% 2/35 [00:07<02:06,  3.82s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.88it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.83it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.43it/s]\u001b[A\n",
            " 71% 5/7 [00:01<00:00,  2.29it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  2.17it/s]\u001b[A\n",
            "                                  \n",
            "\u001b[A{'eval_test_loss': 0.9545534253120422, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.1538, 'eval_test_samples_per_second': 31.708, 'eval_test_steps_per_second': 2.22, 'epoch': 0.29}\n",
            "  6% 2/35 [00:10<02:06,  3.82s/it]\n",
            "100% 7/7 [00:02<00:00,  2.76it/s]\u001b[A\n",
            "{'loss': 0.4945, 'grad_norm': 21.568973541259766, 'learning_rate': 9.411764705882352e-07, 'epoch': 0.43}\n",
            "  9% 3/35 [00:11<02:11,  4.10s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.92it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.81it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.43it/s]\u001b[A\n",
            " 71% 5/7 [00:01<00:00,  2.28it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  2.17it/s]\u001b[A\n",
            "                                  \n",
            "\u001b[A{'eval_test_loss': 1.037205696105957, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.1551, 'eval_test_samples_per_second': 31.695, 'eval_test_steps_per_second': 2.219, 'epoch': 0.43}\n",
            "  9% 3/35 [00:15<02:11,  4.10s/it]\n",
            "100% 7/7 [00:02<00:00,  2.78it/s]\u001b[A\n",
            "{'loss': 0.7009, 'grad_norm': 5.757525444030762, 'learning_rate': 9.117647058823529e-07, 'epoch': 0.57}\n",
            " 11% 4/35 [00:16<02:11,  4.24s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.89it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.79it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.42it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:00,  2.25it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  2.15it/s]\u001b[A\n",
            "                                  \n",
            "\u001b[A{'eval_test_loss': 1.1026721000671387, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.1977, 'eval_test_samples_per_second': 31.272, 'eval_test_steps_per_second': 2.189, 'epoch': 0.57}\n",
            " 11% 4/35 [00:19<02:11,  4.24s/it]\n",
            "100% 7/7 [00:02<00:00,  2.74it/s]\u001b[A\n",
            "{'loss': 0.5189, 'grad_norm': 11.812671661376953, 'learning_rate': 8.823529411764705e-07, 'epoch': 0.71}\n",
            " 14% 5/35 [00:20<02:09,  4.33s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.89it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.80it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.41it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:00,  2.25it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  2.14it/s]\u001b[A\n",
            "                                  \n",
            "\u001b[A{'eval_test_loss': 1.1687999963760376, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.2106, 'eval_test_samples_per_second': 31.146, 'eval_test_steps_per_second': 2.18, 'epoch': 0.71}\n",
            " 14% 5/35 [00:24<02:09,  4.33s/it]\n",
            "100% 7/7 [00:02<00:00,  2.72it/s]\u001b[A\n",
            "{'loss': 0.5092, 'grad_norm': 11.444682121276855, 'learning_rate': 8.529411764705882e-07, 'epoch': 0.86}\n",
            " 17% 6/35 [00:25<02:07,  4.39s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.96it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:01,  2.84it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.45it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:00,  2.27it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  2.16it/s]\u001b[A\n",
            "                                  \n",
            "\u001b[A{'eval_test_loss': 1.2258622646331787, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.1768, 'eval_test_samples_per_second': 31.478, 'eval_test_steps_per_second': 2.203, 'epoch': 0.86}\n",
            " 17% 6/35 [00:28<02:07,  4.39s/it]\n",
            "100% 7/7 [00:02<00:00,  2.75it/s]\u001b[A\n",
            "{'loss': 0.6599, 'grad_norm': 11.149889945983887, 'learning_rate': 8.235294117647058e-07, 'epoch': 1.0}\n",
            " 20% 7/35 [00:29<01:56,  4.17s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.70it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.75it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.37it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:00,  2.24it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  2.13it/s]\u001b[A\n",
            "                                  \n",
            "\u001b[A{'eval_test_loss': 1.2612080574035645, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.2178, 'eval_test_samples_per_second': 31.078, 'eval_test_steps_per_second': 2.175, 'epoch': 1.0}\n",
            " 20% 7/35 [00:32<01:56,  4.17s/it]\n",
            "100% 7/7 [00:02<00:00,  2.71it/s]\u001b[A\n",
            "{'loss': 0.5836, 'grad_norm': 7.692431926727295, 'learning_rate': 7.941176470588235e-07, 'epoch': 1.14}\n",
            " 23% 8/35 [00:33<01:55,  4.27s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.90it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.78it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.40it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:00,  2.24it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  2.14it/s]\u001b[A\n",
            "                                  \n",
            "\u001b[A{'eval_test_loss': 1.2745392322540283, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.2175, 'eval_test_samples_per_second': 31.08, 'eval_test_steps_per_second': 2.176, 'epoch': 1.14}\n",
            " 23% 8/35 [00:36<01:55,  4.27s/it]\n",
            "100% 7/7 [00:02<00:00,  2.72it/s]\u001b[A\n",
            "{'loss': 0.433, 'grad_norm': 11.54071044921875, 'learning_rate': 7.647058823529411e-07, 'epoch': 1.29}\n",
            " 26% 9/35 [00:38<01:53,  4.35s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.86it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.77it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.39it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:00,  2.23it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  2.12it/s]\u001b[A\n",
            "                                  \n",
            "\u001b[A{'eval_test_loss': 1.2696595191955566, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.2355, 'eval_test_samples_per_second': 30.907, 'eval_test_steps_per_second': 2.164, 'epoch': 1.29}\n",
            " 26% 9/35 [00:41<01:53,  4.35s/it]\n",
            "100% 7/7 [00:02<00:00,  2.70it/s]\u001b[A\n",
            "{'loss': 0.4252, 'grad_norm': 10.024511337280273, 'learning_rate': 7.352941176470589e-07, 'epoch': 1.43}\n",
            " 29% 10/35 [00:42<01:50,  4.41s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.83it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.74it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.37it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:00,  2.21it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  2.10it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.2638218402862549, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.2726, 'eval_test_samples_per_second': 30.557, 'eval_test_steps_per_second': 2.139, 'epoch': 1.43}\n",
            " 29% 10/35 [00:45<01:50,  4.41s/it]\n",
            "100% 7/7 [00:02<00:00,  2.68it/s]\u001b[A\n",
            "{'loss': 0.5979, 'grad_norm': 6.239227771759033, 'learning_rate': 7.058823529411765e-07, 'epoch': 1.57}\n",
            " 31% 11/35 [00:47<01:47,  4.47s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.82it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.74it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.35it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:00,  2.19it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  2.09it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.2616995573043823, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.2864, 'eval_test_samples_per_second': 30.428, 'eval_test_steps_per_second': 2.13, 'epoch': 1.57}\n",
            " 31% 11/35 [00:50<01:47,  4.47s/it]\n",
            "100% 7/7 [00:02<00:00,  2.66it/s]\u001b[A\n",
            "{'loss': 0.5727, 'grad_norm': 8.374578475952148, 'learning_rate': 6.764705882352941e-07, 'epoch': 1.71}\n",
            " 34% 12/35 [00:51<01:43,  4.51s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.80it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.68it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.32it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:00,  2.16it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  2.06it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.2532752752304077, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.3353, 'eval_test_samples_per_second': 29.983, 'eval_test_steps_per_second': 2.099, 'epoch': 1.71}\n",
            " 34% 12/35 [00:55<01:43,  4.51s/it]\n",
            "100% 7/7 [00:02<00:00,  2.62it/s]\u001b[A\n",
            "{'loss': 0.5342, 'grad_norm': 7.929442405700684, 'learning_rate': 6.470588235294117e-07, 'epoch': 1.86}\n",
            " 37% 13/35 [00:56<01:40,  4.56s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.74it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.67it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.30it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:00,  2.13it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  2.03it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.2440803050994873, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.3777, 'eval_test_samples_per_second': 29.606, 'eval_test_steps_per_second': 2.072, 'epoch': 1.86}\n",
            " 37% 13/35 [00:59<01:40,  4.56s/it]\n",
            "100% 7/7 [00:02<00:00,  2.58it/s]\u001b[A\n",
            "{'loss': 0.3944, 'grad_norm': 17.290300369262695, 'learning_rate': 6.176470588235294e-07, 'epoch': 2.0}\n",
            " 40% 14/35 [01:00<01:31,  4.37s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.62it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.65it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.29it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:00,  2.14it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  2.04it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.248620867729187, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.3517, 'eval_test_samples_per_second': 29.836, 'eval_test_steps_per_second': 2.089, 'epoch': 2.0}\n",
            " 40% 14/35 [01:03<01:31,  4.37s/it]\n",
            "100% 7/7 [00:02<00:00,  2.60it/s]\u001b[A\n",
            "{'loss': 0.4567, 'grad_norm': 10.265897750854492, 'learning_rate': 5.88235294117647e-07, 'epoch': 2.14}\n",
            " 43% 15/35 [01:05<01:29,  4.47s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.73it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.64it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.28it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:00,  2.12it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  2.02it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.257952094078064, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.3978, 'eval_test_samples_per_second': 29.43, 'eval_test_steps_per_second': 2.06, 'epoch': 2.14}\n",
            " 43% 15/35 [01:08<01:29,  4.47s/it]\n",
            "100% 7/7 [00:02<00:00,  2.56it/s]\u001b[A\n",
            "{'loss': 0.4891, 'grad_norm': 8.333294868469238, 'learning_rate': 5.588235294117647e-07, 'epoch': 2.29}\n",
            " 46% 16/35 [01:10<01:26,  4.56s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.65it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.59it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.24it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:00,  2.08it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  1.98it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.2700492143630981, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.46, 'eval_test_samples_per_second': 28.901, 'eval_test_steps_per_second': 2.023, 'epoch': 2.29}\n",
            " 46% 16/35 [01:13<01:26,  4.56s/it]\n",
            "100% 7/7 [00:02<00:00,  2.52it/s]\u001b[A\n",
            "{'loss': 0.4716, 'grad_norm': 9.694276809692383, 'learning_rate': 5.294117647058823e-07, 'epoch': 2.43}\n",
            " 49% 17/35 [01:14<01:23,  4.65s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.60it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.55it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.20it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:00,  2.04it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  1.96it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.2828141450881958, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.5193, 'eval_test_samples_per_second': 28.415, 'eval_test_steps_per_second': 1.989, 'epoch': 2.43}\n",
            " 49% 17/35 [01:18<01:23,  4.65s/it]\n",
            "100% 7/7 [00:02<00:00,  2.47it/s]\u001b[A\n",
            "{'loss': 0.5442, 'grad_norm': 12.409679412841797, 'learning_rate': 5e-07, 'epoch': 2.57}\n",
            " 51% 18/35 [01:19<01:20,  4.75s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.41it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.50it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.16it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:00,  2.00it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  1.93it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.2970463037490845, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.5875, 'eval_test_samples_per_second': 27.875, 'eval_test_steps_per_second': 1.951, 'epoch': 2.57}\n",
            " 51% 18/35 [01:23<01:20,  4.75s/it]\n",
            "100% 7/7 [00:03<00:00,  2.44it/s]\u001b[A\n",
            "{'loss': 0.4988, 'grad_norm': 9.55986499786377, 'learning_rate': 4.705882352941176e-07, 'epoch': 2.71}\n",
            " 54% 19/35 [01:24<01:17,  4.81s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.47it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.47it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.13it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:01,  1.97it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  1.89it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.313286304473877, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.6227, 'eval_test_samples_per_second': 27.604, 'eval_test_steps_per_second': 1.932, 'epoch': 2.71}\n",
            " 54% 19/35 [01:28<01:17,  4.81s/it]\n",
            "100% 7/7 [00:03<00:00,  2.39it/s]\u001b[A\n",
            "{'loss': 0.5807, 'grad_norm': 7.695165634155273, 'learning_rate': 4.4117647058823526e-07, 'epoch': 2.86}\n",
            " 57% 20/35 [01:29<01:13,  4.88s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.47it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.48it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.14it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:01,  1.98it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  1.90it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.3299505710601807, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.625, 'eval_test_samples_per_second': 27.586, 'eval_test_steps_per_second': 1.931, 'epoch': 2.86}\n",
            " 57% 20/35 [01:33<01:13,  4.88s/it]\n",
            "100% 7/7 [00:03<00:00,  2.40it/s]\u001b[A\n",
            "{'loss': 0.5608, 'grad_norm': 12.685516357421875, 'learning_rate': 4.117647058823529e-07, 'epoch': 3.0}\n",
            " 60% 21/35 [01:34<01:05,  4.67s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.50it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.44it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.13it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:01,  1.95it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  1.88it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.344740867614746, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.6668, 'eval_test_samples_per_second': 27.272, 'eval_test_steps_per_second': 1.909, 'epoch': 3.0}\n",
            " 60% 21/35 [01:37<01:05,  4.67s/it]\n",
            "100% 7/7 [00:03<00:00,  2.36it/s]\u001b[A\n",
            "{'loss': 0.397, 'grad_norm': 10.02953052520752, 'learning_rate': 3.8235294117647053e-07, 'epoch': 3.14}\n",
            " 63% 22/35 [01:39<01:02,  4.80s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.41it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.39it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.08it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:01,  1.92it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  1.84it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.3597391843795776, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.7456, 'eval_test_samples_per_second': 26.698, 'eval_test_steps_per_second': 1.869, 'epoch': 3.14}\n",
            " 63% 22/35 [01:42<01:02,  4.80s/it]\n",
            "100% 7/7 [00:03<00:00,  2.32it/s]\u001b[A\n",
            "{'loss': 0.4489, 'grad_norm': 14.171587944030762, 'learning_rate': 3.529411764705882e-07, 'epoch': 3.29}\n",
            " 66% 23/35 [01:44<00:59,  4.92s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.37it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.35it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.04it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:01,  1.89it/s]\u001b[A\n",
            " 86% 6/7 [00:03<00:00,  1.81it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.3703407049179077, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.8147, 'eval_test_samples_per_second': 26.214, 'eval_test_steps_per_second': 1.835, 'epoch': 3.29}\n",
            " 66% 23/35 [01:48<00:59,  4.92s/it]\n",
            "100% 7/7 [00:03<00:00,  2.28it/s]\u001b[A\n",
            "{'loss': 0.4747, 'grad_norm': 10.304849624633789, 'learning_rate': 3.2352941176470586e-07, 'epoch': 3.43}\n",
            " 69% 24/35 [01:49<00:55,  5.03s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.29it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.32it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.01it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:01,  1.86it/s]\u001b[A\n",
            " 86% 6/7 [00:03<00:00,  1.77it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.3808865547180176, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.8754, 'eval_test_samples_per_second': 25.804, 'eval_test_steps_per_second': 1.806, 'epoch': 3.43}\n",
            " 69% 24/35 [01:53<00:55,  5.03s/it]\n",
            "100% 7/7 [00:03<00:00,  2.24it/s]\u001b[A\n",
            "{'loss': 0.5792, 'grad_norm': 76.34644317626953, 'learning_rate': 2.941176470588235e-07, 'epoch': 3.57}\n",
            " 71% 25/35 [01:54<00:51,  5.13s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.21it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.28it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  1.98it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:01,  1.83it/s]\u001b[A\n",
            " 86% 6/7 [00:03<00:00,  1.75it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.389469861984253, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.9375, 'eval_test_samples_per_second': 25.397, 'eval_test_steps_per_second': 1.778, 'epoch': 3.57}\n",
            " 71% 25/35 [01:58<00:51,  5.13s/it]\n",
            "100% 7/7 [00:03<00:00,  2.21it/s]\u001b[A\n",
            "{'loss': 0.6397, 'grad_norm': 6.88744592666626, 'learning_rate': 2.6470588235294114e-07, 'epoch': 3.71}\n",
            " 74% 26/35 [02:00<00:47,  5.22s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.22it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.26it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  1.96it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:01,  1.82it/s]\u001b[A\n",
            " 86% 6/7 [00:03<00:00,  1.75it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.3959403038024902, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.9439, 'eval_test_samples_per_second': 25.356, 'eval_test_steps_per_second': 1.775, 'epoch': 3.71}\n",
            " 74% 26/35 [02:04<00:47,  5.22s/it]\n",
            "100% 7/7 [00:03<00:00,  2.21it/s]\u001b[A\n",
            "{'loss': 0.4864, 'grad_norm': 11.631363868713379, 'learning_rate': 2.352941176470588e-07, 'epoch': 3.86}\n",
            " 77% 27/35 [02:05<00:42,  5.29s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.26it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.30it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.01it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:01,  1.87it/s]\u001b[A\n",
            " 86% 6/7 [00:03<00:00,  1.79it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.4027973413467407, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.8605, 'eval_test_samples_per_second': 25.904, 'eval_test_steps_per_second': 1.813, 'epoch': 3.86}\n",
            " 77% 27/35 [02:09<00:42,  5.29s/it]\n",
            "100% 7/7 [00:03<00:00,  2.26it/s]\u001b[A\n",
            "{'loss': 0.292, 'grad_norm': 30.703157424926758, 'learning_rate': 2.0588235294117645e-07, 'epoch': 4.0}\n",
            " 80% 28/35 [02:10<00:35,  5.04s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.41it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.34it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.06it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:01,  1.89it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  1.81it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.409125804901123, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.814, 'eval_test_samples_per_second': 26.219, 'eval_test_steps_per_second': 1.835, 'epoch': 4.0}\n",
            " 80% 28/35 [02:14<00:35,  5.04s/it]\n",
            "100% 7/7 [00:03<00:00,  2.29it/s]\u001b[A\n",
            "{'loss': 0.4851, 'grad_norm': 7.6175384521484375, 'learning_rate': 1.764705882352941e-07, 'epoch': 4.14}\n",
            " 83% 29/35 [02:15<00:30,  5.10s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.38it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.38it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.07it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:01,  1.92it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  1.84it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.4143388271331787, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.7542, 'eval_test_samples_per_second': 26.637, 'eval_test_steps_per_second': 1.865, 'epoch': 4.14}\n",
            " 83% 29/35 [02:19<00:30,  5.10s/it]\n",
            "100% 7/7 [00:03<00:00,  2.32it/s]\u001b[A\n",
            "{'loss': 0.4097, 'grad_norm': 7.19621467590332, 'learning_rate': 1.4705882352941175e-07, 'epoch': 4.29}\n",
            " 86% 30/35 [02:20<00:25,  5.12s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.43it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.40it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.10it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:01,  1.94it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  1.86it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.4187841415405273, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.7194, 'eval_test_samples_per_second': 26.886, 'eval_test_steps_per_second': 1.882, 'epoch': 4.29}\n",
            " 86% 30/35 [02:24<00:25,  5.12s/it]\n",
            "100% 7/7 [00:03<00:00,  2.34it/s]\u001b[A\n",
            "{'loss': 0.5165, 'grad_norm': 7.774465560913086, 'learning_rate': 1.176470588235294e-07, 'epoch': 4.43}\n",
            " 89% 31/35 [02:25<00:20,  5.13s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.45it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.43it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.11it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:01,  1.95it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  1.87it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.4227982759475708, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.685, 'eval_test_samples_per_second': 27.137, 'eval_test_steps_per_second': 1.9, 'epoch': 4.43}\n",
            " 89% 31/35 [02:29<00:20,  5.13s/it]\n",
            "100% 7/7 [00:03<00:00,  2.37it/s]\u001b[A\n",
            "{'loss': 0.3911, 'grad_norm': 9.400823593139648, 'learning_rate': 8.823529411764706e-08, 'epoch': 4.57}\n",
            " 91% 32/35 [02:31<00:15,  5.12s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.44it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.44it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.12it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:01,  1.96it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  1.89it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.4256603717803955, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.6525, 'eval_test_samples_per_second': 27.379, 'eval_test_steps_per_second': 1.917, 'epoch': 4.57}\n",
            " 91% 32/35 [02:34<00:15,  5.12s/it]\n",
            "100% 7/7 [00:03<00:00,  2.39it/s]\u001b[A\n",
            "{'loss': 0.5635, 'grad_norm': 15.397494316101074, 'learning_rate': 5.88235294117647e-08, 'epoch': 4.71}\n",
            " 94% 33/35 [02:36<00:10,  5.11s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.46it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.45it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.13it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:01,  1.96it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  1.88it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.4278277158737183, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.6785, 'eval_test_samples_per_second': 27.185, 'eval_test_steps_per_second': 1.903, 'epoch': 4.71}\n",
            " 94% 33/35 [02:39<00:10,  5.11s/it]\n",
            "100% 7/7 [00:03<00:00,  2.35it/s]\u001b[A\n",
            "{'loss': 0.41, 'grad_norm': 6.737751483917236, 'learning_rate': 2.941176470588235e-08, 'epoch': 4.86}\n",
            " 97% 34/35 [02:41<00:05,  5.18s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.30it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.40it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.06it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:01,  1.91it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  1.86it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.429193377494812, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.7826, 'eval_test_samples_per_second': 26.437, 'eval_test_steps_per_second': 1.851, 'epoch': 4.86}\n",
            " 97% 34/35 [02:45<00:05,  5.18s/it]\n",
            "100% 7/7 [00:03<00:00,  2.36it/s]\u001b[A\n",
            "{'loss': 0.7821, 'grad_norm': 14.897391319274902, 'learning_rate': 0.0, 'epoch': 5.0}\n",
            "100% 35/35 [02:45<00:00,  4.93s/it]\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.46it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.44it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.12it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:01,  1.95it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  1.88it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_test_loss': 1.4295881986618042, 'eval_test_accuracy': 0.0, 'eval_test_precision': 0.0, 'eval_test_recall': 0.0, 'eval_test_f1': 0.0, 'eval_test_tp': 0, 'eval_test_tn': 0, 'eval_test_fp': 0, 'eval_test_fn': 100, 'eval_test_runtime': 3.6807, 'eval_test_samples_per_second': 27.169, 'eval_test_steps_per_second': 1.902, 'epoch': 5.0}\n",
            "100% 35/35 [02:49<00:00,  4.93s/it]\n",
            "100% 7/7 [00:03<00:00,  2.37it/s]\u001b[A\n",
            "{'train_runtime': 169.3982, 'train_samples_per_second': 2.952, 'train_steps_per_second': 0.207, 'train_loss': 0.5176188894680568, 'epoch': 5.0}\n",
            "100% 35/35 [02:49<00:00,  4.84s/it]\n",
            "100% 7/7 [00:03<00:00,  2.26it/s]\n",
            "final_test/accuracy: 0.0\n",
            "final_test/precision: 0.0\n",
            "final_test/recall: 0.0\n",
            "final_test/f1: 0.0\n",
            "final_test/tp: 0\n",
            "final_test/tn: 0\n",
            "final_test/fp: 0\n",
            "final_test/fn: 100\n",
            "Saving model to /content/saved_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "5Wh-06kTe6lQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar librer铆as\n",
        "import json\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Especifica la ruta donde se guard贸 tu modelo y los datos de prueba\n",
        "model_path = \"/content/saved_model\"\n",
        "test_data_path = \"data/UMLS_regions_10k/short_prompt/test.json\"\n",
        "\n",
        "# Cargar el tokenizer y el modelo\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "# Mover el modelo a la GPU si est谩 disponible\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Cargar los datos de prueba\n",
        "with open(test_data_path, \"r\") as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "# Preparar los textos y las etiquetas\n",
        "texts = [item['txt'] for item in test_data]\n",
        "labels = [item['label'] for item in test_data]\n",
        "\n",
        "# Tokenizar y hacer predicciones en lotes m谩s peque帽os\n",
        "batch_size = 8  # Ajusta este valor seg煤n sea necesario\n",
        "\n",
        "predicted_labels = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for i in tqdm(range(0, len(texts), batch_size)):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "        inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
        "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "        predicted_labels.extend(predictions.cpu().numpy().tolist())\n",
        "\n",
        "print(predicted_labels)\n",
        "\n",
        "# Evaluar el rendimiento\n",
        "accuracy = accuracy_score(labels, predicted_labels)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(labels, predicted_labels, average=\"binary\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hC8MWW6_W2q7",
        "outputId": "0d7f53d7-59d8-4e67-a49f-bfba65a079ee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 125/125 [00:25<00:00,  4.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Accuracy: 0.789\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F1 Score: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Comprimir la carpeta saved_model\n",
        "shutil.make_archive('/content/saved_model', 'zip', 'saved_model')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vvfU2jJ_dgG0",
        "outputId": "f16bf058-6f36-4b17-fb21-1d6a33b46364"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/saved_model.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Descargar el archivo comprimido\n",
        "files.download('/content/saved_model.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "MKqPY52weF4V",
        "outputId": "08c1aba8-ecf4-4955-bf79-fe720a061e12"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5552fae0-7efe-4002-93ab-794916ae5477\", \"saved_model.zip\", 22)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOIFL2TGp5ip",
        "outputId": "4a8b38b9-139b-4037-8298-49e58bb95995"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/saved_model /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "bPp_ncBWB6I9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demo modelo\n",
        "\n",
        "Predicciones b谩sicas"
      ],
      "metadata": {
        "id": "BG56ebW6DMRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Cargar el modelo y el tokenizer\n",
        "model_path = \"/content/saved_model\"  # Ajusta esta ruta seg煤n tu ubicaci贸n\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "# Configurar el dispositivo\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Definir una funci贸n para hacer predicciones\n",
        "def predict(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "    return predictions.cpu().numpy()[0]\n",
        "\n",
        "# Ejemplo de uso\n",
        "text = \"This is a groundbreaking discovery in the field of medical science.\"\n",
        "prediction = predict(text)\n",
        "print(f\"Prediction: {'Positive' if prediction == 1 else 'Negative'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3C9VT026DP2y",
        "outputId": "ad8e61e9-bed2-4974-ea41-75443ce86ff8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import json\n",
        "\n",
        "# Cargar los datos de prueba\n",
        "test_data_path = \"data/UMLS_regions_10k/short_prompt/test.json\"\n",
        "with open(test_data_path, \"r\") as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "# Preparar los textos y las etiquetas\n",
        "texts = [item['txt'] for item in test_data]\n",
        "labels = [item['label'] for item in test_data]\n",
        "\n",
        "# Tokenizar y hacer predicciones en lotes\n",
        "batch_size = 8\n",
        "predicted_labels = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "        predicted_labels.extend(predictions.cpu().numpy().tolist())\n",
        "\n",
        "# Evaluar el rendimiento\n",
        "accuracy = accuracy_score(labels, predicted_labels)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(labels, predicted_labels, average=\"binary\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Eu8ono_DQdt",
        "outputId": "6d1ce839-707e-4867-9510-2826cd17fd9d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.789\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F1 Score: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Funci贸n para mostrar predicciones con ejemplos espec铆ficos\n",
        "def display_predictions(texts, labels, predictions, num_examples=5):\n",
        "    for i in range(num_examples):\n",
        "        print(f\"Text: {texts[i]}\")\n",
        "        print(f\"Actual Label: {'Negative' if labels[i] == 1 else 'Negative'}\")\n",
        "        print(f\"Predicted Label: {'Positive' if predictions[i] == 1 else 'Negative'}\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "# Mostrar ejemplos espec铆ficos\n",
        "display_predictions(texts, labels, predicted_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvQnS6m6EUQv",
        "outputId": "68ef54b9-1bed-4eeb-a577-9c72831ac85b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Decide if the term: medial internal nasal branch of anterior ethmoidal nerve; medial internal nasal branches of anterior ethmoidal nerve; rami nasales interni mediales nervus ethmoidalis anterioris; set of medial internal nasal branches of anterior ethmoidal nerve is related to the human nervous system. Exclude the only vascular structures, even if connected to the nervous system. If multiple examples or terms with multiple words are given, treat them all as a whole and make your decision based on that.\n",
            "Actual Label: Negative\n",
            "Predicted Label: Negative\n",
            "==================================================\n",
            "Text: Decide if the term: brodmann area 11 of right subcallosal gyrus; brodmann area 11 of right paraterminal gyrus is related to the human nervous system. Exclude the only vascular structures, even if connected to the nervous system. If multiple examples or terms with multiple words are given, treat them all as a whole and make your decision based on that.\n",
            "Actual Label: Negative\n",
            "Predicted Label: Negative\n",
            "==================================================\n",
            "Text: Decide if the term: tgfb ligand-receptor complex; tgfbeta ligand-receptor complex; tgf-beta ligand-receptor complex; tgfb ligand-receptor complex location; tgfbeta ligand-receptor complex location; tgf-beta ligand-receptor complex location; transforming growth factor beta ligand-receptor complex; transforming growth factor beta ligand-receptor complex location is related to the human nervous system. Exclude the only vascular structures, even if connected to the nervous system. If multiple examples or terms with multiple words are given, treat them all as a whole and make your decision based on that.\n",
            "Actual Label: Negative\n",
            "Predicted Label: Negative\n",
            "==================================================\n",
            "Text: Decide if the term: internal granular layer of brodmann area 18 is related to the human nervous system. Exclude the only vascular structures, even if connected to the nervous system. If multiple examples or terms with multiple words are given, treat them all as a whole and make your decision based on that.\n",
            "Actual Label: Negative\n",
            "Predicted Label: Negative\n",
            "==================================================\n",
            "Text: Decide if the term: white matter of right side of brainstem is related to the human nervous system. Exclude the only vascular structures, even if connected to the nervous system. If multiple examples or terms with multiple words are given, treat them all as a whole and make your decision based on that.\n",
            "Actual Label: Negative\n",
            "Predicted Label: Negative\n",
            "==================================================\n"
          ]
        }
      ]
    }
  ]
}